<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NLP语义匹配学习赛记录 | Yiiong's Blog</title><meta name="author" content="Yiiong"><meta name="copyright" content="Yiiong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="# 赛题介绍  背景本次项目来源于阿里云天池的日常学习赛：【NLP系列学习赛】语音助手：对话短文本语义匹配_学习赛_天池大赛-阿里云天池的排行榜 背景是OPPO公司有一个语音助手，这个语音助手需要根据对话来识别意图。赛题要求是参赛队伍需要根据脱敏后的短文本query-pair，来预测他们是否属于同一语义，最终提交一个预测概率文件来进行评测。 简单来说就是提供了两句话，需要判断这两句话是不是同一个">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP语义匹配学习赛记录">
<meta property="og:url" content="http://blog.yiiong.top/posts/37125.html">
<meta property="og:site_name" content="Yiiong&#39;s Blog">
<meta property="og:description" content="# 赛题介绍  背景本次项目来源于阿里云天池的日常学习赛：【NLP系列学习赛】语音助手：对话短文本语义匹配_学习赛_天池大赛-阿里云天池的排行榜 背景是OPPO公司有一个语音助手，这个语音助手需要根据对话来识别意图。赛题要求是参赛队伍需要根据脱敏后的短文本query-pair，来预测他们是否属于同一语义，最终提交一个预测概率文件来进行评测。 简单来说就是提供了两句话，需要判断这两句话是不是同一个">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://blog.yiiong.top/img/ava.jpg">
<meta property="article:published_time" content="2025-12-08T14:00:22.474Z">
<meta property="article:modified_time" content="2025-12-09T12:59:16.564Z">
<meta property="article:author" content="Yiiong">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.yiiong.top/img/ava.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "NLP语义匹配学习赛记录",
  "url": "http://blog.yiiong.top/posts/37125.html",
  "image": "http://blog.yiiong.top/img/ava.jpg",
  "datePublished": "2025-12-08T14:00:22.474Z",
  "dateModified": "2025-12-09T12:59:16.564Z",
  "author": [
    {
      "@type": "Person",
      "name": "Yiiong",
      "url": "http://blog.yiiong.top"
    }
  ]
}</script><link rel="shortcut icon" href="/img/logo.svg"><link rel="canonical" href="http://blog.yiiong.top/posts/37125.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const hour = new Date().getHours()
          const isNight = hour <= 6 || hour >= 24
          if (theme === undefined) isNight ? activateDarkMode() : activateLightMode()
          else theme === 'light' ? activateLightMode() : activateDarkMode()
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":"true`"},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: Yiiong","link":"链接: ","source":"来源: Yiiong's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'FancyBox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP语义匹配学习赛记录',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/universe.css"><meta name="generator" content="Hexo 8.1.1"></head><body><div id="web_bg" style="background: transparent;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/ava.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/tag.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Yiiong's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">NLP语义匹配学习赛记录</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">NLP语义匹配学习赛记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-08T14:00:22.474Z" title="发表于 2025-12-08 22:00:22">2025-12-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-09T12:59:16.564Z" title="更新于 2025-12-09 20:59:16">2025-12-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><meta name="referrer" content="no-referrer"/>
# 赛题介绍

<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>本次项目来源于阿里云天池的日常学习赛：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/532329/rankingList">【NLP系列学习赛】语音助手：对话短文本语义匹配_学习赛_天池大赛-阿里云天池的排行榜</a></p>
<p>背景是OPPO公司有一个语音助手，这个语音助手需要根据对话来识别意图。赛题要求是参赛队伍需要根据脱敏后的短文本query-pair，来预测他们是否属于同一语义，最终提交一个预测概率文件来进行评测。</p>
<p>简单来说就是提供了两句话，需要判断这两句话是不是同一个意思（语义匹配）。</p>
<p>比如用下面的几个训练样本来举例：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">肖战的粉丝叫什么名字 肖战的粉丝叫什么 1</span><br><span class="line"></span><br><span class="line">王者荣耀里面打野谁最厉害 王者荣耀什么英雄最好玩 0</span><br><span class="line"></span><br><span class="line">我想换个手机 我要换手机 1</span><br><span class="line"></span><br><span class="line">我是张睿 我想张睿 0</span><br><span class="line"></span><br><span class="line">不想 不想说 0</span><br></pre></td></tr></table></figure>

<p>可以看到意思完全不同的两句话，最终真值标签输出0，表示不匹配。而意思相同的两句话，最终真值标签输出1。</p>
<h2 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h2><p>最终需要提交一份预测结果文件，结果文件中每行为一个0-1的预测值，代表query-lair语义匹配的概率，与测试数据每行一一对应。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">0.001</span><br><span class="line">0.999</span><br></pre></td></tr></table></figure>

<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p><img src="https://gitee.com/beatrueman/images/raw/master/20251208211020703.png" alt="image"></p>
<h1 id="赛题解析"><a href="#赛题解析" class="headerlink" title="赛题解析"></a>赛题解析</h1><p>本赛题属于自然语言处理（NLP）领域的文本匹配&#x2F;语义相似度分类任务。给定两段脱敏后的短文本（query-pair），系统需要判断它们是否表达相同或相近的语义，并输出二分类结果。</p>
<h2 id="难点与挑战"><a href="#难点与挑战" class="headerlink" title="难点与挑战"></a>难点与挑战</h2><ul>
<li><p>文本脱敏：原始词语被替换为数字ID，模型就无法直接使用词义信息，只能从数字序列中学习潜在的语义模式。</p>
<p>如果直接提供了文本，那问题非常简单，直接使用bert-base-chinese这种预训练中文模型就能得到很好的结果，但是这种脱敏数据，由于不知道词表，就无法使用开源的预训练模型。</p>
</li>
<li><p>句子成对输入：任务需要同时理解两段文本并进行比较，而不仅仅是单句分类。</p>
</li>
<li><p>数字ID分布不均：高频词和低频词的数据差异很大，需要合理的enmedding初始化策略，避免低频数字影响训练效果</p>
</li>
<li><p>句子长度和复杂关系：简单模型难以捕获远距离依赖，需要使用能够建模全局信息的网络结果（比如Transformer、BiLSTM等）</p>
</li>
</ul>
<h2 id="数据集结构分析"><a href="#数据集结构分析" class="headerlink" title="数据集结构分析"></a>数据集结构分析</h2><p>提供了4个.tsv数据集，学习赛采用<code>gaiic_track3_round1_testB_20210317.tsv</code>​来做为测试集。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gaiic_track3_round1_testA_20210228.tsv</span><br><span class="line">gaiic_track3_round1_testB_20210317.tsv</span><br><span class="line">gaiic_track3_round1_train_20210228.tsv # 训练样本10万</span><br><span class="line">gaiic_track3_round2_train_20210407.tsv # 复赛训练样本30万</span><br></pre></td></tr></table></figure>

<p>对于训练集，每行为一个训练样本，由query-pair和真值组成，每行格式如下：</p>
<ul>
<li>query-pair格式：query以中文为主，中间可能带有少量英文单词（如英文缩写、品牌词、设备型号等），采用UTF-8编码，未分词，两个query之间使用\t分割。</li>
<li>真值：真值可为0或1，其中1代表query-pair语义相匹配，0则代表不匹配，真值与query-pair之间也用\t分割。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">72 29 68 69 70 533 1661 1877	28 12 347 72 29 369 16	1</span><br><span class="line">12 453 317 43 1163 317 59 355 1024	8 9 1847 6 1163 317 59	0</span><br><span class="line">12 19 1162 126 53 66	12 19 79 389 126 53 66	1</span><br><span class="line">275 552 553 433 881 338 1104 101 202 2343 14825	995 551 550 1660 2830 1075 662 935	0</span><br><span class="line">421 330 62 12 80 81 82 76	202 62 12 80 838 76	1</span><br><span class="line">177 455 456 3474 964 1364 55 1364	133 134 2246	1</span><br><span class="line">29 168 12 19 1003 719 23 29 263 276	29 23 12 115 115 263 16	1</span><br></pre></td></tr></table></figure>

<h1 id="方案介绍"><a href="#方案介绍" class="headerlink" title="方案介绍"></a>方案介绍</h1><p>针对上述难点，本方案设计如下：</p>
<ol>
<li><p><strong>双塔 Transformer 编码</strong></p>
<p> 分别编码两段文本，能够捕获全局语义特征，解决句子长度和复杂关系的问题。</p>
</li>
<li><p><strong>向量融合进行分类</strong></p>
<p> 将两段文本编码向量进行融合，再输入 MLP 分类器，实现句子成对匹配。</p>
</li>
<li><p><strong>结合频度表初始化 embedding</strong></p>
<p> 针对脱敏数字 ID，通过高频 ID 初始化 embedding，帮助模型快速学习有效特征，缓解数据稀疏问题。</p>
</li>
<li><p><strong>合理训练策略</strong></p>
<p> 合并训练集、划分验证集、使用早停机制和批量训练，充分利用数据和计算资源，提高模型稳定性和精度。</p>
</li>
</ol>
<p>利用这套方案，最终在比赛平台上获得了0.84的最终成绩</p>
<p><img src="https://gitee.com/beatrueman/images/raw/master/20251208211020705.png" alt="image"></p>
<h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h2><p><img src="https://gitee.com/beatrueman/images/raw/master/20251208211020706.png" alt="mermaid-diagram 3"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ================= 配置 =================</span><br><span class="line">config = &#123;</span><br><span class="line">    &quot;max_len&quot;: 128,</span><br><span class="line">    &quot;batch_size&quot;: 128,</span><br><span class="line">    &quot;epochs&quot;: 8,</span><br><span class="line">    &quot;lr&quot;: 2e-4,</span><br><span class="line">    &quot;device&quot;: &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;,</span><br><span class="line">    &quot;embed_dim&quot;: 512,</span><br><span class="line">    &quot;num_heads&quot;: 8,</span><br><span class="line">    &quot;num_layers&quot;: 4,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="张量转换：PairDataset"><a href="#张量转换：PairDataset" class="headerlink" title="张量转换：PairDataset"></a>张量转换：PairDataset</h3><p>功能：</p>
<ul>
<li>读取句子对数据（q1，q2）</li>
<li>处理成固定长度序列（padding、截断）</li>
<li>生成attention mask</li>
</ul>
<h4 id="init"><a href="#init" class="headerlink" title="init()"></a><strong>init</strong>()</h4><p>构造q1、q2、label以及最大序列长度max_len（默认 64，每条句子只保留一半长度，因为双塔模型会拼接两条句子，保留一半长度虽然有丢失信息的风险，但是这么做可以确保两条句子信息对等，提高显存效率，因为序列越长显存占用越高）</p>
<p>max_len &#x3D; q1 + q2 + [CLS&#x2F;SEP&#x2F;padding]</p>
<h4 id="序列填充函数：pad-seq"><a href="#序列填充函数：pad-seq" class="headerlink" title="序列填充函数：pad_seq()"></a>序列填充函数：pad_seq()</h4><p>功能：保证每条输入序列长度固定，返回half_len + 2的list</p>
<ul>
<li><p>[0]相当于[CLS]token，表示序列开头</p>
</li>
<li><p>[1]相当于[SEP]token，表示序列结尾</p>
</li>
<li><p>[2]相当于[PAD]，作为填充，保持固定长度</p>
</li>
</ul>
<h4 id="获取样本函数：getitem"><a href="#获取样本函数：getitem" class="headerlink" title="获取样本函数：getitem()"></a>获取样本函数：<strong>getitem</strong>()</h4><p>功能：返回对应数据集的训练格式。</p>
<ul>
<li>如果是训练集，返回（q1,a1,q2,a2）</li>
<li>如果是测试集，返回（q1,a1,q2,a2,label）</li>
</ul>
<p>这里a1,a2表示<code>attention_mask</code>​，1表示有效token，0表示<code>padding token</code>​。Transformer 在编码时用 <code>src_key_padding_mask=(attn_mask == 0)</code>​ 屏蔽 padding。</p>
<h3 id="双塔Transformer：SiameseTransformer"><a href="#双塔Transformer：SiameseTransformer" class="headerlink" title="双塔Transformer：SiameseTransformer"></a>双塔Transformer：SiameseTransformer</h3><p>两条句子分别经过相同编码器 -&gt; 得到向量 -&gt; 融合 -&gt; 分类</p>
<h4 id="init-1"><a href="#init-1" class="headerlink" title="init()"></a><strong>init</strong>()</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.embedding = nn.Embedding(vocab_size, embed_dim)</span><br></pre></td></tr></table></figure>

<p>功能：为每一个数字ID创建一个随机向量（长度为 <code>embed_dim</code>​），也就是 <code>vocab_size</code>​ × <code>embed_dim</code>​ 的矩阵。之后通过 <code>init_embedding_from_freq</code>​ 用高频ID对应的更有意义的向量进行替换，以帮助模型更快学到有效语义。</p>
<ul>
<li>vocab_size：数字ID的总数量</li>
<li>embed_dim：每个token被映射的向量维度</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">encoder_layer = nn.TransformerEncoderLayer(</span><br><span class="line">    d_model=embed_dim,  # 输入向量维度</span><br><span class="line">    nhead=num_heads,    # 多头注意力的头数</span><br><span class="line">    dim_feedforward=embed_dim * 4, # 前馈网络隐藏层大小</span><br><span class="line">    batch_first=True, # 输入维度[batch, seq_len, embed_dim]</span><br><span class="line">    dropout=0.1 # 防止过拟合：随机丢弃部分神经元，也就是随机将10%的神经元输出置为0</span><br><span class="line">)</span><br><span class="line">self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)</span><br></pre></td></tr></table></figure>

<p>功能：定义Transformer编码器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.mlp = nn.Sequential(</span><br><span class="line">    nn.Linear(embed_dim * 2, embed_dim), # 两个句子的向量拼接【输入 1024 (512*2)，输出 512】</span><br><span class="line">    nn.ReLU(), # ReLU激活函数</span><br><span class="line">    nn.Linear(embed_dim, 2) # 分类：线性层输出2个logit（对应0/1）【输入 512，输出 2 (类别数)】</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>功能：融合两条句子的编码向量并做分类</p>
<h4 id="encode"><a href="#encode" class="headerlink" title="encode"></a>encode</h4><p>功能：把一句话从数字序列编码成固定长度向量</p>
<ol>
<li>将数字序列转换为向量序列emb</li>
<li>把emb用Transformer编码，提示其在attm_mask&#x3D;&#x3D;0不参与计算，输出为out</li>
<li>然后取out的第一个token的向量作为句子表示（从 Transformer 输出的一长串向量中，只取出一个代表整句话的向量）</li>
</ol>
<h4 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h4><p>功能：模型前向传播</p>
<ol>
<li>对q1进行encode，得到句子向量v1</li>
<li>对q2进行encode，得到句子向量v2</li>
<li>使用torch.cat拼接两个向量</li>
<li>使用mlp进行分类输出2个logit</li>
</ol>
<h3 id="训练：train"><a href="#训练：train" class="headerlink" title="训练：train"></a>训练：train</h3><ol>
<li><p>设置训练模式（model.train()）、优化器Adam与损失函数（交叉熵CrossEntropyLoss）</p>
</li>
<li><p>设置早停，当连续patience轮验证准确率没有提升，就停止训练，降低过拟合风险。</p>
<p> <code>best_val_auc</code>​：记录验证集的最好auc</p>
<p> <code>patience</code>​：早停阈值，如果连续 <code>patience</code>​ 轮验证准确率没有提升，就停止训练</p>
<p> <code>early_stop_counter</code>​：记录连续验证没有提升的轮数</p>
</li>
<li><p>开始epoch循环</p>
<p> 每次循环遍历训练集的每个batch，把数据转移到GPU上</p>
<p> 接着前向传播：<code>logits = model(...)</code>​ → 模型输出每个类别的分数</p>
<p> 计算损失Loss</p>
<p> 最后反向传播与梯度更新</p>
<ul>
<li><code>optimizer.zero_grad()</code>​ 清除上一步梯度</li>
<li><code>loss.backward()</code>​ 计算梯度</li>
<li><code>optimizer.step()</code>​ 更新模型参数</li>
</ul>
<p> 累计batch损失，打印输出</p>
</li>
<li><p>验证阶段：model.eval()</p>
<p> 遍历验证集，计算每一轮验证集的AUC。</p>
</li>
<li><p>早停判断</p>
<p> 如果当前验证准确率比历史最好值高，就更新best_val_acc，重置早停计数器并保存当前模型。否则早停计数器加1，若连续patience轮没提升就停止训练。</p>
</li>
</ol>
<h3 id="预测：predict"><a href="#预测：predict" class="headerlink" title="预测：predict"></a>预测：predict</h3><p>遍历测试集，输入两条句子及其attention_mask，得到模型输出logits。</p>
<p>然后通过softmax将分数转换为概率，取标签1（匹配）的概率。</p>
<h2 id="频度表初始化：init-embedding-from-freq"><a href="#频度表初始化：init-embedding-from-freq" class="headerlink" title="频度表初始化：init_embedding_from_freq"></a>频度表初始化：init_embedding_from_freq</h2><p>为了应对脱敏文本的问题，这里参考了论坛中的一位参赛选手的方案：<a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/forum/post/935989">AUC 0.9094 BERT经典方案_天池技术圈-阿里云天池</a></p>
<p>即让这些脱敏数字ID变得相对来说有意义，让模型一开始就对高频数字有”合理的表示“，帮助模型更快学习语义。</p>
<p>为此，我统计了训练集里所有数字ID的频率，然后从<a target="_blank" rel="noopener" href="https://lingua.mtsu.edu/chinese-computing/statistics/char/list.php?Which=MO&utm_source=chatgpt.com">https://lingua.mtsu.edu/chinese-computing/statistics/char/list.php?Which=MO&amp;utm_source&#x3D;chatgpt.com</a>中下载了一份汉字频度表文件，取前max_chars个最常用字符，保存在top_chars中。</p>
<p>接着为top_chars生成随机向量（char_vectors），最后将最频繁出现的数字ID对应的embedding替换为char_vectors，这样高频ID就有了合理的表示，低频ID仍然保持随机初始化。</p>
<p><img src="https://gitee.com/beatrueman/images/raw/master/20251208211020707.jpg" alt="IMG_0506"></p>
<h2 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h2><p>正式比赛分为了初赛和复赛，提供了两个训练集。对于学习赛，我将两个训练集合并为一个大的训练集，训练样本有10+40w条。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_train = pd.concat([df_train1, df_train2], ignore_index=True)</span><br></pre></td></tr></table></figure>

<p>这里我划分了训练集和验证集。对于整个数据集，90%作为训练集，10%作为验证集来评估模型效果、调参和早停。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df_tr, df_val = train_test_split(</span><br><span class="line">    df_train, </span><br><span class="line">    test_size=0.1, </span><br><span class="line">    random_state=42, </span><br><span class="line">    stratify=df_train[&#x27;label&#x27;] # 保持正负样本比例一致，防止验证集偏斜，提高验证结果可靠性</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>统计vocab_size，因为Embedding层需要知道有多少词&#x2F;数字ID。刚开始没有统计这个，导致ID超过embedding行数就会报错：某个索引值超出了目标张量的大小。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\cuda\Indexing.cu:1290: block: [103,0,0], thread: [60,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.</span><br><span class="line">C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\cuda\Indexing.cu:1290: block: [103,0,0], thread: [61,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.</span><br><span class="line">  File &quot;c:\Users\bearx\Desktop\project\code\train\train.py&quot;, line 154, in &lt;module&gt;</span><br><span class="line">    predictions = predict(model, test_loader, device)</span><br><span class="line">  File &quot;c:\Users\bearx\Desktop\project\code\train\train.py&quot;, line 111, in predict</span><br><span class="line">    probs = torch.softmax(out, dim=1)[:, 1]  # 取概率</span><br><span class="line">RuntimeError: CUDA error: device-side assert triggered</span><br><span class="line">CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.</span><br><span class="line">For debugging consider passing CUDA_LAUNCH_BLOCKING=1.</span><br><span class="line">Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.</span><br></pre></td></tr></table></figure>

<h2 id="提分尝试"><a href="#提分尝试" class="headerlink" title="提分尝试"></a>提分尝试</h2><p>提分路径：0.78（BiLSTM） &#x3D;&gt; 0.73（BERT）&#x3D;&gt; 0.77（频度表映射+预训练base-bert-chinese） &#x3D;&gt; 0.84（双塔Transformer）</p>
<h3 id="BiLSTM"><a href="#BiLSTM" class="headerlink" title="BiLSTM"></a>BiLSTM</h3><p>BiLSTM（Bidirectional Long Short-Term Memory）：双向长短期记忆网络是一种RNN变体，能够处理序列文本数据（比如文本）并捕获前后依赖信息。</p>
<p>它擅长捕捉长距离依赖，经常用于文本分类、句子相似度匹配等NLP任务。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class BiLSTM(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128):</span><br><span class="line">        super().__init__()</span><br><span class="line">        # 1. Embedding 层，将数字ID映射为向量</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size + 10, embed_dim, padding_idx=0)</span><br><span class="line"></span><br><span class="line">        # 2. BiLSTM 层</span><br><span class="line">        # batch_first=True 表示输入 shape = (batch, seq_len, embed_dim)</span><br><span class="line">        # bidirectional=True 表示双向LSTM</span><br><span class="line">        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)</span><br><span class="line"></span><br><span class="line">        # 3. 全连接层用于分类</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(hidden_dim * 4, 256),  # hidden_dim*4：两条句子 * 双向输出</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(0.3),</span><br><span class="line">            nn.Linear(256, 2)  # 输出2类</span><br><span class="line">        )</span><br><span class="line">		</span><br><span class="line">	# 将每条句子变成一个固定长度向量</span><br><span class="line">    def encode(self, x):</span><br><span class="line">        x = self.embedding(x)      # 转为向量</span><br><span class="line">        out, _ = self.lstm(x)      # BiLSTM 编码</span><br><span class="line">        out, _ = torch.max(out, dim=1)  # 取序列维度的最大值（Max Pooling）</span><br><span class="line">        return out</span><br><span class="line">	</span><br><span class="line">	# 前向传播</span><br><span class="line">	def forward(self, q1, q2):</span><br><span class="line">        v1 = self.encode(q1)       # 编码第一条句子</span><br><span class="line">        v2 = self.encode(q2)       # 编码第二条句子</span><br><span class="line">        x = torch.cat([v1, v2], dim=1)  # 拼接两条句子向量</span><br><span class="line">        return self.fc(x)           # 分类输出</span><br></pre></td></tr></table></figure>

<p>利用BiLSTM获取了0.78的成绩</p>
<p><img src="https://gitee.com/beatrueman/images/raw/master/20251208211020708.png" alt="image"></p>
<p>放弃BiLSTM，选择Transformer的原因：</p>
<ul>
<li>对于很长的数字ID序列，BiLSTM捕获全局模式不如Transformer强。Transformer通过自注意力机制，可以让每个token直接与所有token建立联系</li>
<li>BiLSTM层数少（<code>&quot;embed_dim&quot;: 128</code>​）、参数少，表达能力有限，难以学习到复杂的语义结构。Transformer可以堆叠多层encoder，embed_dim高（<code>&quot;embed_dim&quot;: 512</code>​）</li>
<li>在相同的数据与训练流程下，BiLSTM 双塔的最佳得分约为 0.78，而 Transformer 双塔可以稳定达到 0.84。</li>
</ul>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>在最初尝试中，我将脱敏后的数字 ID 序列直接转换为 tensor 输入 BERT。但这种做法与 BERT 的设计机制完全不匹配，导致模型性能显著下降，最终准确率只有 <strong>0.73</strong>。</p>
<p>主要原因：</p>
<ul>
<li><p>BERT的词表是基于中文字符和词训练的</p>
<p>BERT接收的不是数字ID，而是汉字，并且词表中包含万个token的语义分布。但是脱敏后的数字ID并没有出现在BERT词表里，所以模型无法区分句子之间的差异。</p>
</li>
<li><p>BERT的enbedding是预训练好的中文语义空间</p>
<p>比如很多中文都有固定语义的embedding，但数字ID完全破坏了这个模式，也就是说预训练只是全部失效</p>
</li>
</ul>
<p>在意识到这些问题后，参考了论坛里一位参赛选手的方案，选择了一份汉字频度表，更改了输入，也就是把数字ID频率和汉字频度结合，embedding后让数字ID拥有一定意义，并且使用bert-base-chinese预训练模型，最终的分数达到0.78。</p>
<h2 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h2><ol>
<li>跑一轮epoch耗时过长，加上T4显卡是租的，按时间收费。所以目前仅跑了8个epoch，训练时验证集AUC一直在升高。其实可以再多跑几个epoch，结合早停机制，AUC应该还能继续提高。</li>
<li>模型融合：将多个不同类型的模型进行融合，也就是说把各个模型的输出预测值做平均或者加权平均等。</li>
<li>特征融合：目前只是concat(v1,v2)输入给MLP，可以加入差值（v1-v2）与乘积（v1 * v2）可能会提升判断力。</li>
</ol>
<h1 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h1><p>以前虽然经常听到 Transformer、NLP 这些名词，但始终没有真正动手实践过。上学期的数据挖掘课程的O2O预测项目让我第一次接触到完整的机器学习流程，也感受到了它的魅力。而这学期通过这个 NLP 的小项目，我初步理解了处理文本类任务的基本思路，也认识了常用的模型和方法。</p>
<p>这次的提分过程相比去年做O2O预测要轻松一些——一方面得益于 AI 的帮助，另一方面论坛里大佬们的“点拨”也非常关键。整体提分还算顺利，虽然中间也遇到一些曲折。尤其是从 0.78 提升到 0.84 这段，我第一次真正感受到 Transformer 的强大，也深刻意识到显卡对深度学习实验的重要性。在那个阶段，我常常觉得做机器学习像是在做“黑盒测试”，也难怪很多人说是在“炼丹”——训练一次结果出来后，如果想继续提升，你必须同时考虑模型结构、参数、数据、预处理等多个因素，而且一时很难判断问题究竟出在哪里。也可能是因为目前能力有限，提分的方向常常没有特别明确的思路。</p>
<p>这次的 NLP 实践其实只是对 Transformer、BERT、BiLSTM 这些经典模型有了一个初步认识，它们内部的具体机制和细节我还没完全掌握。不过这次的体验激起了我继续深入学习的兴趣，未来如果有机会，我希望能够系统地理解这些模型背后的原理。</p>
<p>‍</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://blog.yiiong.top">Yiiong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://blog.yiiong.top/posts/37125.html">http://blog.yiiong.top/posts/37125.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://blog.yiiong.top" target="_blank">Yiiong's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-share"><div class="social-share" data-image="/img/ava.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/59602.html" title="O2O用户优惠券预测记录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">O2O用户优惠券预测记录</div></div><div class="info-2"><div class="info-item-1"> # 数据挖掘汇报  大家好，今天由我向大家汇报一下我们组的项目。我们组做的项目题目是：基于XGBoost模型的O2O用户优惠券使用行为分析与预测方法研究。相信一些同学应该对这个题目很熟悉，因为这个题目来源于CSA数据工程实践的考核任务。之所以选择这个题目，一方面是因为它涵盖了许多我们在数据挖掘课程中学习过的机器学习相关知识点，另一方面也是因为我在这个项目上投入了大量时间和精力，并从中真正获得了一些实质性的收获。 因此，我希望借此机会向大家分享我的学习成果和一些心得体会。 下面我会从这几个方面来具体展开整个项目流程，最后还会讲讲K-Means的代码实现。 项目介绍首先是项目介绍。 可能很多同学不知道O2O是做什么的，这里我对O2O做一个解释。O2O是“Online to Offline”的缩写，它是一种商业模式，指的是通过线上平台将用户引导到线下实体店进行消费的方式。比如美团外卖就采用的这种模式，也就是用户线上先下单，商家线下制作并且送餐这种。 那我们为什么要对优惠券进行预测呢？因为优惠券是O2O的一种重要营销手段，反映到我们的生活上也是，哪个商家发优惠券我们就比较喜欢去这个商家...</div></div></div></a><a class="pagination-related" href="/posts/2351.html" title="K8s实践记录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">K8s实践记录</div></div><div class="info-2"><div class="info-item-1"> # K8s实践记录  Level 01.在节点上创建一个持久化目录mkdir -p /data/nginx/logs #创建多级目录chmod 777 /data/nginx/logs #为系统上的每个人提供读、写和执行权限   2.创建一个nginx pod，并在其中配置一个存储卷来将持久化目录挂载到Pod的/var/log/nginx目录中。apiVersion: v1kind: Podmetadata:  name: nginxspec:  volumes:  - name: nginx-logs    hostPath:      path: /data/nginx/logs  containers:  - name: nginx    image: nginx    volumeMounts:    - name: nginx-logs      mountPath: /var/log/nginx    3.配置nginx日志保存到指定的目录在nginx配置文件nginx.conf中添加以下配置,将nginx访问日志保存到/var/log/nginx/access.l...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/59602.html" title="O2O用户优惠券预测记录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-08</div><div class="info-item-2">O2O用户优惠券预测记录</div></div><div class="info-2"><div class="info-item-1"> # 数据挖掘汇报  大家好，今天由我向大家汇报一下我们组的项目。我们组做的项目题目是：基于XGBoost模型的O2O用户优惠券使用行为分析与预测方法研究。相信一些同学应该对这个题目很熟悉，因为这个题目来源于CSA数据工程实践的考核任务。之所以选择这个题目，一方面是因为它涵盖了许多我们在数据挖掘课程中学习过的机器学习相关知识点，另一方面也是因为我在这个项目上投入了大量时间和精力，并从中真正获得了一些实质性的收获。 因此，我希望借此机会向大家分享我的学习成果和一些心得体会。 下面我会从这几个方面来具体展开整个项目流程，最后还会讲讲K-Means的代码实现。 项目介绍首先是项目介绍。 可能很多同学不知道O2O是做什么的，这里我对O2O做一个解释。O2O是“Online to Offline”的缩写，它是一种商业模式，指的是通过线上平台将用户引导到线下实体店进行消费的方式。比如美团外卖就采用的这种模式，也就是用户线上先下单，商家线下制作并且送餐这种。 那我们为什么要对优惠券进行预测呢？因为优惠券是O2O的一种重要营销手段，反映到我们的生活上也是，哪个商家发优惠券我们就比较喜欢去这个商家...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/ava.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Yiiong</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Beatrueman"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:yiiong@redrock.team" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Study in Redrock SRE!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4"><span class="toc-number">2.</span> <span class="toc-text">提交</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-number">3.</span> <span class="toc-text">评估</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B5%9B%E9%A2%98%E8%A7%A3%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">赛题解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-number">4.1.</span> <span class="toc-text">难点与挑战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90"><span class="toc-number">4.2.</span> <span class="toc-text">数据集结构分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%A1%88%E4%BB%8B%E7%BB%8D"><span class="toc-number">5.</span> <span class="toc-text">方案介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84"><span class="toc-number">5.1.</span> <span class="toc-text">代码结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E8%BD%AC%E6%8D%A2%EF%BC%9APairDataset"><span class="toc-number">5.1.1.</span> <span class="toc-text">张量转换：PairDataset</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#init"><span class="toc-number">5.1.1.1.</span> <span class="toc-text">init()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%A1%AB%E5%85%85%E5%87%BD%E6%95%B0%EF%BC%9Apad-seq"><span class="toc-number">5.1.1.2.</span> <span class="toc-text">序列填充函数：pad_seq()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%A0%B7%E6%9C%AC%E5%87%BD%E6%95%B0%EF%BC%9Agetitem"><span class="toc-number">5.1.1.3.</span> <span class="toc-text">获取样本函数：getitem()</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%A1%94Transformer%EF%BC%9ASiameseTransformer"><span class="toc-number">5.1.2.</span> <span class="toc-text">双塔Transformer：SiameseTransformer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#init-1"><span class="toc-number">5.1.2.1.</span> <span class="toc-text">init()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#encode"><span class="toc-number">5.1.2.2.</span> <span class="toc-text">encode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#forward"><span class="toc-number">5.1.2.3.</span> <span class="toc-text">forward</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%EF%BC%9Atrain"><span class="toc-number">5.1.3.</span> <span class="toc-text">训练：train</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%EF%BC%9Apredict"><span class="toc-number">5.1.4.</span> <span class="toc-text">预测：predict</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%91%E5%BA%A6%E8%A1%A8%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9Ainit-embedding-from-freq"><span class="toc-number">5.2.</span> <span class="toc-text">频度表初始化：init_embedding_from_freq</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86"><span class="toc-number">5.3.</span> <span class="toc-text">数据集划分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E5%88%86%E5%B0%9D%E8%AF%95"><span class="toc-number">5.4.</span> <span class="toc-text">提分尝试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BiLSTM"><span class="toc-number">5.4.1.</span> <span class="toc-text">BiLSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT"><span class="toc-number">5.4.2.</span> <span class="toc-text">BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF"><span class="toc-number">5.5.</span> <span class="toc-text">优化思路</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A"><span class="toc-number">6.</span> <span class="toc-text">心得体会</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/52501.html" title="MariaDB Galera错误的更改配置方式导致集群无法拉起">MariaDB Galera错误的更改配置方式导致集群无法拉起</a><time datetime="2025-12-22T13:07:45.035Z" title="发表于 2025-12-22 21:07:45">2025-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/52500.html" title="MariaDB Galera容器集群恢复方法">MariaDB Galera容器集群恢复方法</a><time datetime="2025-12-22T13:07:45.014Z" title="发表于 2025-12-22 21:07:45">2025-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/18658.html" title="Redis主从同步实验">Redis主从同步实验</a><time datetime="2025-12-08T14:55:52.970Z" title="发表于 2025-12-08 22:55:52">2025-12-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/22140.html" title="运维常用工具">运维常用工具</a><time datetime="2025-12-08T14:00:22.525Z" title="发表于 2025-12-08 22:00:22">2025-12-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/36066.html" title="算法记录">算法记录</a><time datetime="2025-12-08T14:00:22.521Z" title="发表于 2025-12-08 22:00:22">2025-12-08</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Yiiong</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="/"></script><div class="js-pjax"></div><canvas id="universe"></canvas><script src="/js/universe.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script async data-pjax src="/"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>