[{"title":"Baidu实习总结","url":"/posts/42488.html","content":"\n时间过得真快，不知不觉中，短暂又漫长的三个多月北漂实习生活就要画上句号了。周围的一切好像都知道我要准备离开了：洗澡的肥皂刚好快用完了、洗面奶也是、路边的银杏树也都落光了叶子......说实话，真舍不得结束实习。\n\n这段时间里，做了一些事、认识了一些人，体验了一段难忘的北漂时光，自己也成长了许多。\n很幸运自己在实习期间遇到了特别好的mentor、leader和十分卓越的团队，还有我的几个“实习搭子”。真希望自己以后工作了也能遇到和他们一样好的人，也希望自己以后能像他们一样优秀。\n成长篇回想起这段实习经历，感觉自己是幸运的，能来百度这样的互联网大厂我觉得并不是自己的实力有多强，更多的是运气。\n第一次来到中关村软件园，这里遍布着平时耳熟能详的互联网公司，什么腾讯、快手、滴滴……简直就是中国互联网的“十字路口”。\n有时候上下班，看到百度科技园上的“熊掌”大logo，心里总是会感叹：原来我也能在这里实习啊。\n​\n个人能力这三个多月，我想特别感谢我的mentor杰哥。\n刚开始的一个月里，我经常会做一些偏“运营”的工作：比如跟进故障改进进度、录入资源信息等等，我总觉得自己是“干杂活”的。其实这段时间的目的主要是为了熟悉工作流程。到现在，我已经能比较熟练的使用团队的运维平台，特别是基于business form的低代码功能制作一些Grafana操作面板，我甚至在即将离职的这段时间里能够几乎独立对接一个商采代码审查平台的运维工作，真的感觉越干越有兴趣。\n除了分配任务，mt经常会给我一些“职场道理”和心里话。有一两次我在工作上出了差错，杰哥没有批评我，而是纠正，默认以“经验不足”来包容我。杰哥对待任何事都是精益求精的，刚来的一个月杰哥经常会让制作一些监控图面板，当时我做好面板交付给杰哥，杰哥总是“不满意”，然后修改很多，导致我有些“破防”。后来我明白，在职场里的事无巨细其实是对自己的一种自保，因为做事马虎，往往出问题时自己会“背锅”。\n另外杰哥也经常告诉我做事要有逻辑性，要培养自己的思维能力。AI时代，普通人在技术上已经不分上下，最重要的是做一件事要有自己的思路，要有自己的逻辑。就像人人都能用AI，但是用的好不好得看prompt是什么。\nSRE思维SRE思维的培养，我想特别感谢我的leader。\n有次偶然的机会翻到了leader的blog，leader的SRE运维之路可以用“传奇”来形容。他在上家公司，做到了全公司一号运维，各种奖拿了个遍。现在我所处的团队也是他一手带起来的，在去年还拿到了“卓越团队”的荣誉称号。（我以后也要像他一样优秀哇）\nleader在SRE运维领域颇有造诣，在团队的知识库里，我学到了很多。特别是在变更管理、故障管理、资源盘点管理以及各种制度规范。我觉得很多地方都是网校可以参考的。我认为最值得学习的是变更管理和资源盘点管理。对于变更管理，团队对变更有着严格的规范，要求每在CD平台上发布，都需要有对应的变更单。这方面我也参与了变更合规审查的自动化判别任务，不过因为不合规情况种类太多，自己能力有限（太菜），这块最终没有完美实现……\n还有就是资源盘点管理，团队把所属资源分为了系统、模块和组件三类，然后将分类情况展示在自研运维平台护航（Grafana），我觉得做资源盘点是十分有必要的，因为身为运维，你必须要知道自己运维了什么。\n白屏化也是一个值得学习的地方。团队会写一些带参数的自动化py或shell脚本，放在git上，利用Jenkins来实现脚本执行。Jenkins只需要设置好一些参数，SRE只关注参数值就可以，然后一键就能执行脚本。\n最有意思的是团队自研的运维平台——护航平台。它是对Grafana的二次开发，团队把它真的是玩到了极致，它不仅有基础的可观测监控图、告警能力，还可以实现低代码功能（business table插件、infinity插件），变更管理就是完全在护航平台上实现的。SRE只需要点点点就可以实现对数据库的增删查改功能，扩展性特别强。我在实习期间就做了很多监控图，还有基于business form的操作表单等等。\n另外在实习中我发现，在公司里，业务是第一，下来才是技术。一种技术，只有把它产品化、业务化后，才能发挥出它的价值。看过leader的blog，他就是一个善于把技术产品化的“大牛”，他在高中时就能通过当站长来实现盈利。我想对于一个程序员来说，要想在公司里干的更好，就必须要实现从技术到产品或管理的转变。\n最后是SRE的精髓——稳定性。团队所有的任务都是为了维护稳定性，稳定性意识是SRE必须所拥有的。在这里，我知道了发生故障第一时间应该先止损，然后通告，最后再排查。并且每次故障后，必须要开展复盘工作，写CaseStudy。工位旁还张贴了“稳定性军规”，几条稳定性保障操作已经深深刻在我的潜意识里。\n虽然整个实习过程中没有太多的debug经历，但是我觉得这些制度规范的宝贵经验有时候比debug更有价值。\n环境篇公司环境、福利都特别好。\n工作：我在百度科技园实习，这里一共5栋楼，是百度新修的工区。园区就像一个“大学”一样，各种设施样样俱全：健身房、乒乓球馆、食堂、超市、理发馆、咖啡馆什么都有。工作之外还会经常举办一些活动，很丰富。\n饮食：百度的食堂挺好吃的，价格相较重庆能贵点，一顿饭15到20左右。种类很多，而且会定期出新品，到现在我还没在公司点过外卖。早餐是免费的，鸡蛋包子馄饨都有，不过早餐不怎么变花样，吃多了还是有点乏味。\n​​​​​\n福利：公司每天下午都会提供下午茶：水果或者酸奶，每层工区也有茶水间，有免费的咖啡，茶饮等等。内部的零售柜卖的东西也比较便宜，一听可乐不到2元。实习这段时间我还第一次接触了健身，第一次进健身房还做了好几天的心理建设。健身成果的话算是有点痕迹，主要我也是健身玩玩，没有特别focus。\n​​​\n在公司也度过了自己的22岁生日，收到了专属生日生肖小度熊。\n​\n还有中秋礼包\n​\n1024程序员节这天还抽到了大奖：0.1g金币\n​\n生活篇其实我中考完的暑假来过一次北京，当时是和妈妈一起来旅游的。没想到2025年我还能再次来到北京，不过这次是以“打工人”身份来的，我称这次经历是“北漂”。\n住我在北京昌平区沙河高教园住，这里虽说有点远，但是交通还算便利，有“牛马”专线——昌平线。我记得第一天去上班，不知道沙河高教园这站有空车，硬是挤上了摩肩接踵的早高峰。昌平线时刻都是满的，无论是工作日还是周六周天。下班的话，公司也有专门开到小区门口的班车，挺好。\n我租的这个地方算是“北漂人”的最好归宿，特别是在西二旗上班的打工人。这里房租便宜，而且还是单间，还带独立卫浴，一个人住的话很舒服。周围基础生活设施都有，什么超市呀、小饭馆的都有。主要这里有很多所大学，相比密云、房山，这里算北京城内了。（其实就是郊区）。\n玩实习这段时间也去了一些地方走走。\n实习期间刚好碰上九三阅兵，在天安门封场前一天，我特意骑了个共享单车从天安门前经过。特别宏伟，长安街特别宽，骑车有种“踏实”的感觉。\n\n去了鼓楼、什刹海那一片，尝了尝包子炒肝和豆汁，不得不说豆汁确实难喝，一股变质的酸味。鼓楼算个打卡的地方，挺出片。什刹海风景也不错，适合散步。\n\n去了国贸三里屯那片，这里是北京CBD，遍布高楼，楼高到仰脖看看不到顶。我当时一个人去，拿一杯咖啡，在街区走走，还有种都市白领的感觉。\n​\n去了北京动物园，看了心心念念的“西直门三太子”——萌兰，太可爱了。北京的秋天也特别美，树上一片红黄绿，像水彩画一样。\n​\n哦国庆还去了天津，感受了松弛的天津文化，去了渤海边，第一次看海👀\n​\nTODO个人规划其实到现在还没决定好要考研还是要就业，这段实习的目的不仅是为了把自己在大一大二学到的运维知识做一个实战应用，更多的其实是给自己留一个“保底”，万一考研失败了还能有段实习经历，就业不至于“零基础”。\n说实话其实还是很想再找一段实习，因为在实习的时候真的是在做自己喜欢的事情，而且对于实习生而言，公司其实就像个大学一样，没有正职的压力，还可以享受到公司的福利，并且有工资拿，很不错。唉，但是我回去之后还是得好好沉淀，感觉自己能力还是不够。下一段实习很想去腾讯或者字节，看上天安排吧……\n代码能力说实话还是得提升自己的代码能力，特别是Go语言。现在还是比较依赖AI，好在我能知道怎么让AI实现我的需求，AI写的我能知道哪里需要哪里不需要。不过打铁还需自身硬，回去我还是要好好学下Go语言。\n还有一个是Shell脚本能力，这个对于我们干运维这行来说是必不可少。在实习过程中确实感受到了，很多场景都需要这个工具来提高效率。\n运维方面第一个是中间件的运维学习。在实习之前，只是接触过MySQL，pg这类数据库的运维，甚至Redis都只是会搭建部署。实习时发现，作为运维，有很多中间件都需要学习，像什么Kafka、es、etcd、prometheus等等。\n第二个是K8s。现在还是仅仅会用K8s，有时间确实要学学K8s的底层原理，特别是各个组件的工作原理。\n第三个是网络。计算机网络对于运维来说十分重要，这方面也一直是我的弱点。有机会一定要好好提升一下网络debug能力。\n最后一个是eBPF和Linux内核。一直说想学学Linux Kernel，但是总是搁置了。还有eBPF也是，未来打算用eBPF做一个限流工具，然后作为插件接入我的IPBlock。\n‍\n‍\n","categories":["Blogs"],"tags":["Blogs"]},{"title":"利用通义灵码实现我的第一次开源贡献","url":"/posts/16924.html","content":"\n## 结缘开源\n\n最早了解开源是从学校的兴趣组织开始的。2023年10月21日，openSUSE亚洲峰会在我们学校召开，这次会议汇聚了许多来自openSUSE社区贡献者以及对开源感兴趣的爱好者们。我第一次知道有这么多志同道合的爱好者在进行开源贡献，他们以个人兴趣为驱动力共同维护护着一个社区。这次峰会激发了我对开源社区的兴趣，一颗种子也开始埋在我的心中——我也想要做一次开源贡献！\n\n2024年6月24日，我上完课准备回宿舍，刚好在必经之路上看到“2024天池云原生编程挑战赛”的宣传海报。我一看居然是阿里云举办的，回到宿舍后就立马仔细查看了详细的比赛信息。”赛道3：用通义灵码,人人都是开源贡献者“很符合我的兴趣，我也特别想能有一个被Merged的PR。赛道3有很多个选题，结合我自己学习过云原生的一些知识，我最终选择了Higress开源项目，这是阿里云自己开发的API网关，它和Nginx、Traefik很类似，但是它增加了许多新的功能，并且更聚焦于云原生环境中的服务网格。尤其是它有丰富的插件，特别是与AI有关的插件，使得用户可以很轻易的拥有一个属于自己的AI。\n\n参加比赛因为Higress这个开源项目是开放式赛题，浏览了当时的issues，因为接触过CI&#x2F;CD，并且懂得Kubernetes和Docker的一些操作，所以我最终选择了这个issue：支持通过 GitHub Actions 来构建和发布 Wasm 插件镜像 · Issue #1052 · alibaba&#x2F;higress\n\n通义灵码体验虽然接触过CI&#x2F;CD，但是我从来没有在Github上实现过它，也不会Github Action的语法格式。通义灵码给了我很大的帮助。我按照通义灵码安装文档将它安装到了我的VS Code中，接下来就是愉快的使用环节了。\n在使用通义灵码的过程中，我先复制了一份写好的Github Action Workflow，利用通义灵码的解释代码和直接对话功能，我很快地了解了Github Action的基本语法以及格式。\n\n\n然后我就开始编写issue所需要的Github Action Workflow了。\n其中代码优化建议功能帮了我很大的忙，我只需要把有疑问的代码勾选，然后再执行/generate optimization，通义灵码就能很方便的为我指出错误，并且提出建议。在以往的话我还需要先把代码复制下来，然后再抛给AI，有时还得附带上下文。\n\n提交PR写好Github Action Workflow，我提交了PR。但是这个PR不是一次性就被Merge的。这其中我和项目负责人进行了很多有趣的讨论。提交PR的那段时间刚好是我的考试周，每天复习之余，最上心的事情就是查看邮箱，看看负责人有没有给我留言，每次收到邮件我都会十分激动，因为他真的在看我的贡献，真的在为我的贡献提出建议！\n\n为了这份脚本能够成功执行，我进行了很多次的测试。而且在测试的过程中，我甚至也找出了官方文档存在的一些问题。我还提了一个自己发现的issue！按照Wasm 插件镜像规范构建出的镜像不可用 · Issue #1100 · alibaba&#x2F;higress (github.com)\n2024年7月8日，我进行了最后一次测试。我写了十分详细的测试文档，然后提交。直到我看到负责人给我评论了”LGTM(Looks Good To Me)”，我高兴极了。很快他Merge了我的PR。就这样我得到了第一个属于自己的Merged PR。\n\n参赛心得技术提升通过这次比赛，我学会了Github Action Workflow的基本语法和编写格式，并且对CI&#x2F;CD在企业中的应用有了更进一步的了解。\n并且我还上手了阿里云开发的新一代API网关Higress，体验了它丰富的功能，感受到了它在云原生环境下的强大实力\n开源氛围Higress是一个活跃的开源社区，在这个社区中大家都积极的分享自己的知识和idea，乐于寻找bug。这种互帮互助，乐于分享的活跃氛围大大激发了我的开源兴趣。\n个人体会这次比赛不仅提升了我的技术，还让我学会了如何沟通、如何解决问题等等。我收获很多，相信在以后我会对开源社区做更多高质量的贡献，争取在多几个PR~\n致谢十分感谢阿里云以及天池为我们广大开源爱好者搭建了一个广阔无垠的舞台，让我们有机会将梦想转化为现实，将创意付诸实践。\n同时也要感谢项目的相关工作人员，无论是对于技术上的指导和建议，还是比赛资料的统计以及奖品的组织发放，大家都辛苦了！\n未来的路还很长，但我相信，在阿里云与天池的陪伴下，我们将携手共进，共创辉煌。\n","categories":["Blogs"],"tags":["Achievements","Blogs"]},{"title":"MariaDB Galera错误的更改配置方式导致集群无法拉起","url":"/posts/52501.html","content":"\n\n故障描述sre-tools-database 命名空间下由 MariaDB Operator 管理的 Galera 集群包含 3 个 Pod 实例。在一次配置变更过程中，将对应的 StatefulSet 直接缩容至 0，随后再扩容回 3。\n扩容后，三个 Pod 均无法进入 Running 状态，集群节点相互等待（死锁），始终无法形成 Primary View，最终因连接超时而启动失败。\nmariadb-galera-0日志...2025-12-21 14:10:02 0 [Note] WSREP: gcomm: connecting to group &#x27;mariadb-operator&#x27;, peer &#x27;mariadb-galera-0.mariadb-galera-internal.sre-tools-database.svc.cluster.local:,mariadb-galera-1.mariadb-galera-internal.sre-tools-database.svc.cluster.local:,mariadb-galera-2.mariadb-galera-internal.sre-tools-database.svc.cluster.local:&#x27;2025-12-21 14:10:02 0 [Note] WSREP: (be2303ad-b24e, &#x27;tcp://0.0.0.0:4567&#x27;) Found matching local endpoint for a connection, blacklisting address tcp://100.112.98.78:45672025-12-21 14:10:05 0 [Note] WSREP: EVS version upgrade 0 -&gt; 12025-12-21 14:10:05 0 [Note] WSREP: PC protocol upgrade 0 -&gt; 1[*]2025-12-21 14:10:05 0 [Warning] WSREP: no nodes coming from prim view, prim not possible[*]2025-12-21 14:10:05 0 [Note] WSREP: view(view_id(NON_PRIM,be2303ad-b24e,1) memb &#123;be2303ad-b24e,0&#125; joined &#123;&#125; left &#123;&#125; partitioned &#123;&#125;)2025-12-21 14:10:06 0 [Warning] WSREP: last inactive check more than PT1.5S ago (PT3.50146S), skipping check2025-12-21 14:10:35 0 [Note] WSREP: PC protocol downgrade 1 -&gt; 02025-12-21 14:10:35 0 [Note] WSREP: view((empty))[*]2025-12-21 14:10:35 0 [ERROR] WSREP: failed to open gcomm backend connection: 110: failed to reach primary view: 110 (Connection timed out)at ./gcomm/src/pc.cpp:connect():1602025-12-21 14:10:35 0 [ERROR] WSREP: ./gcs/src/gcs_core.cpp:gcs_core_open():221: Failed to open backend connection: -110 (Connection timed out)2025-12-21 14:10:36 0 [ERROR] WSREP: ./gcs/src/gcs.cpp:gcs_open():1674: Failed to open channel &#x27;mariadb-operator&#x27; at &#x27;gcomm://mariadb-galera-0.mariadb-galera-internal.sre-tools-database.svc.cluster.local,mariadb-galera-1.mariadb-galera-internal.sre-tools-database.svc.cluster.local,mariadb-galera-2.mariadb-galera-internal.sre-tools-database.svc.cluster.local&#x27;: -110 (Connection timed out)[*]2025-12-21 14:10:36 0 [ERROR] WSREP: gcs connect failed: Connection timed out2025-12-21 14:10:36 0 [ERROR] WSREP: wsrep::connect(gcomm://mariadb-galera-0.mariadb-galera-internal.sre-tools-database.svc.cluster.local,mariadb-galera-1.mariadb-galera-internal.sre-tools-database.svc.cluster.local,mariadb-galera-2.mariadb-galera-internal.sre-tools-database.svc.cluster.local) failed: 7[*]2025-12-21 14:10:36 0 [ERROR] Abortingroot@Redrock-ButterBeer:/var/lib/mysql#\n\n故障原因分析**直接原因：**不当的运维操作导致 Galera 集群状态完全丢失在修改数据库配置时，直接编辑了下游的 ConfigMap，并试图通过 将 StatefulSet 缩容至 0 再扩容 的方式强制使配置生效。\n该操作等同于同时下线所有Galera节点，即终止集群。\n\n@https://mariadb.com/docs/galera-cluster/galera-management/installation-and-deployment/getting-started-with-mariadb-galera-cluster#restarting-the-cluster \nIf you shut down all nodes at the same time, then you have effectively terminated the cluster. Of course, the cluster’s data still exists, but the running cluster no longer exists. When this happens, you’ll need to bootstrap the cluster again.\n\n导致\n\n集群中不存在任何存活节点可作为 Primary Component\n没有节点持有可用于恢复的有效集群视图（View）\n所有节点在启动时都在等待其他节点先行加入，形成”相互等待”的死锁状态\n\n最终集群无法自动选主，全部节点启动失败。\n深层原因\n**绕过了Operator的声明式管理逻辑：**没有意识到由Operator管理的资源，应该通过修改上层CR的Spec来触发Operator的Reconcile逻辑，而不是直接修改干预下层资源（ConfigMap、StatefulSet），导致 Operator 无法自动识别集群的故障状态并触发内置的故障恢复逻辑。\n**缺乏测试验证：**进行高风险破环性更新（即同时停止所有数据库实例）前应该先测试，违反了运维操作规范。不过幸好有备份。\n**容灾建设不完善：**没有集群恢复的标准SOP，对于故障发生后也只能依赖临时排障+文档查阅，恢复成本高，时间周期长。\n\n故障发生时间线\n2025&#x2F;12&#x2F;21 21:40\n由于 SRE_center 数据库的 change_info 表在插入中文数据后显示为拉丁字符，排查发现 character-set-server 为 latin，因此决定调整为 utf8mb4。\\n通过以下命令修改 MariaDB Galera 的配置 ConfigMap：\nkubectl edit cm -n sre-tools-database mariadb-galera-config \n新增配置内容：\n[mariadb]character-set-server=utf8mb4 collation-server=utf8mb4_general_ci \n\n\n\n2025&#x2F;12&#x2F;21 21:50\n修改配置后，对 mariadb-galera StatefulSet 进行操作，直接缩容至 0，再扩容至 3。\\n新启动的 Pod 无法正常进入 Running 状态，Pod 日志中报错（详见故障描述）。\\n3 个 Pod 无法完成选主，集群进入死锁状态。\n\n\n\n\n2025&#x2F;12&#x2F;21 22:00\n查阅官方文档及相关资料后确认：\\n当 Galera 集群被整体终止后，需要人工介入重新引导集群。\n恢复思路为：\n\n先启动一个节点作为 Most Advanced Node\n再由其他节点加入该集群\n\n因此将 StatefulSet 缩容至 1，尝试启动 Pod 0。\n\n\n\n\n2025&#x2F;12&#x2F;21 23:44\n操作时先暂停Operator对MariaDB集群的管理\nkubectl patch mariadb mariadb-galera -n sre-tools-database \\  --type merge -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;suspend&quot;:true&#125;&#125;&#x27;\n\n通过修改 Pod 0 对应 PV 中 /var/lib/mysql/grastate.dat 文件，将 safe_to_bootstrap 设置为 1，并在 StatefulSet 中为 mariadb 容器添加启动参数 mariadbd --wsrep_new_cluster，成功使 Pod 0 正常启动。\n# GALERA saved stateversion: 2.1uuid:    0a9026bb-de80-11f0-aa0c-7ec0fff494fdseqno:   -1safe_to_bootstrap: 1\n\n\n\n2025&#x2F;12&#x2F;21 23:50\n对 Pod 1、Pod 2 也采用了相同方式进行启动。\\n但该操作会导致每个 Pod 都被视为独立集群，形成集群脑裂，不符合主从架构。\nMariaDB [(none)]&gt; SHOW STATUS LIKE &#x27;wsrep_cluster_size&#x27;;+--------------------+-------+| Variable_name      | Value |+--------------------+-------+| wsrep_cluster_size | 1     |+--------------------+-------+1 row in set (0.001 sec)MariaDB [(none)]&gt;\n\n\n\n2025&#x2F;12&#x2F;22 00:01\n删除 Pod 1、Pod 2 对应 PV 中的 grastate.dat 文件，\\n并移除 StatefulSet 中的 --wsrep_new_cluster 启动参数，随后重新启动这两个 Pod。\nPod 正常 Running 后，检查集群状态：\nMariaDB [(none)]&gt; SHOW STATUS LIKE &#x27;wsrep_cluster_size&#x27;;+--------------------+-------+| Variable_name      | Value |+--------------------+-------+| wsrep_cluster_size | 3     |+--------------------+-------+1 row in set (0.001 sec)MariaDB [(none)]&gt;\n\n恢复Operator对MariaDB的管理\nkubectl patch mariadb mariadb-galera -n sre-tools-database \\    --type merge -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;suspend&quot;:false&#125;&#125;&#x27;\n\n集群恢复正常，服务功能恢复。\n\n\n解决方式见MariaDB Galera容器集群恢复方法\n改进措施\n如果是Operator管理的资源，请通过修改对应CR（Custom Resource）来修改某些配置，避免Operator与人工操作发生冲突。\n永远不要把Galera集群StatefulSet的replicas设置为0，另外也禁止缩容操作。\nGalera集群必须保证(N&#x2F;2) + 1在线。3节点必须保证2个节点在线，否则将无法完成选主并进入不可用状态。\n优先使用滚动更新kubectl rollout restart，避免一次性中断所有节点。\n某些破坏性变更应该先进行测试，操作时要想清楚会发生什么后果。\n关于运维操作（如配置修改、扩缩容、重启）应做好 操作记录与时间点标记（ButterBeer已打开history时间记录），便于事后审计与问题回溯。\n\n\n\nOperator原理\nhttps://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/operator/\n\n\nMariaDB grastate.dat\nMariaDB集群引导：https://mariadb.com/docs/galera-cluster/high-availability/resetting-the-quorum-cluster-bootstrap#find-the-most-advanced-node\nMariaDB恢复：https://mariadb.com/docs/galera-cluster/high-availability/understanding-quorum-monitoring-and-recovery#recovering-from-a-full-cluster-shutdown\n\n\n","categories":["Trouble Shooting"],"tags":["Trouble Shooting"]},{"title":"MariaDB Galera容器集群恢复方法","url":"/posts/52500.html","content":"\n\n\n\n\n\nMariaDB集群引导：https://mariadb.com/docs/galera-cluster/high-availability/resetting-the-quorum-cluster-bootstrap#find-the-most-advanced-node\nMariaDB恢复：https://mariadb.com/docs/galera-cluster/high-availability/understanding-quorum-monitoring-and-recovery#recovering-from-a-full-cluster-shutdown\n\n\n当集群出现崩溃时，可以通过下面的方法来恢复集群。\n整体思路\n先启动一个节点作为 Most Advanced Node\n再由其他节点加入该集群\n\n操作步骤手动指定一个Pod为Most Advanced Node暂停Operator对集群的接管kubectl patch mariadb mariadb-galera -n sre-tools-database \\    --type merge -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;suspend&quot;:true&#125;&#125;&#x27;\n\n将StatefulSet的Replica指定1将 StatefulSet 的 replicas 缩容至 1，仅保留一个候选节点。\\n由于该 Pod 使用的是崩溃前的 PVC（safe_to_bootstrap此时为0），其 Galera 状态无法形成 Primary View，通常无法正常启动，需要人工引导。\nkubectl scale sts mariadb-galera -n sre-tools-database --replicas=1\n\n在StatufulSet中为mariadb容器添加启动参数mariadbd --wsrep_new_cluster，添加在args或command里。\n修正grastate.dat因为Pod会重启，难以修改文件。因此我们可以直接找到Pod /var/lib/mysql所挂载的PV，直接在PV中修改文件。\n我们这里先修改Pod 0的。\nroot@Redrock-ButterBeer:~# kubectl describe pv pvc-ada10c7d-f72d-4059-a5bc-2158ca7c34ecName:              pvc-ada10c7d-f72d-4059-a5bc-2158ca7c34ecLabels:            &lt;none&gt;Annotations:       local.path.provisioner/selected-node: redrock-butterbeer                   pv.kubernetes.io/provisioned-by: rancher.io/local-pathFinalizers:        [kubernetes.io/pv-protection]StorageClass:      local-path-performanceStatus:            BoundClaim:             sre-tools-database/storage-mariadb-galera-0Reclaim Policy:    RetainAccess Modes:      RWOVolumeMode:        FilesystemCapacity:          100GiNode Affinity:       Required Terms:      Term 0:        kubernetes.io/hostname in [redrock-butterbeer]Message:           Source:    Type:          HostPath (bare host directory volume)    [*]Path:          /database/pvc-ada10c7d-f72d-4059-a5bc-2158ca7c34ec_sre-tools-database_storage-mariadb-galera-0    HostPathType:  DirectoryOrCreateEvents:            &lt;none&gt;\n\n将safe_to_bootstrap设置为1\n\nGalera 是一个全同步集群。为了保证数据绝对一致，它必须知道：在集群关闭或崩溃时，哪一个节点是最后离开的？（因为最后一个离开的节点拥有最完整、最新的数据）。\n\n当 safe_to_bootstrap: 0 时：节点认为”我可能不是最后一个关闭的，或者我不确定我的数据是不是最新的”。为了防止数据丢失，它会拒绝独自启动，因为它怕自己启动后变成了”老大”，导致其他数据更新的节点被它覆盖。\n当 safe_to_bootstrap: 1 时：节点认为”我是最后一个关闭的，我拥有最全的数据”。它被允许作为引导节点（Bootstrap Node）来开启一个全新的集群。\n\n\nroot@Redrock-ButterBeer:/database/pvc-ada10c7d-f72d-4059-a5bc-2158ca7c34ec_sre-tools-database_storage-mariadb-galera-0/storage# cat grastate.dat # GALERA saved stateversion: 2.1uuid:    0a9026bb-de80-11f0-aa0c-7ec0fff494fdseqno:   -1safe_to_bootstrap: 0\n\n手动重启Pod 0删除Pod 0，让其重启。预期情况是Pod 0正常Running。\nkubectl delete pod -n sre-tools-database mariadb-galera-0\n\n其他节点加入集群删除启动参数在StatefulSet中取消mariadbd --wsrep_new_cluster参数\n在Pod1，Pod2对应的PV中删除grastate.dat删除该文件，清除错误状态。注意这两个Pod不要设置safe_to_bootstrap&#x3D;1，否则这两个节点都会认为自己是主节点。\n当节点成功加入集群以后，这个配置文件会自动生成。\n扩容Stateful到3kubectl scale sts mariadb-galera -n sre-tools-database --replicas=3\n\n检查Pod的状态，预期状态为3个Pod正常Running。\n恢复Operator对集群的接管kubectl patch mariadb mariadb-galera -n sre-tools-database \\    --type merge -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;suspend&quot;:false&#125;&#125;&#x27;\n\n至此集群恢复完毕\nCheckList\n检查集群节点数量：在Pod 0执行入选SQL，如果为3证明集群成功重建。\nSHOW STATUS LIKE &#x27;wsrep_cluster_size&#x27;;-- 预期为3\n检查同步状态\nSHOW STATUS LIKE &#x27;wsrep_local_state_comment&#x27;;-- 预期为 &#x27;Synced&#x27;\n检查3个Pod的日志是否有Ready to connection\n\n检查依赖此数据库的服务是否正常运行\n\n\n","categories":["Trouble Shooting"],"tags":["Trouble Shooting"]},{"title":"Redis主从同步实验","url":"/posts/18658.html","content":"\n# 需求\n\n\nA：master，不加密（普通端口 6379）\nB：slave，从 A 同步，不加密（端口 16379），同时监听 TLS 加密端口 16380\nC：slave，从 B 同步，通过 TLS（端口 16380）\n\n部署生成自签名TLS证书 使用openssl生成自签名证书mkdir -p certs &amp;&amp; cd certs# 创建 CAopenssl genrsa -out ca.key 4096openssl req -x509 -new -nodes -key ca.key -sha256 -days 3650 -out ca.crt -subj &quot;/CN=Redis-CA&quot;# 创建 server key 和证书签名请求（CSR）openssl genrsa -out redis.key 2048openssl req -new -key redis.key -out redis.csr -subj &quot;/CN=redis-server&quot;# 使用 CA 签发 server 证书openssl x509 -req -in redis.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out redis.crt -days 3650 -sha256# 权限设置chmod 644 redis.crt redis.key ca.crt\n\nCA相关证书\n\nca.key：CA的私钥，用来签发证书（给redis.crt签名）\nca.crt：CA的公钥证书，用来让客户端验证某个证书（比如redis.crt）是不是CA签发的，客户端拿redis.crt来验证时，会用这个公钥解签名。\n\nRedis服务器相关证书\n\nredis.key：Redis 服务端的私钥，用来解密客户端发来的数据、以及在 TLS 握手时证明”我是 Redis 服务端”。\nredis.csr：证书签名请求（Certificate Signing Request）包含 Redis 服务器的公钥 + 服务器身份信息（域名、组织等），生成 CSR 的目的是交给 CA 签名，生成一个正式证书。\nredis.crt：Redis 服务器的证书\n\nRedis 配置文件port 6379tls-port 6380tls-cert-file /certs/redis.crttls-key-file /certs/redis.keytls-ca-cert-file /certs/ca.crttls-auth-clients no# 作为slave，同步redis-areplicaof redis-a 6379\n\n# 关闭非加密 TCP 端口port 0tls-port 6380tls-cert-file /certs/redis.crttls-key-file /certs/redis.keytls-ca-cert-file /certs/ca.crt# 单向TLStls-auth-clients no# 从加密端口同步 redis-breplicaof redis-b 6380# 允许主从复制也走 TLS 加密通道tls-replication yes\n\ndocker-compose启动三个redis容器\nredis-a作为master，暴露容器端口6379到6379\nredis-b作为slave，暴露容器端口6379到16379，容器端口6380到16380（TLS）\nredis-c作为slave，暴露容器端口6380到26380（TLS）\nversion: &#x27;3.8&#x27;services:  redis-a:    image: redis:latest    container_name: redis-a    ports:      - &quot;6379:6379&quot;    volumes:      - ./data/redis-a:/data    command: [&quot;redis-server&quot;, &quot;--port&quot;, &quot;6379&quot;]    networks:      - redis-net  redis-b:    image: redis:latest    container_name: redis-b    ports:      - &quot;16379:6379&quot;      - &quot;16380:6380&quot;    volumes:      - ./data/redis-b:/data      - ./certs:/certs      - ./redis-b.conf:/usr/local/etc/redis/redis.conf    command: [&quot;redis-server&quot;, &quot;/usr/local/etc/redis/redis.conf&quot;]    networks:      - redis-net    depends_on:      - redis-a  redis-c:    image: redis:latest    container_name: redis-c    ports:      - &quot;26380:6380&quot;    volumes:      - ./data/redis-c:/data      - ./certs:/certs      - ./redis-b.conf:/usr/local/etc/redis/redis.conf    command: [&quot;redis-server&quot;, &quot;/usr/local/etc/redis/redis.conf&quot;]    networks:      - redis-net    depends_on:      - redis-bnetworks:  redis-net:    driver: bridge\n\n验证查看主从状态目标：\n\nB 是 A 的从节点\nC 是 B 的从节点（通过 TLS）\n\n查看 Redis B 是否同步 Redis A（非 TLS）可以看到b以a作为master，master_link_status:up 说明同步正常\nroot@liyixiong01:~/redis# docker exec -it redis-b redis-cli -p 6379 info replication# Replicationrole:slavemaster_host:redis-amaster_port:6379master_link_status:up\n\n查看 Redis C 是否通过 TLS 同步 Redis Broot@liyixiong01:~/redis# docker exec -it redis-c redis-cli   --tls   --cert /certs/redis.crt   --key /certs/redis.key   --cacert /certs/ca.crt   -p 6380 info replication# Replicationrole:slavemaster_host:redis-bmaster_port:6380master_link_status:up\n\n验证数据同步链路是否真实工作在 Redis A 写入数据root@liyixiong01:~/redis# docker exec -it redis-a redis-cli -p 6379 set hello worldOK\n\n在 Redis B 查看是否能读到（非TLS）root@liyixiong01:~/redis# docker exec -it redis-b redis-cli -p 6379 get hello\\&quot;world&quot;\n\n在 Redis C 查看是否能读到（TLS）root@liyixiong01:~/redis# docker exec -it redis-c redis-cli \\  --tls \\  --cert /certs/redis.crt \\  --key /certs/redis.key \\  --cacert /certs/ca.crt \\  -p 6380 get hello&quot;world&quot;","categories":["服务器运维"],"tags":["服务器运维","Redis"]},{"title":"运维常用工具","url":"/posts/22140.html","content":"\n## Harbor\n\n\nHarbor (goharbor.io)\n\n尽管Docker官方提供了公共的镜像仓库DockerHub，但从安全性和稳定性等方面考虑，部署私有镜像仓库是非常有必要的。Harbor是一个由VMware公司开源的企业级的Docker Registry管理项目，是我们搭建私有镜像仓库的不二之选。\n整体架构\n如上图所示是 Harbor 2.0 的架构图，从上到下可分为代理层、功能层和数据层。\n\n代理层：代理层实质上是一个 Nginx 反向代理，负责接收不同类型的客户端请求，包括浏览器、用户脚本、Docker 等，并根据请求类型和 URI 转发给不同的后端服务进行处理。\n功能层：\nPortal：是一个基于 Argular 的前端应用，提供 Harbor 用户访问的界面。\nCore：是 Harbor 中的核心组件，封装了 Harbor 绝大部分的业务逻辑。\nJobService：异步任务组件，负责 Harbor 中很多比较耗时的功能，比如 Artifact 复制、扫描、垃圾回收等。\nDocker Distribution：Harbor 通过 Distribution 实现 Artifact 的读写和存取等功能。\nRegistryCtl：Docker Distribution 的控制组件。\nNotary（可选） ：基于 TUF 提供镜像签名管理的功能。\n扫描工具（可选） ：镜像的漏洞检测工具。\nChartMuseum（可选） ：提供 API 管理非 OCI 规范的 Helm Chart，随着兼容 OCI 规范的 Helm Chart 在社区上被更广泛地接受，Helm Chart 能以 Artifact 的形式在 Harbor 中存储和管理，不再依赖 ChartMuseum，因此 Harbor 可能会在后续版本中移除对 ChartMuseum 的支持。\n\n\n数据层：\nRedis：主要作为缓存服务存储一些生命周期较短的数据，同时对于 JobService 还提供了类似队列的功能。\nPostgreSQL：存储 Harbor 的应用数据，比如项目信息、用户与项目的关系、管理策略、配置信息、Artifact 的元数据等等。\nArtifact 存储：存储 Artifact 本身的内容，Artiact就是每次推送镜像、Helm Chart 时，数据最终存储的地方。默认情况下，Harbor 会把 Artifact 写入本地文件系统中。用户也可以修改配置，将 Artifact 存储在外部存储中，例如阿里云的对象存储 OSS 等等。\n\n\n\nHarbor的安装Harbor安装示范\nHarbor的使用基本推送与拉取先登录\ndocker login https://harbor.yiiong.top:8443\n\n镜像推送（先打标签，再推送）\nharbor地址&#x2F;项目名&#x2F;镜像名:版本\ndocker tag mysql harbor.yiiong.top:8443/library/mysql:latest\n\n镜像拉取\ndocker pull harbor.yiiong.top:8443/library/mysql:latest\n\n机器人账户在一些情况下，为了提高 Harbor 仓库的安全性考虑，不建议直接在流水线任务（CI&#x2F;CD）中直接使用个人账户。因为这种方式会导致权限管理和维护变得复杂，尤其是当项目是公开的时候。\n为解决这个问题，可以考虑使用 Harbor 自带的机器人账户（Robot Account）。机器人账户是一种特殊类型的账户，允许非个人实体访问 Harbor 仓库并执行特定的操作，而无需提供实际的用户名和密码。通过机器人账户，可以实现自动化 CI&#x2F;CD 流程中的推送和拉取镜像操作，确保安全同时减少人为处理。\n内网穿透工具frp\nGithub\n文档 | frp (gofrp.org)\n\nfrp 是一款高性能的反向代理应用，专注于内网穿透。它支持多种协议，包括 TCP、UDP、HTTP、HTTPS 等，并且具备 P2P 通信功能。使用 frp，我们可以使用frp安全、便捷地将内网服务暴露到公网，通过拥有公网 IP 的节点进行中转。\n在0.52.0版本开始支持toml格式的配置文件了，并将在后继某个版本开始取消对ini配置格式的支持。\n通过SSH访问内网机器利用frp将处于内网香橙派中的服务穿透至公网_proxy.go:204-CSDN博客\n通过自定义域名访问内网的 Web 服务利用frp将处于内网香橙派中的服务穿透至公网_proxy.go:204-CSDN博客\n后台运行一切都没有问题后，我们把它们制作成service，保证它们可以在后台稳定运行\n服务端\nvim /usr/lib/systemd/system/frps.service\n\nfrps.service配置如下\n[Unit]Description=frps Server ServiceAfter=network.targetWants=network.target[Service]Type=simpleExecStart=/root/frp-all/frp-os-server/frps -c /root/frp-all/frp-os-server/frps.toml # 你的frps文件的绝对路径[Install]WantedBy=multi-user.target\n\n启动\nsystemctl start frps\n\n开机自启\nsystemctl enable frps\n\n查看日志\nsystemctl status frps\n\n客户端\n与服务端同理，frpc.service的配置中ExecStart修改为frpc文件的绝对路径，下面操作相同\n原理\nfrpc并不监听哪个端口，只是它在局域网主机中启动时会通过一个随机端口来与外网服务器的7010端口建立一个隧道连接，隧道建立后7010端口和那个frpc所用过的随机端口并不直接参与数据包的传输。frps会将发往外网服务器6010端口的数据直接插入隧道中传至frpc中，这是隧道技术的一个关键特性，允许数据在网络上高效、安全地传输。\nngrok\nDownload (ngrok.com)\n\nngrok 通过在公共端点和本地运行的 Web 服务器之间建立一个免费且安全的通道，实现内网主机的服务可以暴露给外网。\n你可以使用ngrok将你处于内网的服务暂时暴露在公网，方便测试服务。\n先添加Authtokenngrok config add-authtoken &lt;token&gt;开启隧道ngrok http &lt;port&gt;\n\nNginx介绍Nginx是一款轻量级的Web 服务器&#x2F;反向代理服务器及 电子邮件（IMAP&#x2F;POP3）代理服务器。其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。\n正向代理与反向代理正向代理\n正向代理，指的是通过代理服务器 代理浏览器/客户端去重定向请求访问到目标服务器 的一种代理服务。正向代理服务的特点是代理服务器 代理的对象是浏览器/客户端，也就是对于目标服务器 来说浏览器/客户端是隐藏的。\n正向代理有什么用途？\n举个例子：我是一个用户，我访问不了某网站，但是能访问一个代理服务器，这个代理服务器能访问那个我不能访问的网站，于是我先连上代理服务器，告诉他我需要那个无法访问网站的内容，代理服务器去把数据包取回来，然后返回给我。\n\n访问原来无法访问的资源，如google （通过正向代理实现科学上网）\n可以做缓存，加速访问资源\n对客户端访问授权，上网进行认证\n代理可以记录用户访问记录（上网行为管理）\n对外隐藏用户信息，保护隐私\n\n反向代理\n反向代理，指的是浏览器/客户端并不知道自己要访问具体哪台目标服务器，只知道去访问代理服务器 ，代理服务器再通过反向代理 +负载均衡实现请求分发到应用服务器的一种代理服务。反向代理服务的特点是代理服务器 代理的对象是应用服务器，也就是对于浏览器/客户端 来说应用服务器是隐藏的。\n大型网站，通常将反向代理作为公网访问地址，而Web应用服务器作为内网。\n有了反向代理，用户永远不会与使用它的Web服务器进行直接通信。可以将它们看作web服务 器或服务器集群的某种包装器。通过负载平衡和缓存，它们可以保护web服务器免遭攻击，并提供更好 的web性能。\n反向代理的用途：\n\n反向代理服务器像一个大门一样保证内网的安全，我们可以使用反向代理提供WAF（Web Application Firewall）功能，阻止web攻击 。\n负载均衡，通过反向代理服务器来优化网站的负载。\n\n配置文件介绍nginx.conf为最主要的配置文件\nnginx 文件结构\n\nNginx 配置详解 | 菜鸟教程 (runoob.com)\n\n...              #全局块events &#123;         #events块   ...&#125;http      #http块&#123;    ...   #http全局块    server        #server块    &#123;         ...       #server全局块        location [PATTERN]   #location块        &#123;            ...        &#125;        location [PATTERN]         &#123;            ...        &#125;    &#125;    server    &#123;      ...    &#125;    ...     #http全局块&#125;\n\n\n全局块：配置影响nginx全局的指令。一般有运行nginx服务器的用户组，nginx进程pid存放路径，日志存放路径，配置文件引入，允许生成worker process数等。\nevents块：配置影响nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。\nhttp块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，             4定义，日志自定义，是否使用sendfile传输文件，连接超时时间，单连接请求数等。\nserver块：配置虚拟主机的相关参数，一个http中可以有多个server。\nlocation块：配置请求的路由，以及各种页面的处理情况。\n\n应用反向代理+负载均衡实现效果：访问nginx.yiiong.top:8000时，将请求平均分配到8881和8882端口。\nnginx.yiiong.top解析到172.20.14.31\ndocker启动两个nginx容器，分别映射到宿主机8881、8882端口\n为了区分，我修改了主页内容（/usr/share/nginx/html/index.html）\n反向代理\nhttp &#123;\t\tserver &#123;                listen 8000;                server_name 172.20.14.31;                location / &#123;                        proxy_pass 172.20.14.31:8881; # 反向代理到8881                &#125;        &#125;\n\n反向代理+负载均衡\nupstream可以提供负载均衡和高可用性。\nweight：weight代表每个服务器的权重值，默认为1，权重越高，被分配的用户数就越多。这种分配方式主要用于后端服务器性能不均的情况。\n此时，访问8882端口的次数会是8881端口的两倍。\nhttp &#123;        upstream web&#123;                server 172.20.14.31:8881 weight=5;                server 172.20.14.31:8882 weight=10;        &#125;                server &#123;                listen 8000;                server_name 172.20.14.31;                location / &#123;                        proxy_pass http://web; # 反向代理到8881或8882                &#125;        &#125;\n\n文件服务器server &#123;                listen 8888;                server_name localhost;                charset utf-8;                root /home; # 显示该目录下的文件                location / &#123;                        autoindex on; # 自动生成目录索引页面                        autoindex_exact_size on; # 显示文件大小                        autoindex_localtime on; # 显示文件的最后修改时间                        &#125;        &#125;\n\n有些类型的文件链接在浏览器里点击后并不会直接下载而是只预览，这是我们只需要在mime.types中添加application/octet-stream  &lt;文件类型&gt;即可。\napplication/octet-stream 是一种 MIME 类型，用于指示未知类型的文件。\n\nddns由于DHCP分配给用户设备的IP地址并不固定，为了将设备当前的IP及时映射到域名上，使用户在任何时候都可以通过固定的地址访问到设备，产生了动态域名解析DDNS技术。\nddns-go 是一个轻量级的动态域名解析服务更新器，由Go语言编写，其主要功能是监测你的公共IP地址变化，并自动将新IP同步到支持自定义脚本的DDNS服务商。\n\njeessy2&#x2F;ddns-go: Simple and easy to use DDNS. Support Aliyun, Tencent Cloud, Dnspod, Cloudflare, Callback, Huawei Cloud, Baidu Cloud, Porkbun, GoDaddy, Namecheap, NameSilo… (github.com)\n\n\n内网穿透方法总结\n\nSoftether VPNVPN（Virtual Private Network，虚拟私人网络）\n\n7.3 在 Linux 上安装和初始配置 - SoftEther VPN 项目\n下载\n\n介绍Softether VPN是一个能够虚拟网卡和集线器（HUB）的工具，只要可以访问互联网，都可以使用它来组建远程局域网。\nVPN用途\n安全远程访问：VPN允许远程用户通过公共网络安全地连接到公司内部网络。\n绕过地理限制：通过连接到位于其他地区的VPN服务器，用户可以访问被地理限制的网站、内容和服务。这是因为连接到VPN后，我们的IP地址会被伪装为VPN服务器所在地的IP地址，使得我们看起来好像是从该地区访问网站。\n\n前提想要科学上网，我们就得有一台公网海外服务器。\n将 VPN Server 安装到 Linux 操作系统需要以下软件和库。检查系统是否已安装并启用以下软件和库。(make和gcc一定要有)\n\nmake\nGCC软件\nBinutils 软件\ntar、gzip 或其他用于提取软件包文件的软件\nchkconfig 系统实用程序\ncat、cp 或其他基本文件操作实用程序\nEUC-JP、UTF-8 或其他代码页表，用于日语环境\nlibc （glibc） 库\nzlib 库\nOpenSSL 库\nReadline 库\nncurses 库\npthread 库\n\n将压缩包解压，并将其移动至/usr/local/vpnserver，然后执行make进行编译，出现如下即成功。\n\n启动服务\nroot@RainYun-SJkdAk4z:/usr/local/vpnserver# ./vpnserver startThe SoftEther VPN Server service has been started.Let&#x27;s get started by accessing to the following URL from your PC:https://172.16.120.106:5555/  orhttps://172.16.120.106/Note: IP address may vary. Specify your server&#x27;s IP address.A TLS certificate warning will appear because the server uses self signed certificate by default. That is natural. Continue with ignoring the TLS warning.\n\n可以访问到我们的后台。\n\n检查服务运行状态root@RainYun-SJkdAk4z:/usr/local/vpnserver# ./vpncmdvpncmd command - SoftEther VPN Command Line Management UtilitySoftEther VPN Command Line Management Utility (vpncmd command)Version 4.43 Build 9799   (English)Compiled 2023/08/31 10:50:49 by buildsan at crosswin with OpenSSL 3.0.9Copyright (c) 2012-2023 SoftEther VPN Project. All Rights Reserved.By using vpncmd program, the following can be achieved. 1. Management of VPN Server or VPN Bridge 2. Management of VPN Client3. Use of VPN Tools (certificate creation and Network Traffic Speed Test Tool)Select 1, 2 or 3: 3VPN Tools has been launched. By inputting HELP, you can view a list of the commands that can be used.VPN Tools&gt;checkCheck command - Check whether SoftEther VPN Operation is Possible---------------------------------------------------SoftEther VPN Operation Environment Check ToolCopyright (c) SoftEther VPN Project.All Rights Reserved.If this operation environment check tool is run on a system and that system passes, it is most likely that SoftEther VPN software can operate on that system. This check may take a while. Please wait...Checking &#x27;Kernel System&#x27;...               PassChecking &#x27;Memory Operation System&#x27;...               PassChecking &#x27;ANSI / Unicode string processing system&#x27;...               PassChecking &#x27;File system&#x27;...               PassChecking &#x27;Thread processing system&#x27;...               PassChecking &#x27;Network system&#x27;...               PassAll checks passed. It is most likely that SoftEther VPN Server / Bridge can operate normally on this system.The command completed successfully.\n\n出现successfully后就可以开始搭建我们的vpnserver了。\n安装vpnserverroot@RainYun-SJkdAk4z:/usr/local/vpnserver# ./vpncmdvpncmd command - SoftEther VPN Command Line Management UtilitySoftEther VPN Command Line Management Utility (vpncmd command)Version 4.43 Build 9799   (English)Compiled 2023/08/31 10:50:49 by buildsan at crosswin with OpenSSL 3.0.9Copyright (c) 2012-2023 SoftEther VPN Project. All Rights Reserved.By using vpncmd program, the following can be achieved. 1. Management of VPN Server or VPN Bridge 2. Management of VPN Client3. Use of VPN Tools (certificate creation and Network Traffic Speed Test Tool)Select 1, 2 or 3: 1 // 选1，安装vpnserverSpecify the host name or IP address of the computer that the destination VPN Server or VPN Bridge is operating on. By specifying according to the format &#x27;host name:port number&#x27;, you can also specify the port number. (When the port number is unspecified, 443 is used.)If nothing is input and the Enter key is pressed, the connection will be made to the port number 8888 of localhost (this computer).Hostname of IP Address of Destination: 154.3.0.248 // 填写你的公网IPIf connecting to the server by Virtual Hub Admin Mode, please input the Virtual Hub name. If connecting by server admin mode, please press Enter without inputting anything.Specify Virtual Hub Name: // 不填，直接回车Connection has been established with VPN Server &quot;154.3.0.248&quot; (port 443).You have administrator privileges for the entire VPN Server.\n\n下载Softether VPN Server ManagerSecureNAT的作用提供虚拟NAT和虚拟DHCP服务。\n具体流程类似wifi路由器上网过程。\nwifi路由器上网过程：客户端通过路由器DHCP到一个内网IP，然后通过路由器的NAT功能，将内网IP映射到路由器的公网IP，通过路由器的公网IP进行上网。\nVPN：客户端连接到VPN Server，通过虚拟DHCP获得一个虚拟IP，通过SecureNAT，将这个虚拟IP映射到VPN Server的IP，通过VPN Server的IP上网。\n\n通过Client使用VPNPC端通过SoftEther VPN Client使用\n移动端(需要支持L2TP-一种VPN协议)使用自带VPN直接连接即可\nProxmox VE\nProxmox VE\n\nProxmox虚拟环境（简称PVE）是用于操作来宾操作系统的基于Debian Linux和KVM的虚拟化平台。\n你可以把它类比为VMware，只不过使用PVE的前提是你得有一个小迷你主机或闲置的电脑，因为PVE安装时是一个系统镜像。\n区别：\n\nVMware基于自家开发的 ESXi 虚拟化技术，是一种完全虚拟化，敏感指令在操作系统和硬件之间被捕捉处理，客户操作系统无需修改，且所有软件都能在虚拟机中运行。\nPVE主要基于 KVM，是一种基于内核的虚拟机，可将Linux内核转化成虚拟机监视器。同时它还有裸机安装器、网页版远程管理界面、HA集群堆栈、统一存储、柔性网络及可选的商业支持。\n\n局限性：\nProxmox群集的虚拟化和存储主机的最大数量为32台物理服务器。\n\n下载：ISO Installer - Proxmox Virtual Environment\n\n安装CPU类别：\n\nhost：性能最佳但兼容性不好，如果想将虚拟机从一台宿主机迁移到另一台 CPU 型号或架构不同的宿主机时，可能会出现兼容性问题。\nkvm64：兼容性好但性能不是最佳，它可以让我们在不同的宿主机之间迁移虚拟机。\n\nLVM（Logical Volume Manager）是一种在 Linux 操作系统上使用的逻辑卷管理器，它允许用户对存储设备进行灵活的管理。\n\n动态调整逻辑卷的大小，而无需重新分区磁盘。\n实现数据的快速备份和还原。\n支持数据的快速迁移和快速扩展存储容量。\n提高数据安全性和可靠性，包括数据冗余和快速恢复能力。\n\n","categories":["服务器运维"],"tags":["服务器运维"]},{"title":"算法记录","url":"/posts/36066.html","content":"\n项目的组件放在`internal/`​下\n\n\ncontroller（控制器）：负责核心业务逻辑的调度和协调，监听 Kubernetes 中 IPBlock 自定义资源的变化，维护和管理其生命周期。\n\nengine（封禁后端）：负责封禁命令的实际执行。\n\nnotify（通知机制）：负责将封禁、解封等事件以多种方式通知给运维人员或其他系统。\n\ntrigger（触发器）：事件触发中心，负责监听外部告警系统或业务事件（如 Grafana Alert等），并根据 Alert 策略触发相关封禁操作，自动创建 IPBlock CR，且支持自动解封。\n\npolicy（封禁策略）：定义 IP 封禁的判定规则和执行策略。\n\n\n### 介绍封禁后端均被抽象为API，IPBlock-Operator-Plus通过调用API来实现实际的封禁行为。目前通过`engine/control.py`​来实现API，接口列表如下：| 接口     | 方法 | 说明                                          || -------- | ---- | --------------------------------------------- || /limit   | GET  | iptables限流接口                              || /unlimit | GET  | iptables解限流接口                            || /limits  | GET  | iptables查看当前限流情况                      || /update  |      | XDP封禁端口                                   || /remove  |      | XDP解封禁端口                                 || /ban     | GET  | （弃用）可从nginx日志查，返回超过一定次数的IP || /execute | GET  | （弃用）接收IP，对其执行XDP封禁               |engine支持列表：- XDP：依赖于[evilsp/xdp_banner: 一个简单的 XDP 小程序，用于 BAN IP](https://github.com/evilsp/xdp_banner)- iptables：依赖于Linux工具iptables。  规则为 IP 每分钟最多发起10个新连接（可突发20次），否则DROP  ```go  iptables -A INPUT -s &lt;IP&gt; -p tcp --dport &lt;TARGET_PORT&gt; \\    -m state --state NEW \\    -m hashlimit --hashlimit 10/min --hashlimit-burst 20 \\    --hashlimit-mode srcip --hashlimit-name limit_&lt;IP_REPLACED&gt; \\    -j ACCEPT  ​  iptables -A INPUT -s &lt;IP&gt; -p tcp --dport &lt;TARGET_PORT&gt; -j DROP\n\n扩展开发指南engine定义了接口，新adapter只需要实现这两个方法即可。\ntype Adapter interface &#123;    // Ban 对某个 IP 发起封禁    // ip: 要封禁的 IP 地址    // isParmanent: 是否永久封禁（true 表示永久）    // durationSeconds: 封禁时长（单位：秒，仅在临时封禁时生效）    Ban(ip string, isParmanent bool, durationSeconds int) (string, error)​    // UnBan 解封某个 IP    UnBan(ip string) (string, error)&#125;​\n\n然后在NewAdapter​注册对应的adapter\nfunc NewAdapter(name, gatewayHost string) Adapter &#123;    switch name &#123;    case &quot;xdp&quot;:        return &amp;XDPAdapter&#123;GatewayHost: gatewayHost&#125;    case &quot;iptables&quot;:        return &amp;IptablesAdapter&#123;GatewayHost: gatewayHost&#125;    default:​        return &amp;XDPAdapter&#123;GatewayHost: gatewayHost&#125;    &#125;&#125;\n\n接着在controller.py​中要实现对应的API，最后在configmap​中engine​字段指定对应的adapter名即可。\nnotify介绍. \nnotify支持列表：\n\nlark：飞书，通过机器人来进行通知\n\n扩展开发指南notify定义了接口，新notify只需要实现这个接口即可。\n// 主计算函数int Calculate(char e[])&#123;    numStack OPND = InitNumStack();    opStack OPTR = InitOpStack();    PushOp(OPTR, &#x27;#&#x27;);  // 初始化运算符栈    char ch, op;    int a, b, result = 0;    int num = 0;    int flag = 0; // 检测是否为多位数    strcat(e, &quot;#&quot;);    for(int i = 0; e[i] != &#x27;\\0&#x27;; i++)    &#123;        ch = e[i];        if (ch &gt;= &#x27;0&#x27; &amp;&amp; ch &lt;= &#x27;9&#x27;) &#123;            num = num * 10 + (ch - &#x27;0&#x27;); // 累加组合成多位数            flag = 1;        &#125;        else &#123;            if(flag) &#123;                PushNum(OPND, num);  // 压入操作数                num = 0;                flag = 0;            &#125;            while(getPriority(GetOpTop(OPTR), ch) == &#x27;&gt;&#x27;) &#123;                PopOp(OPTR, &amp;op);                PopNum(OPND, &amp;b);                PopNum(OPND, &amp;a);                result = Operate(a, b, op);                PushNum(OPND, result);            &#125;            if (getPriority(GetOpTop(OPTR), ch) == &#x27;&lt;&#x27;) &#123;                PushOp(OPTR, ch);  // 压入当前运算符            &#125; else if (getPriority(GetOpTop(OPTR), ch) == &#x27;=&#x27;) &#123;                PopOp(OPTR, &amp;op); // 出栈匹配            &#125;        &#125;    &#125;    PopNum(OPND, &amp;result);    return result;&#125;\n\n以lark为例，在lark.go​中实现两个方法\n// NewLarkNotify 创建一个 LarkNotify（飞书通知器）实例。//// 参数://   - webhookURL: 飞书群机器人的 Webhook 地址。//   - templatePaths: 各类通知类型所用的 card 模板路径映射（如 map[&quot;ban&quot;]=&quot;templates/ban.json&quot;）。//// 返回值://   - 成功: 返回 LarkNotify 实例。//   - 失败: 返回 error，通常是模板解析失败或配置不合法。func NewLarkNotify(webhookURL string, templatePaths map[string]string) (*LarkNotify, error) &#123;&#125;// Notify 实现 Notifier 接口，// 根据事件类型和变量构造飞书卡片消息，并发送至 webhook。//// 参数://   - ctx: 请求上下文，用于控制超时和取消等。//   - eventType: 事件类型（如 &quot;ban&quot;、&quot;resolve&quot;、&quot;error&quot;）。//   - vars: 模板变量键值对，例如 &#123;&quot;ip&quot;: &quot;1.2.3.4&quot;, &quot;reason&quot;: &quot;恶意连接&quot;&#125;。//// 返回值://   - 成功: 返回 nil。//   - 失败: 返回 error，表示通知失败。func (l *LarkNotify) Notify(ctx context.Context, eventType string, vars map[string]string) error &#123;&#125;\n\n最后在main.go​中watchConfigMap​中完善loadNotify​。\nint main()&#123;    char e[MAXSIZE]; // 一条表达式     char p[MAXSIZE][MAXSIZE]; // 所有表达式    int count = 0;    printf(&quot;输入表达式，每行一个，以\\&quot;=\\&quot;结束：\\n&quot;);    while(1) &#123;        gets(e); // 读取一行表达式        // 去除换行符        e[strcspn(e,&quot;\\n&quot;)] = 0;        if(strcmp(e, &quot;=&quot;) == 0) &#123;            break;        &#125;        strcpy(p[count++], e);    &#125;    printf(&quot;计算结果为：\\n&quot;);    for(int i = 0; i &lt;count; i++) &#123;        int result = Calculate(p[i]);        printf(&quot;%d\\n&quot;, result);    &#125;    return 0;&#125;\n\ntrigger介绍trigger支持列表：\n\ngrafana：可以 Grafana Alert 联动，通过 Webhook 进行触发。\n\n扩展开发指南{0}注意新 trigger 在开发时需要考虑并发问题。具体实现可查看 grafana.go 中的处理逻辑，也可参考核心功能模块开发文档中并发处理的逻辑介绍。trigger同样定义了接口，新trigger只需要实现接口即可。\ngo&#x2F;&#x2F; Trigger 是封禁事件的触发器接口，Start 启动监听任务，Stop 停止监听任务type Trigger interface {    Name() string    Start(ctx context.Context) error    Stop(ctx context.Context) error}\ntrigger在manager.go​中实现了StartAll​和StopAll​，会启动在configmap​中指定的所有trigger。\n  triggers:                                          # 触发器，目前仅支持 Grafana    - name: grafana      addr: &quot;:8090&quot;      path: &quot;/trigger/grafana&quot;    - name: your_trigger      other: xxx\n\ntrigger自定义的配置字段，在main.go​中进行配置\n// Grafana Trigger 示例type TriggerConfig struct &#123;\tName string `yaml:&quot;name&quot;`\tAddr string `yaml:&quot;addr,omitempty&quot;`\tPath string `yaml:&quot;path,omitempty&quot;`&#125;// 解析 trigger 字符串为 YAML 列表func parseTriggers(yamlStr string) ([]TriggerConfig, error) &#123;\tvar triggers []TriggerConfig\terr := yaml.Unmarshal([]byte(yamlStr), &amp;triggers)\tif err != nil &#123;\t\treturn nil, err\t&#125;\treturn triggers, nil&#125;// 选择触发器func CreateTriggerByConfig(cfg TriggerConfig, mgr ctrl.Manager) trigger.Trigger &#123;\tswitch cfg.Name &#123;\tcase &quot;grafana&quot;:\t\treturn &amp;trigger.GrafanaTrigger&#123;\t\t\tClient: mgr.GetClient(),\t\t\tAddr:   cfg.Addr,\t\t\tPath:   cfg.Path,\t\t\t// 1000 个 IP， 60 秒防抖\t\t\t// LRU中最多保存1000个IP，达到1000个后会自动淘汰最近最少使用的IP\t\t\t// 对于同一个IP，如果最近60s内发生过一次封禁，那么这60s内再次收到该IP的相同请求时，会被防抖识别为重复\t\t\tDebouncer: utils.NewLRUDebouncer(1000, 60*time.Second),\t\t\tIPLocker:  utils.NewIPLock(),\t\t&#125;\t// TODO 其他触发器 ...\tdefault:\t\treturn nil\t&#125;&#125;\n\npolicy支持功能列表：- whitelist：白名单机制policy实现比较灵活，下面描述白名单机制的实现流程。1. 在`policy/watchlist.go`​中实现三个函数   ```go   // 白名单机制   // 单IP白名单   // CIDR白名单   // 标签匹配      type Whitelist struct &#123;   \tipNets []*net.IPNet // IPNet的指针切片，保存CIDR网段   \tips    []net.IP     // 精确IP白名单   &#125;      // 新建白名单   func NewWhitelist(ipList []string) *Whitelist &#123;&#125;      // 判断目标IP是否在白名单中   func (w *Whitelist) IsWhitelisted(ip string) bool &#123;&#125;      // 打印所有白名单内容   func (w *Whitelist) StringSlice() []string &#123;&#125;\n\n\n在config/loader.go​中实现一个LoadWhitelistFromConfigMap​，用于加载定义在configmap​中的白名单IP。\n\n在main.go​的watchConfigMap​中来监听白名单的新建和更新情况。\n\n\n‍\n","categories":["CS"],"tags":["数据结构","C"]},{"title":"数据结构","url":"/posts/1106.html","content":"\n## 绪论\n\n数据：所有能输入到计算机中并可以被计算机处理的信号。\n数据元素：用于完整地描述一个对象，是数据的基本单位。\n数据项：组成数据元素的、有独立含义的、不可分割的最小单位。\n数据对象：性质相同的数据元素的集合，是数据的一个子集。\n数据结构：相互之间存在一个或多种特定关系的数据元素的集合。\n逻辑结构：从具体问题抽象出来的数学模型。\n存储结构：逻辑结构在计算机中的存储表示。分顺序存储结构和链式存储结构。\n抽象数据类型：由用户定义的，表示应用问题的数学模型，以及定义在这个模型上的一组操作的总称。\n时间复杂度O(1) &lt; O(logn) &lt; O(n) &lt; O(nlogn) &lt; O(nn) &lt; O(nn*n)\nO(2的n次方) &lt; O(n!) &lt; O(n的n次方)\n冒泡排序\n最好时间复杂度：O(n) -&gt; 一轮冒泡就排好\n最坏时间复杂度：O(n*n)\n线性表顺序表的基本操作顺序表的存储结构\nplaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAXSIZE 100#define OVERFLOW -1typedef struct&#123;    int *elem; // 存储空间的基地址    int length; // 当前长度&#125;SqList; // 顺序表的结构类型为SqList\n\n初始化plaintext// 初始化int InitList(SqList *L)&#123;    L-&gt;elem = (int *)malloc(sizeof(int) * MAXSIZE); // 分配内存    if (!L-&gt;elem) &#123;        exit(OVERFLOW);    &#125; // 分配内存失败退出程序    L-&gt;length = 0; // 创建一个空顺序表    return 0;&#125;plaintext// 调用int main()&#123;    int L;    int result = InitList(&amp;L);    if (result==0) &#123;        printf(&quot;Init successfully!&quot;);    &#125;    else &#123;        printf(&quot;Init error!&quot;);    &#125;    return 0;&#125;\n\n取值（按位置查找值）时间复杂度O(1)\nplaintext// 取值// 使用 int *value 而不是 int value 的原因是函数需要将找到的元素值返回给调用者。// 使用指针可以实现这一点，而不需要通过函数的返回值来传递多个结果。int GetElem(SqList *L, int i, int *e)&#123;    if (i &lt; 1 || i &gt; L-&gt;length) &#123;        return -1; // 判断i的合法性    &#125;    *e = L-&gt;elem[i-1]; // 第i-1个位置存储的第i个数据元素    return 0;&#125;plaintext// 主函数调用int main() &#123;\t// 顺序表赋值    L.elem[0] = 10;    L.elem[1] = 20;    L.length = 2;    int value, result2 = 1;    result2 = GetElem(&amp;L, 1, &amp;value); // 查找顺序表第一个元素    if (result2 == 0) &#123;        printf(&quot;索引1处的值为%d\\n&quot;,value);    &#125;    else    &#123;        printf(&quot;索引超出范围&quot;);    &#125;    return 0;&#125;\n\n查找（按值查找位置）O(n)\nplaintext// 按值查找位置int LocateElem(SqList *L, int e)&#123;    for(int i=0;i &lt; L-&gt;length; i++) &#123;        if(L-&gt;elem[i] == e) &#123;            return i + 1;        &#125;    &#125;    return 0;&#125;plaintextint main()&#123;\tL.elem = 20;\tL.length = 2;\t// 按值查找    int elem = 20;    int pos = LocateElem(&amp;L, elem);    if ( pos != 0 ) &#123;        printf(&quot;%d在顺序表中的位置是%d&quot;, elem, pos);    &#125;    else &#123;        printf(&quot;值未找到&quot;);    &#125;    return 0;&#125;\n\n插入最好：O(1) 在最后一个位置插入\n最坏：O(n) 在第一个位置插入\n平均：O(n)\nplaintext// 插入int ListInsert(SqList *L, int i, int e)&#123;    if( i&lt;1 || i&gt;L-&gt;length+1 ) &#123; // 位置不合法        printf(&quot;位置不合法！\\n&quot;);        return -1;    &#125;    if( L-&gt;length == MAXSIZE ) &#123; // 存储空间已满        printf(&quot;存储空间已满！\\n&quot;);        return -1;    &#125;    for(int j = L-&gt;length-1; j &gt;= i-1; j-- ) // 将i后的元素依次向后移动    &#123;        L-&gt;elem[j+1] = L-&gt;elem[j]; // 插入位置之后元素后移    &#125;    L-&gt;elem[i-1] = e; // 插入元素    ++L-&gt;length;    return 0;&#125;plaintext// 调用int main() &#123;// 插入元素    int position = 3;    int a = 6;    L.length = 5;    printf(&quot;原先的顺序表元素为:\\n&quot;);    for(int j = 0; j &lt; 5; j++)    &#123;        L.elem[j] = j+1;        printf(&quot;%d&quot;, L.elem[j]);    &#125;    printf(&quot;\\n插入后的顺序表元素为:\\n&quot;);    int r = ListInsert(&amp;L, position, a);    if (r == 0) &#123;        printf(&quot;插入成功！\\n&quot;);        for(int k = 0; k &lt; 6; k++) &#123;            printf(&quot;%d&quot;,L.elem[k]);        &#125;    &#125;    else    &#123;         printf(&quot;插入失败！\\n&quot;);    &#125;    return 0;&#125;\n\n删除最好：O(1) 删除最后一个位置\n最坏：O(n) 删除第一个位置\n平均：O(n)\nplaintext// 删除void ListDelete(SqList *L, int i)&#123;    if( i &lt; 1 || i &gt; L-&gt;length) &#123;        printf(&quot;位置不合法!\\n&quot;);    &#125;    for(int j = i; j &lt;= L-&gt;length-1; j++)    &#123;        L-&gt;elem[j-1] = L-&gt;elem[j]; // 前移    &#125;    --L-&gt;length;&#125;plaintextint main() &#123;// 删除元素    int d = 3;    ListDelete(&amp;L, d);    printf(&quot;删除后的顺序表内容为:\\n&quot;);    for(int i = 0; i &lt; L.length; i++) &#123;        printf(&quot;%d&quot;,L.elem[i]);    &#125;    return 0; &#125;\n\n链表的基本操作单链表单链表操作完整代码plaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct&#123;    int data; // 数据域    struct LNode *next; // 指针域，指向下一个节点&#125;LNode;typedef LNode* LinkList; // 定义链表类型// 初始化LinkList InitList()&#123;    LinkList L = (LinkList)malloc(sizeof(LNode)); // 生成头结点    if(L == NULL) &#123;        printf(&quot;链表初始化失败！\\n&quot;);        exit(0);    &#125;    L-&gt;next = NULL; // 头指针设置为空    printf(&quot;链表初始化成功！\\n&quot;);    return L;&#125;// 前插法创建单链表void CreateList_H(LinkList *L, int n)&#123;    // 先创建一个空链表    *L = InitList();    for(int i = 0; i &lt; n; ++i)    &#123;        LNode *p = (LinkList)malloc(sizeof(LNode)); // 生成新结点        scanf(&quot;%d&quot;, &amp;p-&gt;data); // 输入元素值赋给新结点的数据域        p-&gt;next = (*L)-&gt;next;  // 将新结点*p插入到头结点之后        (*L)-&gt;next = p;    &#125;&#125;// 尾插法创建单链表void CreateList_R(LinkList *L ,int n)&#123;    *L = InitList();    LNode *r = *L; // 尾指针r指向头结点    for(int i = 0; i &lt; n; ++i)    &#123;        LNode *p = (LinkList)malloc(sizeof(LNode));        scanf(&quot;%d&quot;, &amp;p-&gt;data); // 输入元素值赋给新结点的数据域        p-&gt;next = NULL; // 新结点的下个结点指向空，也就是新结点为尾结点        r-&gt;next = p; // 原来的尾结点的下一个结点指向新结点        r = p; // 尾指针指向新结点    &#125;&#125;// 取值int GetElem(LinkList L, int i, int *e)&#123;    LinkList p = L-&gt;next;    int j = 1; // 计数器    while(p &amp;&amp; j &lt; i)    &#123;        p = p-&gt;next;        ++j;    &#125;    if(!p || j &gt; i)    &#123;        return -1;    &#125;    *e = p-&gt;data;    return 0;&#125;// 查找LinkList *LocateListElem(LinkList L, int e)&#123;    LinkList p = L-&gt;next;    while(p &amp;&amp; p-&gt;data != e)    &#123;        p = p-&gt;next;    &#125;    return p;&#125;// 插入int ListInsert(LinkList *L, int i, int e)&#123;// 在带头结点的单链表L中第i个位置插入值为e的新结点    //LNode *p = *L;    LinkList p = *L;    int j = 0;    while(p &amp;&amp; (j &lt; i-1))    &#123;        p = p-&gt;next; // 查找第i-1个结点，p指向该结点        ++j; //    &#125;    if( !p || j &gt; i - 1)    &#123;        return -1;    &#125;    //LNode *s = (LinkList)malloc(sizeof(LNode));    LinkList s = (LinkList)malloc(sizeof(LNode));    if (s == NULL) &#123;        return -1; // 内存分配失败    &#125;    s-&gt;data = e;    s-&gt;next = p-&gt;next;    p-&gt;next = s;    return 0;&#125;// 删除int ListDelete(LinkList *L, int i)&#123;    LinkList p = *L;    int j = 0;    while( (p-&gt;next) &amp;&amp; (j &lt; i-1) )    &#123;        p = p-&gt;next;        ++j;    &#125;    // 当i&gt;n或者i&lt;1时，位置不合理    if( !(p-&gt;next) || (j &gt; i-1) )    &#123;        return -1;    &#125;    LNode *q = p-&gt;next; // 临时保存被删除结点的地址以备释放    p-&gt;next = q-&gt;next; // 改变删除结点前驱结点的指针，也就是让i-1个结点直接指向被删除结点（q）的下一个结点（q-&gt;next）    free(q);    return 0;&#125;// 打印链表void PrintList(LinkList L)&#123;    LNode *p = L-&gt;next; // 从首元结点    while(p != NULL) &#123;        printf(&quot;%d -&gt; &quot;, p-&gt;data);        p = p-&gt;next;    &#125;    printf(&quot;NULL\\n&quot;);&#125;// 释放链表void DestroyList(LinkList *L)&#123;    if(*L == NULL) &#123;        return;    &#125;    LNode *p = (*L)-&gt;next;    while(p != NULL)&#123;        LNode *temp = p;        p = p-&gt;next;        free(temp);    &#125;    free(*L); // 释放头结点    *L = NULL; // 设置链表指针为NULL&#125;int main()&#123;    LinkList list;    int n;    printf(&quot;请输入链表中元素的数量：&quot;);    scanf(&quot;%d&quot;, &amp;n);    printf(&quot;创建链表，请输入%d个元素：\\n&quot;,n);    //CreateList_H(&amp;list, n);    CreateList_R(&amp;list, n);    printf(&quot;插入的元素为：\\n&quot;);    PrintList(list);    int m = 2;    int e;    if (GetElem(list, m, &amp;e) == 0) &#123;        printf(&quot;第%d个位置的元素为：%d\\n&quot;, m, e);    &#125; else &#123;        printf(&quot;获取元素失败\\n&quot;);    &#125;    // 查找数据为m的位置    printf(&quot;数据为%d的地址是%p\\n&quot;, m , LocateListElem(list, m));    // 插入    int pos = 3;    int elem = 18;    int result = ListInsert(&amp;list, pos, elem);    if(result == 0)    &#123;        printf(&quot;插入成功！\\n&quot;);        printf(&quot;在第%d位置插入了%d\\n&quot;,pos, elem);        PrintList(list);    &#125;    else    &#123;        printf(&quot;插入失败\\n&quot;);    &#125;    // 删除    int r = ListDelete(&amp;list, pos);    if(result == 0)    &#123;        printf(&quot;删除成功！\\n&quot;);        printf(&quot;删除了第%d个位置\\n&quot;,pos);        PrintList(list);    &#125;    else    &#123;        printf(&quot;删除失败\\n&quot;);    &#125;    DestroyList(&amp;list); // 释放链表&#125;\n\n链表包括两个域：\n\n数据域：存储数据元素信息\n指针域：存储直接后继存储位置\n\n单链表的存储结构\nplaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct&#123;    int data; // 数据域    struct LNode *next; // 指针域，指向下一个节点&#125;LNode;typedef LNode* LinkList; // 定义链表类型LNode：用于表示链表中的一个节点。用于创建新的节点。用于访问节点的成员（如 data 和 next）。LinkList：用于表示链表的头指针。用于传递和返回链表的头指针。用于遍历链表。整个程序中LNode*等价于LinkList\n\n单链表初始化plaintext// 初始化LinkList InitList()&#123;    LinkList L = (LinkList)malloc(sizeof(LNode)); // 生成头节点    if(L == NULL) &#123;        printf(&quot;链表初始化失败！\\n&quot;);        exit(0);    &#125;    L-&gt;next = NULL; // 头指针设置为空    printf(&quot;链表初始化成功！\\n&quot;);    return L;&#125;plaintext// 调用int main()&#123;    LinkList list = InitList();    free(list);    return 0;&#125;\n\n取值（按位置查找值）步骤：\n\n指针p指向首元节点，用j做计数器初值赋值为1\n从首元节点开始依次顺着链域向下访问，只要指向当前结点的指针p不为空，并且没有到达序号为i的结点，则做以下循环\np指向下一个结点\n计数器j相应加1\n\n\n退出循环时，如果p为空，或者计数器j大于i，说明指定的序号i值不合法（即i大于表长n或者i小于等于0），取值失败返回-1.\n当j&#x3D;i时，p所指向的结点就是要找的第i个结点，用e来保存当前的数据域。\n\nplaintext// 取值int GetElem(LinkList L, int i, int *e)&#123;    LinkList p = L-&gt;next;    int j = 1; // 计数器    while(p &amp;&amp; j &lt; i)    &#123;        p = p-&gt;next;        ++j;    &#125;    if(!p || j &gt; i)    &#123;        return -1;    &#125;    *e = p-&gt;data;    return 0;&#125;plaintext// 调用\tint m = 2;    int e;    if (GetElem(list, m, &amp;e) == 0) &#123;        printf(&quot;第%d个位置的元素为：%d\\n&quot;, m, e);    &#125; else &#123;        printf(&quot;获取元素失败\\n&quot;);    &#125;\n\n\n查找（按值查找结点地址）步骤：\n\n用p指向首元结点\n从首元结点依次顺着链域next向下查找，只要当前结点指针p不为空，并且p所指向结点的数据域不等于e，则执行p指向下一个结点操作\n返回p。若查找成功，p此时即为结点的地址值。否则p为NULL。\n\nplaintext// 查找LinkList *LocateListElem(LinkList L, int e)&#123;    LinkList p = L-&gt;next;    while(p &amp;&amp; p-&gt;data != e)    &#123;        p = p-&gt;next;    &#125;    return p;&#125;\n\n前插法创建单链表步骤：\n\n创建一个只有头结点的空链表，就是InitList()做的事情\n根据带创建链表，循环n次，执行以下操作\n生成新结点*p\n输入元素赋值给*p的数据域\n将新结点*p插入到头结点之后\n\n\n\n\nplaintext// 前插法创建单链表void CreateList_H(LinkList *L, int n)&#123;    // 先创建一个空链表    *L = InitList();    for(int i = 0; i &lt; n; ++i)    &#123;        // LNode *p = (LNode *)malloc(sizeof(LNode));        LNode *p = (LinkList)malloc(sizeof(LNode)); // 生成新结点        scanf(&quot;%d&quot;, &amp;p-&gt;data); // 输入元素值赋给新结点的数据域        p-&gt;next = (*L)-&gt;next;  // 将新结点*p插入到头结点之后        (*L)-&gt;next = p;    &#125;&#125;\n\n\n后插法创建单链表\n步骤：\n\n创建一个只有头结点的空链表，就是InitList()做的事情\n尾指针r初始化，指向头结点\n根据创建链表，循环n次以下操作\n生成新结点*p\n输入元素赋值给*p的数据域\n将新结点p插入到尾结点r之后\n尾指针r指向新的尾结点*p\n\n\n\nplaintext// 尾插法创建单链表void CreateList_R(LinkList *L ,int n)&#123;    *L = InitList();    LNode *r = *L; // 尾指针r指向头结点    for(int i = 0; i &lt; n; ++i)    &#123;        LNode *p = (LinkList)malloc(sizeof(LNode));        scanf(&quot;%d&quot;, &amp;p-&gt;data); // 输入元素值赋给新结点的数据域        p-&gt;next = NULL; // 新结点的下个结点指向空，也就是新结点作为尾结点        r-&gt;next = p; // 原来的尾结点的下一个结点指向新结点        r = p; // 尾指针指向新结点    &#125;&#125;\n\n\n插入\n步骤：\n\n找第i-1个位置的结点，然后指针p指向该结点\n生成一个新结点*s\n将*s的数据域置为e\n*s的指针域指向第i个结点（s-&gt;next &#x3D; p-&gt;next）\np的指针域指向新结点s（p-&gt;next &#x3D; s）\n\nplaintext// 插入int ListInsert(LinkList *L, int i, int e)&#123;// 在带头结点的单链表L中第i个位置插入值为e的新结点    //LNode *p = *L;    LinkList p = *L;    int j = 0;    while(p &amp;&amp; (j &lt; i-1))    &#123;        p = p-&gt;next; // 查找第i-1个结点，p指向该结点        ++j; //    &#125;    if( !p || j &gt; i - 1 )    &#123;        return -1;    &#125;    //LNode *s = (LNode *)malloc(sizeof(LNode));    LinkList s = (LinkList)malloc(sizeof(LNode));    if (s == NULL) &#123;        return -1; // 内存分配失败    &#125;    s-&gt;data = e;    s-&gt;next = p-&gt;next;    p-&gt;next = s;    return 0;&#125;plaintext// 调用    int pos = 3;    int elem = 18;    int result = ListInsert(&amp;list, pos, elem);    if(result == 0)    &#123;        printf(&quot;插入成功！\\n&quot;);        printf(&quot;在第%d位置插入了%d\\n&quot;,pos, elem);        PrintList(list);    &#125;    else    &#123;        printf(&quot;插入失败\\n&quot;);    &#125;\n\n删除步骤：\n\n找ai-1，指针p指向该结点\n临时保存待删除结点ai的地址在q中，以备释放\n将结点*p的指向ai的后继节点\n释放ai的空间\n\n\nplaintext// 删除int ListDelete(LinkList *L, int i)&#123;    LinkList p = *L;    int j = 0;    while( (p-&gt;next) &amp;&amp; (j &lt; i-1) )    &#123;        p = p-&gt;next;        ++j;    &#125;    // 当i&gt;n或者i&lt;1时，位置不合理    if( !(p-&gt;next) || (j &gt; i-1) )    &#123;        return -1;    &#125;    LNode *q = p-&gt;next; // 临时保存被删除结点的地址以备释放    p-&gt;next = q-&gt;next; // 改变删除结点前驱结点的指针，也就是让i-1个结点直接指向被删除结点（q）的下一个结点（q-&gt;next）    free(q);    return 0;&#125;// 为什么循环条件使用p-&gt;next而不是p？如果使用p作为循环条件，即while(p &amp;&amp; j &lt; i-1)，这可能会导致以下问题：1.当i为1时：如果要删除第一个节点，即i=1，那么在第一次迭代之前，j已经等于0了，此时j &lt; i-1就不成立了，循环不会执行，p仍然是头指针。这意味着我们无法进入循环去访问p-&gt;next，也就不能进行删除操作。2.遍历至链表末尾：使用p-&gt;next作为条件可以保证我们在到达链表的最后一个元素时停止，因为最后一个元素的next是指向NULL的。如果我们只检查p，则会一直遍历到p为NULL为止，这样我们就无法访问p-&gt;next来进行删除操作。plaintext// 删除    int r = ListDelete(&amp;list, pos);    if(result == 0)    &#123;        printf(&quot;删除成功！\\n&quot;);        printf(&quot;删除了第%d个位置\\n&quot;,pos);        PrintList(list);    &#125;    else    &#123;        printf(&quot;删除失败\\n&quot;);    &#125;\n\n\n查看链表元素plaintext// 打印链表void PrintList(LinkList L)&#123;    LNode *p = L-&gt;next; // 从首元结点    while(p != NULL) &#123;        printf(&quot;%d -&gt; &quot;, p-&gt;data);        p = p-&gt;next;    &#125;    printf(&quot;NULL\\n&quot;);&#125;\n\n释放单链表plaintext// 释放链表void DestroyList(LinkList *L)&#123;    if(*L == NULL) &#123;        return;    &#125;    LNode *p = (*L)-&gt;next;    while(p != NULL)&#123;        LNode *temp = p;        p = p-&gt;next;        free(temp);    &#125;    free(*L); // 释放头结点    *L = NULL; // 设置链表指针为NULL&#125;\n\n双向链表双向链表初始化双向链表结构体\nplaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct DuLNode&#123;    int data; // 数据域    struct DuLNode *prior; // 直接前驱    struct DuLNode *next; // 直接后驱&#125;DuLNode;typedef DuLNode* DuList;\n\n从后向前查看链表元素先设置头结点，然后找尾结点\n从前向后就不用找尾结点了\nplaintext//  从后向前打印双向链表void PrintList_E(DuList L)&#123;    DuList p = L-&gt;next; // 头结点    // 找最后一个结点    while (p-&gt;next != NULL) &#123;        p = p-&gt;next;    &#125;        while(p != NULL) &#123;        printf(&quot;%d -&gt; &quot;, p-&gt;data);        p = p-&gt;prior;    &#125;    printf(&quot;NULL\\n&quot;);&#125;\n\n\n插入\nplaintext// 插入int ListInsert(DuList *L, int i, int e)&#123;    DuList p = LocateListElem(*L, i);    if(!p)    &#123;        return -1;    &#125;    DuList s = (DuList)malloc(sizeof(DuLNode));    s-&gt;data = e;    s-&gt;prior = p-&gt;prior;    if (p-&gt;prior != NULL)    &#123;        p-&gt;prior-&gt;next = s;    &#125;    s-&gt;next = p;    p-&gt;prior = s;    return 0;&#125;plaintext// 调用，第三个位置插入19int main()&#123;    DuList list;    //InitDuList();    int n;    printf(&quot;请输入双向链表中元素的数量：&quot;);    scanf(&quot;%d&quot;, &amp;n);    CreateList_H(&amp;list, n);    PrintList(list);    int pos = 3;    int e = 19;    // 插入    int result = ListInsert(&amp;list, pos, e);    if(result == 0)    &#123;        printf(&quot;\\n插入成功！\\n&quot;);        printf(&quot;在第%d个位置插入了%d\\n&quot;, pos, e);        PrintList(list);    &#125;    else    &#123;        printf(&quot;插入失败！&quot;);    &#125;    return 0;&#125;\n\n\n删除\nplaintext// 删除void ListDelete_DuL(DuList *L, int i)&#123;    DuList p = LocateListElem(*L, i);    if(!p)        return;    p-&gt;prior-&gt;next = p-&gt;next;    p-&gt;next-&gt;prior = p-&gt;prior;    free(p);&#125;\n\n\n栈和队列顺序栈的基本操作完整代码plaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAXSIZE 100#define ERROR -1typedef struct&#123;    int *base; // 栈底指针    int *top; // 栈顶指针    int stacksize; // 栈可用的最大容量&#125;SqStack, *Stack;// 初始化Stack InitStack()&#123;    Stack s = (Stack)malloc(sizeof(SqStack));    if (s == NULL)    &#123;        printf(&quot;栈内存分配失败！\\n&quot;);        exit(1);    &#125;    // 为栈底分配内存    s-&gt;base = (int *)malloc(MAXSIZE * sizeof(int));    if(s-&gt;base == NULL) &#123;        free(s);        printf(&quot;栈内存分配失败！\\n&quot;);        exit(1);    &#125;    s-&gt;top = s-&gt;base;    s-&gt;stacksize = MAXSIZE;    printf(&quot;栈初始化成功！\\n&quot;);    return s;&#125;// 入栈int Push(Stack s, int e)&#123;    // 插入元素e为新的栈顶元素    if(s-&gt;top-s-&gt;base == s-&gt;stacksize)    &#123;        printf(&quot;栈满！\\n&quot;);        return ERROR;    &#125;    *s-&gt;top = e;    s-&gt;top++; // 栈顶上移    return 0;&#125;// 出栈int Pop(Stack s, int *e)&#123;    if(s-&gt;top == s-&gt;base)    &#123;        printf(&quot;栈空！\\n&quot;);        return ERROR;    &#125;    s-&gt;top--; // 栈顶指针下移    *e = *s-&gt;top; // 栈顶元素赋值给e    return 0;&#125;// 取栈顶元素int GetTop(Stack s, int *e)&#123;    // 返回栈顶元素，不修改栈顶指针    if(s-&gt;top != s-&gt;base) // 如果栈非空    &#123;        *e = *(s-&gt;top - 1);        return 0;    &#125;    return ERROR;&#125;int main()&#123;    Stack s = InitStack();    int n; // 栈长    printf(&quot;请输入栈长：&quot;);    scanf(&quot;%d&quot;, &amp;n);    // 入栈    for(int i = 0; i &lt; n; i++) &#123;        int e;        printf(&quot;输入入栈元素：&quot;);        scanf(&quot;%d&quot;, &amp;e);        if(Push(s, e) == 0) &#123;            printf(&quot;入栈成功：%d\\n&quot;, e);        &#125;        else &#123;            printf(&quot;入栈失败：%d&quot;, e);        &#125;    &#125;    // 出栈    for(int i = 0; i &lt; n; i++) &#123;        int e;        if(Pop(s, &amp;e) == 0)&#123;            printf(&quot;出栈成功，元素为%d\\n&quot;,e);            int topElem;            if(GetTop(s, &amp;topElem) == 0) &#123;                printf(&quot;此时栈顶元素为：%d\\n&quot;, topElem);            &#125;            else &#123;                printf(&quot;栈空，无法获取栈顶元素！\\n&quot;);            &#125;        &#125;        else            printf(&quot;出栈失败！\\n&quot;);    &#125;    return 0;&#125;\n\n特点：先入后出，后入先出\nplaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAXSIZE 100#define ERROR -1typedef struct&#123;    int *base; // 栈底指针    int *top; // 栈顶指针    int stacksize; // 栈可用的最大容量&#125;SqStack, *Stack;\n\n\n初始化plaintext// 初始化Stack InitStack()&#123;    Stack s = (Stack)malloc(sizeof(SqStack));    if (s == NULL)    &#123;        printf(&quot;栈内存分配失败！\\n&quot;);        exit(1);    &#125;    // 为栈底分配内存    s-&gt;base = (int *)malloc(MAXSIZE * sizeof(int));    if(s-&gt;base == NULL) &#123;        free(s);        printf(&quot;栈内存分配失败！\\n&quot;);        exit(1);    &#125;    s-&gt;top = s-&gt;base;    s-&gt;stacksize = MAXSIZE;    printf(&quot;栈初始化成功！\\n&quot;);    return s;&#125;\n\n入栈步骤：\n\n判断栈是否满（s-&gt;top - s-&gt;base &#x3D;&#x3D; 0）\n将新元素压入栈顶，栈顶指针加一\n\nplaintext// 入栈int Push(Stack s, int e)&#123;    // 插入元素e为新的栈顶元素    if(s-&gt;top-s-&gt;base == s-&gt;stacksize)    &#123;        printf(&quot;栈满！\\n&quot;);        return ERROR;    &#125;    *s-&gt;top = e;    s-&gt;top++; // 栈顶上移    return 0;&#125;\n\n出栈步骤：\n\n判断栈是否为空（s-&gt;top &#x3D;&#x3D; s-&gt;base）\n栈顶指针减一，栈顶元素出栈\n\nplaintext// 出栈int Pop(Stack s, int *e)&#123;    if(s-&gt;top == s-&gt;base)    &#123;        printf(&quot;栈空！\\n&quot;);        return ERROR;    &#125;    s-&gt;top--; // 栈顶指针下移    *e = *s-&gt;top; // 栈顶元素赋值给e    return 0;&#125;\n\n获取栈顶元素plaintext// 取栈顶元素int GetTop(Stack s, int *e)&#123;    // 返回栈顶元素，不修改栈顶指针    if(s-&gt;top != s-&gt;base) // 如果栈非空    &#123;        *e = *(s-&gt;top - 1);        return 0;    &#125;    return ERROR;&#125;plaintext// 出栈    for(int i = 0; i &lt; n; i++) &#123;        int e;        if(Pop(s, &amp;e) == 0)&#123;            printf(&quot;出栈成功，元素为%d\\n&quot;,e);            int topElem;            if(GetTop(s, &amp;topElem) == 0) &#123;                printf(&quot;此时栈顶元素为：%d\\n&quot;, topElem);            &#125;            else &#123;                printf(&quot;栈空，无法获取栈顶元素！\\n&quot;);            &#125;        &#125;        else            printf(&quot;出栈失败！\\n&quot;);    &#125;\n\n\n循环队列的基本操作完整代码plaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAXQSIZE 100#define ERROR -1typedef struct&#123;    int *base; // 存储空间基地址    int front; // 头指针    int rear; // 尾指针&#125;SqQueue, *Queue;// 初始化Queue InitQueue()&#123;    Queue q = (Queue)malloc(sizeof(SqQueue));    q-&gt;base = (Queue)malloc(MAXQSIZE * sizeof(SqQueue));    if(!q-&gt;base)    &#123;        printf(&quot;队列初始化失败！\\n&quot;);        exit(0);    &#125;    q-&gt;rear = 0;    q-&gt;front = q-&gt;rear;    return q;&#125;// 求队列长度int QueueLength(Queue q)&#123;    // 对于非循环队列，差值可能为负数，所以需要和MAXQSIZE求余    return (q-&gt;rear - q-&gt;front + MAXQSIZE) % MAXQSIZE;&#125;// 入队int EnQueue(Queue q, int e)&#123;    if(( q-&gt;rear + 1 ) % MAXQSIZE == q-&gt;front)        return ERROR;    q-&gt;base[q-&gt;rear] = e; // 新元素插入队尾    q-&gt;rear = (q-&gt;rear + 1) % MAXQSIZE; // 队尾指针加1    return 0;&#125;// 出队int DeQueue(Queue q, int *e)&#123;    if( q-&gt;front == q-&gt;rear)        return ERROR;    *e = q-&gt;base[q-&gt;front]; // 保存队头元素    q-&gt;front = (q-&gt;front + 1) % MAXQSIZE; // 队头指针加1    return 0;&#125;// 取循环队列队头元素int GetHead(Queue q)&#123;    if(q-&gt;front != q-&gt;rear)        return q-&gt;base[q-&gt;front];&#125;int main()&#123;    Queue q = InitQueue();    int n, e;    printf(&quot;请输入队列的长度：&quot;);    scanf(&quot;%d&quot;, &amp;n);    for(int i = 0; i &lt; n; i++)    &#123;        printf(&quot;入队元素：&quot;);        scanf(&quot;%d&quot;, &amp;e);        int result = EnQueue(q, e);        if(result == 0) &#123;            printf(&quot;成功入队！\\n&quot;);            printf(&quot;此时队列长度为：%d\\n&quot;, QueueLength(q));        &#125;        else        &#123;            printf(&quot;入队失败！\\n&quot;);        &#125;    &#125;    printf(&quot;\\n\\n&quot;);    // 出队    for(int j = 0; j &lt; n; j++)    &#123;        if(DeQueue(q, &amp;e) == 0) &#123;            printf(&quot;此时队列长度为：%d\\n&quot;, QueueLength(q));            // 显示当前队列元素            int head = GetHead(q);            if(head != ERROR) &#123;                printf(&quot;出队元素：%d\\n&quot;, e);                printf(&quot;当前队头元素为：%d\\n&quot;,head);                printf(&quot;\\n&quot;);            &#125;            else                printf(&quot;队列为空，无法取队头元素！\\n&quot;);        &#125;        else            printf(&quot;出队失败！\\n&quot;);    &#125;    free(q-&gt;base);    free(q);    return 0;&#125;\n\n特点：先入先出，后入后出\n循环队列判断\n队空：Q.front &#x3D; Q.rear\n队满：(Q.rear + 1)%MAXSIZE &#x3D;&#x3D; Q.front\n少用一个存储位置，约定一个位置为空\n结构体\nplaintext#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAXQSIZE 100#define ERROR -1typedef struct&#123;    int *base; // 存储空间基地址    int front; // 头指针    int rear; // 尾指针&#125;SqQueue, *Queue;\n\n初始化plaintext// 初始化Queue InitQueue()&#123;    Queue q = (Queue)malloc(sizeof(SqQueue));    q-&gt;base = (Queue)malloc(MAXQSIZE * sizeof(SqQueue));    if(!q-&gt;base)    &#123;        printf(&quot;队列初始化失败！\\n&quot;);        exit(0);    &#125;    q-&gt;rear = 0;    q-&gt;front = q-&gt;rear;    return q;&#125;\n\n求队列长度对于非循环队列，差值可能为负数，所以需要和MAXQSIZE求余\nplaintext// 求队列长度int QueueLength(Queue q)&#123;    // 对于非循环队列，差值可能为负数，所以需要和MAXQSIZE求余    return (q-&gt;rear - q-&gt;front + MAXQSIZE) % MAXQSIZE;&#125;\n\n入队步骤：\n\n判断队列是否为满\n将新元素插入队尾\n队尾指针加1（q-&gt;rear + 1） % MAXSIZE\n\nplaintext// 入队int EnQueue(Queue q, int e)&#123;    if(( q-&gt;rear + 1 ) % MAXQSIZE == q-&gt;front)        return ERROR;    q-&gt;base[q-&gt;rear] = e; // 新元素插入队尾    q-&gt;rear = (q-&gt;rear + 1) % MAXQSIZE; // 队尾指针加1    return 0;&#125;\n\n出队步骤：\n\n判断队空\n保存队头元素\n队头指针加1\n\nplaintext// 出队int DeQueue(Queue q, int *e)&#123;    if( q-&gt;front == q-&gt;rear)        return ERROR;    *e = q-&gt;base[q-&gt;front]; // 保存队头元素    q-&gt;front = (q-&gt;front + 1) % MAXQSIZE; // 队头指针加1    return 0;&#125;\n\n取队头元素plaintext// 取循环队列队头元素int GetHead(Queue q)&#123;    if(q-&gt;front != q-&gt;rear)        return q-&gt;base[q-&gt;front];&#125;\n\n\n树和二叉树树的基本术语\n\n结点：树中的一个独立单元。例如A B C D\n\n结点的度：结点拥有的子树数。例如A的度为3，C的度为1\n\n树的度：树内各结点度的最大值。\n度为3，不含根节点\n\n叶子：度为0的结点。结点K L F G M I J都是叶子\n\n非终端结点：度不为0的结点。\n内部节点\n\n双亲和孩子：结点的子树的根称为该结点的孩子，该结点称为孩子的双亲。例如B的双亲为A，B的孩子有E和F\n\n兄弟：同一个双亲的孩子之间互称兄弟。\n例如H I J互为兄弟\n\n祖先：从根到该结点所经分支上的所有节点。例如M的祖先为H D A\n\n子孙：以某结点为根的子树中的任意一个结点都称为该结点的子孙。例如B的子孙为E K L\n\n层次：结点的层次从根开始定义，根为第一层，根的孩子为第二层。树中任一结点的层次等于其双亲结点的层次加一。\n\n堂兄弟：双亲在同一层的结点互为堂兄弟。例如G与E F H I J互为堂兄弟\n\n树的深度：树中结点的最大层次。例如图中的深度为4\n\n有序树和无序树：如果将树中结点的各个子树看成从左到右是有次序的（即不能互换），则称该树为有序树，否则为无序树。在有序树中最左边子树的根称为第一个孩子，最右边称为最后一个孩子\n\n森林：是m棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林。\n\n\n二叉树性质\n在二叉树的第i层上至多有2^(i-1)个结点（i &gt;&#x3D; 1）\n\n深度为k的二叉树至多有2^k - 1个结点\n\n对任何一个二叉树T，如果其终端结点数为n0，度为2的结点数为n2，则n0 &#x3D; n2 + 1\n\n具有n个结点的完全二叉树的深度为(不大于log2n的最大整数 + 1)\n\n如果对一棵有n个结点的完全二叉树（其深度为不大于log2n的最大整数 + 1）的结点按层次编号（从第1层到第不大于log2n的最大整数 + 1层，每层从左到右），则对任一结点i（ 1 &lt;&#x3D; i &lt;&#x3D; n ），有\n如果i为1，则结点i是二叉树的根，无双亲；如果i &gt; 1, 则其双亲PARENT[i]是不大于i&#x2F;2的最大整数\n如果2i &gt; n，则结点i无左孩子；否则其左孩子LCHILD(i)是结点2i\n如果2i + 1 &gt; n, 则结点i无右孩子；否则其右孩子RCHILD(i)是结点2i + 1\n\n\n满二叉树：深度为k且含有2^k - 1个结点的二叉树。每一层上的结点数都是最大的结点数\n完全二叉树：深度为k的，有n个结点的二叉树。\n特点是：\n\n叶子结点只可能在层次最大的两层出现\n对任一结点，若其右分支下的子孙的最大层次为l，则其左分支下的子孙的最大层次必为l或l + 1\n\n\n","categories":["CS"],"tags":["CS"]},{"title":"基于Jenkins+Git+SonarQube+Draft+Buildah+Helm的CI/CD流","url":"/posts/9102.html","content":"\n## 构建部署工具及镜像作用介绍\n\n工具Jenkins：通过Pipeline实现CI&#x2F;CD流\nSonar：进行代码检查\ndraft：根据项目自动生成Dockerfile与Helm Chart\nBuildah：打包、推送镜像\nHelm：部署Chart到Kubernetes集群\n镜像beatrueman/builder:1.0：整合了draft与buildah，负责镜像构建与镜像推送\nbeatrueman/deployer:1.0：整合了Helm，用于部署Chart到Kubernetes\nsonarsource/sonar-scanner-cli:latest：用于执行代码检查\njenkins/inbound-agent:3206.vb_15dcf73f6a_9-2：它是 Jenkins Pipeline 中的一种代理机制，允许在 Jenkins 中动态创建代理节点以执行特定的构建任务。\n整体流程\n\n开发人员推送代码到Git仓库，自动触发Jenkins CI&#x2F;CD流\nSonarQube进行代码检查\n查找Dockerfile，如果没有则通过Draft自动生成Dockerfile和Helm Chart\n使用buildah进行镜像打包与镜像推送到Harbor仓库\n使用Helm将Chart部署在Kubernetes集群上，并把打包好的chart包推送至Harbor\n\n参数参数化构建\n\n\n\n变量名\n表示值\n可选项\n\n\n\nHARBOR_REGISTRY\nHarbor仓库名\n\n\n\nPROJECT_NAME\n项目名称\n必须小写\n\n\nENTRYPOINT\n项目入口文件（仅用于Python）\napp.py或main.py\n\n\nPORT\n项目暴露入口\n\n\n\nIMAGE_NAME\n镜像名称\n\n\n\nTAG\n镜像标签\n\n\n\nSONAR_PROJECT_NAME\nsonar代码检查项目名称\n\n\n\n准备插件下载在插件管理中搜索并下载以下插件\nKubernetes：Kubernetes版本4238.v41b_3ef14a_5d8\nSonarQube Scanner for Jenkins：SonarQube Scanner for Jenkins版本\n添加凭据\n\nSonarQube凭据保存的内容为在SonarQube中生成的全局令牌\nHarbor-Secret凭据保存Harbor的用户名和密码\nkubeconfig保存最后一步生成的kubeconfig\n\nKubernetes集群连接以使用Kubernerts部署的Jenkins为例（部署方法请自行查询）\n\n1.在系统管理 &gt;&gt; Clouds中新增一个cloud\n2.主要填入以下配置\n\n名称\nKubernetes地址：https://&lt;your_ip&gt;:6443\nKubernetes命名空间（需要与jenkins部署在同一个命名空间）\nJenkins地址：Jenkins在K8s部署，填入http://ClusterIP:Port (http://10.96.3.38:8080）。若不在K8s部署，需要将/root/.kube/configbase64编码后保存为凭据，然后再填入jenkins的暴露地址。\nJenkins通道：填入10.96.1.180:50000，注意一定不要加http\n\n将cloud名称填入cloud &quot;&quot;\n\n\n\n可以点击连接测试检查是否可以连接集群\n\nSonarQube准备1.手工新建一个项目\n2.新建一个全局令牌\n\n3.将该令牌生成的token添加进Jenkins的全局凭据中\n4.在系统配置中，填入Sonar的服务地址与凭据\n\n使用受限的kubeconfig使用该工具kubeconfig-generator，生成一个受限制的kubeconfig\n\n新建一个命名空间，用于最终项目的部署\n在 kubeconfig-generator.py中，指定NAMESPACE 、CLUSTER_SERVER、SA_NAME\n\n\n3.将生成的kubeconfig下载，以secret file形式添加进Jnekins凭据\n4.因为此时的config是受限的，需要生成一个rolebinding，用于jenkins命名空间下的default用户控制test命名空间下的一些操作\napiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: jenkins-rolebinding  namespace: test # 控制test命名空间subjects:- kind: ServiceAccount  name: default  namespace: jenkins # 这里假设Jenkins服务账户位于jenkins命名空间roleRef:  kind: ClusterRole  name: edit # 或者你可以定义一个自定义的Role，只包含所需的最小权限  apiGroup: rbac.authorization.k8s.io\n\n完整Jenkinsfilepipeline &#123;    agent &#123;        kubernetes &#123;            cloud &quot;Redrock-Cloud&quot;            yaml &quot;&quot;&quot;apiVersion: v1kind: Podmetadata:  labels:    component: jenkins-agentspec:  containers:    - name: sonar-scanner      image: sonarsource/sonar-scanner-cli:latest      command:        - cat      tty: true    - name: builder      image: beatrueman/builder:1.0      securityContext:        privileged: true      command:        - cat      tty: true    - name: deployer      image: beatrueman/deployer:1.0      command:        - cat      tty: true    - name: jnlp      image: jenkins/inbound-agent:3206.vb_15dcf73f6a_9-2      resources:        limits:          memory: &quot;1Gi&quot;          cpu: &quot;200m&quot;&quot;&quot;&quot;        &#125;    &#125;    // environment &#123;    //     HARBOR_REGISTRY = &#x27;&#x27;    //     PROJECT_NAME = &#x27;myapp&#x27; // 项目名称，必须小写    //     ENTRYPOINT = &#x27;main.py&#x27; // 项目入口文件，app.py或main.py    //     PORT = &#x27;6666&#x27; // 项目暴露的端口    //     IMAGE_NAME = &#x27;yicloud&#x27; // 镜像名称    //     TAG = &#x27;v1&#x27; // 镜像标签    //     SONAR_PROJECT_NAME = &#x27;Python&#x27; // sonar项目名称    // &#125;    stages &#123;        stage(&#x27;git clone&#x27;) &#123;            steps &#123;                echo &quot;1.正在克隆代码......&quot;                git url: &quot;&quot;            &#125;        &#125;        stage(&#x27;SonarQube code checking&#x27;) &#123;            steps &#123;                container(&#x27;sonar-scanner&#x27;) &#123;                    echo &#x27;2.正在进行代码检查......&#x27;                    echo &#x27;代码检查结果请在面板查看！&#x27;                    withSonarQubeEnv(&#x27;SonarQube&#x27;) &#123;                        sh &#x27;&#x27;&#x27;sonar-scanner \\                            -Dsonar.projectKey=$&#123;SONAR_PROJECT_NAME&#125; \\                            -Dsonar.projectName=$&#123;SONAR_PROJECT_NAME&#125; \\                            -Dsonar.sources=&quot;/home/jenkins/agent/workspace/$&#123;JOB_NAME&#125;&quot; \\                            -Dsonar.projectVersion=1.0 \\                            -Dsonar.sourceEncoding=UTF-8                        &#x27;&#x27;&#x27;                    &#125;                &#125;            &#125;        &#125;        stage(&#x27;Image build&#x27;) &#123;            steps &#123;                script &#123;                    echo &#x27;3.正在制作镜像......&#x27;                    try &#123;                        def result = sh(script: &quot;ls -al | grep Dockerfile&quot;, returnStatus: true)                        if (result == 0) &#123;                            echo &quot;找到 Dockerfile，开始构建镜像&quot;                            container(&#x27;builder&#x27;) &#123;                                sh &#x27;&#x27;&#x27;                                    draft create -a myapp --variable PORT=$&#123;PORT&#125; \\                                        --variable APPNAME=$&#123;PROJECT_NAME&#125; \\                                        --variable SERVICEPORT=8088 \\                                        --variable NAMESPACE=test \\                                        --variable IMAGENAME=$&#123;HARBOR_REGISTRY&#125;/library/$&#123;IMAGE_NAME&#125; \\                                        --variable IMAGETAG=$&#123;TAG&#125; \\                                        --variable ENTRYPOINT=$&#123;ENTRYPOINT&#125; \\                                        --variable VERSION=3 \\                                        --deploy-type helm \\                                        --deployment-only                                    rm &quot;/home/jenkins/agent/workspace/$&#123;JOB_NAME&#125;/charts/templates/namespace.yaml&quot;                                    buildah bud -t $&#123;HARBOR_REGISTRY&#125;/library/$&#123;IMAGE_NAME&#125;:$&#123;TAG&#125; .                                &#x27;&#x27;&#x27;                            &#125;                        &#125; else &#123;                            echo &quot;未找到 Dockerfile，将生成 Dockerfile&quot;                            container(&#x27;builder&#x27;) &#123;                                def jobNameLower = &quot;$&#123;JOB_NAME&#125;&quot;.toLowerCase()                                sh &#x27;&#x27;&#x27;                                    draft create -a myapp --variable PORT=$&#123;PORT&#125; \\                                        --variable APPNAME=$&#123;PROJECT_NAME&#125; \\                                        --variable SERVICEPORT=8088 \\                                        --variable NAMESPACE=test \\                                        --variable IMAGENAME=$&#123;HARBOR_REGISTRY&#125;/library/$&#123;IMAGE_NAME&#125; \\                                        --variable IMAGETAG=$&#123;TAG&#125; \\                                        --variable ENTRYPOINT=$&#123;ENTRYPOINT&#125; \\                                        --variable VERSION=3 \\                                        --deploy-type helm                                    rm &quot;/home/jenkins/agent/workspace/$&#123;JOB_NAME&#125;/charts/templates/namespace.yaml&quot;                                    buildah bud -t $&#123;HARBOR_REGISTRY&#125;/library/$&#123;IMAGE_NAME&#125;:$&#123;TAG&#125; .                                &#x27;&#x27;&#x27;                            &#125;                        &#125;                    &#125; catch (err) &#123;                        echo &quot;查找 Dockerfile 发生错误: $&#123;err&#125;&quot;                    &#125;                &#125;            &#125;        &#125;        stage(&#x27;Image push&#x27;) &#123;            steps &#123;                echo &quot;3. Pushing image&quot;                container(&#x27;builder&#x27;) &#123;                    script &#123;                        withCredentials([usernamePassword(credentialsId: &#x27;Redrock-Harbor-Secret&#x27;, passwordVariable: &#x27;passwd&#x27;, usernameVariable: &#x27;username&#x27;)]) &#123;                            sh &quot;buildah login -u $&#123;username&#125; -p $&#123;passwd&#125; $&#123;env.HARBOR_REGISTRY&#125;&quot;                            sh &quot;buildah images&quot;                            sh &quot;buildah push $&#123;HARBOR_REGISTRY&#125;/library/$&#123;IMAGE_NAME&#125;:$&#123;TAG&#125;&quot;                        &#125;                    &#125;                &#125;            &#125;        &#125;                stage(&#x27;Deploy to Kubernetes&#x27;) &#123;            steps &#123;                echo &quot;5.即将把服务部署在Kubernetes集群上......&quot;                container(&#x27;deployer&#x27;) &#123;                    script &#123;                        // 使用受限制的kubeconfig                        def kubeConfigCreds = credentials(&#x27;kubeconfig&#x27;)                        // 写入临时kubeconfig文件                        sh &#x27;echo &quot;$&#123;kubeConfigCreds&#125;&quot; | base64 --decode &gt; /tmp/kubeconfig.yaml&#x27;                        sh &#x27;ls&#x27;\t\t\t            // 部署并获取返回码，如果成功部署则打包chart并上传至Harbor仓库                        def chartYamlContent = readFile &quot;charts/Chart.yaml&quot; // 读取Chart.yaml                        def chartYaml = readYaml text: chartYamlContent                        def chartVersion = chartYaml.version // 获取Chart的version                        def packageName = &quot;$&#123;PROJECT_NAME&#125;-$&#123;chartVersion&#125;.tgz&quot; // 打包后的Chart名                                                def helmDeployResult = sh(script: &quot;helm install $&#123;PROJECT_NAME&#125; -n test --kubeconfig /tmp/kubeconfig.yaml charts&quot;, returnStatus: true)\t\t\t                if(helmDeployResult == 0) &#123;\t\t\t                    echo &#x27;部署成功！&#x27;\t\t\t                    echo &#x27;正在打包Chart，并上传至Harbor仓库&#x27;\t\t\t                    \t\t\t                    sh &quot;helm package charts&quot;\t\t\t                    withCredentials([usernamePassword(credentialsId: &#x27;Redrock-Harbor-Secret&#x27;, passwordVariable: &#x27;passwd&#x27;, usernameVariable: &#x27;username&#x27;)]) &#123;\t\t\t                        sh &quot;helm registry login $&#123;env.HARBOR_REGISTRY&#125; -u $&#123;username&#125; -p $&#123;passwd&#125;&quot;\t\t\t                        sh &quot;helm push $&#123;packageName&#125; oci://$&#123;HARBOR_REGISTRY&#125;/library&quot;\t\t\t                    &#125;\t\t\t                    \t\t\t                &#125; else &#123;\t\t\t                    error &quot;Chart部署失败，Helm返回码: $&#123;helmDeployResult&#125;&quot;\t\t\t                &#125;\t\t\t                                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;\n\n\n安全警示根据buildah image v1.34: Error: open &#x2F;usr&#x2F;lib&#x2F;containers&#x2F;storage&#x2F;overlay-images&#x2F;images.lock: permission denied · Issue #5332 · containers&#x2F;buildah (github.com)\nbuilder容器不得不开启privileged，否则无法进行正常的打包与推送镜像。\n原因与解决可参考：https://opensource.com/article/19/3/tips-tricks-rootless-buildah\n\n","categories":["Achievements"],"tags":["Achievements"]},{"title":"红岩打印姬复活记录","url":"/posts/48620.html","content":"\n下面记录一下在复活网校打印机机器人-**红岩打印姬**时遇到的问题和解决方案\n\n做出的更新1.使用了Helemet-CI进行镜像打包\n2.更改了Dockerfile的内容，使镜像可以快速且正确的被打包\n3.由于飞书接收消息接口的json接收体发生了改变，对reply.py中的file_name、file_key和image_key三个变量的获取造成了影响，使用json.loads()函数解决\n1.读取环境变量问题机器人通过configmap读取app_id，app_secret等环境变量，没有configmap的话机器人会报以下错误\nTraceback (most recent call last):  File &quot;/nonebot/bot.py&quot;, line 22, in &lt;module&gt;    driver.register_adapter(FeishuAdapter)  File &quot;/usr/local/lib/python3.10/dist-packages/nonebot/internal/driver/abstract.py&quot;, line 77, in register_adapter    self._adapters[name] = adapter(self, **kwargs)  File &quot;/usr/local/lib/python3.10/dist-packages/nonebot/adapters/feishu/adapter.py&quot;, line 46, in __init__    self.feishu_config: Config = Config(**self.config.dict())  File &quot;pydantic/main.py&quot;, line 341, in pydantic.main.BaseModel.__init__pydantic.error_wrappers.ValidationError: 1 validation error for Configfeishu_bots  value is not a valid list (type=type_error.list)\n\n提示feishu_bots不是一个有效列表，意味着机器人未读取到configmap里的环境变量\n在部署时也不能直接把这些环境变量直接明文填写在.env.prod中\n2.Dockerfile的修改2.1 apt换源问题最开始的Dockerfile使用了python:slim镜像，导致在执行apt-get update命令时异常的慢\n并且在这个镜像（debian镜像也是）中不能更改镜像源，会提示\n\n于是更改为\nFROM ubuntu:22.04\n\n使用sed命令更改镜像源为清华源\nRUN sed -i &#x27;s/archive.ubuntu.com/mirrors.tuna.tsinghua.edu.cn/g&#x27; /etc/apt/sources.list   \n\n2.2 pip换源问题最开始的Dockerfile中的所有pip install均未指定源，经常会出现网络问题\n\n于是我向所有pip都指定了源\n使用pipx install nb-cli经常出现网络问题\n\npipx 是一个自由开源程序，允许你在隔离的虚拟环境中安装和运行 Python 应用。\n\n\n使用pip install nb-cli -i https://pypi.tuna.tsinghua.edu.cn/simple即可解决\n2.3 未找到nb命令/bin/sh: 1: /root/.local/bin/nb: not found\n\n在Dockerfile中\nCMD which nb\n\n即可显示正确的nb位置\n3.IngressRoute请求路径下划线问题在IngressRoute中，请求路径最后的下划线会影响解析\n\n在本实例中，不加下划线才能被正确解析\n\n4.飞书接收消息接收体改变影响变量问题飞书接收消息json接收体结构如下\n&#123;    &quot;schema&quot;: &quot;2.0&quot;,    &quot;header&quot;: &#123;        &quot;event_id&quot;: &quot;56dc6536843b7bd94680cf2af1c8791&quot;,        &quot;token&quot;: &quot;aJ2XlWEC5uNfiaQMzaNUCbFMeIW28828&quot;,        &quot;create_time&quot;: &quot;1698134093609&quot;,        &quot;event_type&quot;: &quot;im.message.receive_v1&quot;,        &quot;tenant_key&quot;: &quot;17cf1f7e2f06d758&quot;,        &quot;app_id&quot;: &quot;cli_a42011585561100d&quot;    &#125;,    &quot;event&quot;: &#123;        &quot;message&quot;: &#123;            &quot;chat_id&quot;: &quot;oc_8b46f745993928bcfe0f23fe30cee4d0&quot;,            &quot;chat_type&quot;: &quot;p2p&quot;,            &quot;content&quot;: &quot;&#123;\\&quot;file_key\\&quot;:\\&quot;file_v2_718639f3-fbf7-4a12-8ff3-56ea16b1d4ag\\&quot;,\\&quot;file_name\\&quot;:\\&quot;a.pdf\\&quot;&#125;&quot;,            &quot;create_time&quot;: &quot;1698134093362&quot;,            &quot;message_id&quot;: &quot;om_3fb62cafecae381509e6e36eeb2f5cc3&quot;,            &quot;message_type&quot;: &quot;file&quot;,            &quot;update_time&quot;: &quot;1698134093362&quot;        &#125;,        &quot;sender&quot;: &#123;            &quot;sender_id&quot;: &#123;                &quot;open_id&quot;: &quot;ou_b7f2f10f337267948c94258ab8c684e&quot;,                &quot;union_id&quot;: &quot;on_01a1d695ec162830a027295516386bdc&quot;,                &quot;user_id&quot;: &quot;9a8cfgee&quot;            &#125;,            &quot;sender_type&quot;: &quot;user&quot;,            &quot;tenant_key&quot;: &quot;17cf1f7e2f06d758&quot;        &#125;    &#125;&#125;\n\n对原先reply.py中file_name、file_key和image_key三个变量的获取造成了影响\n\n现通过json.loads()函数重新提取这三个变量，其他同理\ne为整个接收体的内容，event为event的内容，其他同理\n\n","categories":["Trouble Shooting"],"tags":["Trouble Shooting"]},{"title":"核心功能模块","url":"/posts/11627.html","content":"\n## 介绍\n\ncontroller是IPBlock-Operator-Plus的核心模块，在ipblock_controller.go​中实现。它负责核心业务逻辑的调度和协调，监听 Kubernetes 中 IPBlock 自定义资源的变化，维护和管理其生命周期。它是整个项目的核心部分，协调各个模块协同工作。\nReconcile（调和、协调、解决冲突）​是Kubernetes Operator的核心方法，用于驱动资源的状态的期望一致性，这也是Kubernetes controller的核心理念与任务。对于IPBlock-Operator-Plus而言，它的任务就是处理每一个IPBlock​对象的状态变更、触发动作以及更新状态字段（Status）等核心逻辑。\n结构定义IPBlockReconciler​接口定义\ntype IPBlockReconciler struct &#123;\tclient.Client                      // 客户端通信\tScheme        *runtime.Scheme      // 序列化和反序列化\tRecorder      record.EventRecorder // Event记录器\tAdapter       engine.Adapter       // 封禁适配器接口\tAdapterName   string\tGatewayHost   string\tCmName        string\tCmNamespace   string\tWhitelist     *policy.Whitelist // ConfigMap读取\tmu            sync.RWMutex      // 读写锁\tNotifier      notify.Notifier   // 通知接口&#125;\n\nipblock_controller.go​中实现的函数，如下：\n// 处理CRD各种事件的具体业务逻辑, req包含标识当前对象的信息：名称和命名空间// ctrl 是 controller-runtime的主入口包// ctrl.Request 表示控制器需要处理的资源对象标识，也就是哪个资源要 Reconcile// ctrl.Result 控制是否需要重新排队执行该资源，也就是是否需要再次执行func (r *IPBlockReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) &#123;&#125;// 计算当前Spec的Hashfunc HashSpec(spec opsv1.IPBlockSpec) (string, error) &#123;&#125;// 用于解决对象版本冲突引发的更新问题func (r *IPBlockReconciler) UpdateIPBlockStatus(ctx context.Context, ipblock *opsv1.IPBlock, updateFn func(*opsv1.IPBlock)) (*opsv1.IPBlock, error) &#123;&#125;// 定时解封func (r *IPBlockReconciler) scheduleAutoUnblock(ipblock *opsv1.IPBlock) &#123;&#125;// 并发安全，通过锁来更新白名单func (r *IPBlockReconciler) UpdateWhitelist(wl *policy.Whitelist) &#123;&#125;// 并发安全，通过锁来更新GatewayHostfunc (r *IPBlockReconciler) UpdateGatewayHost(newHost string) &#123;&#125;\n\n字段定义IPBlock定义\ntype IPBlock struct &#123;\tmetav1.TypeMeta   `json:&quot;,inline&quot;`\tmetav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`\tSpec   IPBlockSpec   `json:&quot;spec,omitempty&quot;`\tStatus IPBlockStatus `json:&quot;status,omitempty&quot;`&#125;// Spec// 封禁请求type IPBlockSpec struct &#123;\t// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster\t// Important: Run &quot;make&quot; to regenerate code after modifying this file\t// Foo is an example field of IPBlock. Edit ipblock_types.go to remove/update\tFoo      string   `json:&quot;foo,omitempty&quot;`\tIP       string   `json:&quot;ip&quot;`                 // 目标IP\tReason   string   `json:&quot;reason,omitempty&quot;`   // 封禁原因\tSource   string   `json:&quot;source,omitempty&quot;`   // 封禁来源，如 &quot;alertmanager&quot;、&quot;manual&quot;、&quot;webhook&quot;，便于追踪\tBy       string   `json:&quot;by,omitempty&quot;`       // 谁触发的封禁\tDuration string   `json:&quot;duration,omitempty&quot;` // 封禁持续时间\tTags     []string `json:&quot;tags,omitempty&quot;`     // 关键词筛选\tUnblock  bool     `json:&quot;unblock,omitempty&quot;`  // 用户显式解封\tTrigger  bool     `json:&quot;trigger,omitempty&quot;`  // 用户显式请求重新封禁&#125;// Status// 封禁状态type IPBlockStatus struct &#123;\t// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster\t// Important: Run &quot;make&quot; to regenerate code after modifying this file\tResult       string `json:&quot;result,omitempty&quot;` // success, failed, unblocked\tPhase        string `json:&quot;phase,omitempty&quot;`  // pending, active, expired\tBlockedAt    string `json:&quot;blockedAt,omitempty&quot;`\tUnblockedAt  string `json:&quot;unblockedAt,omitempty&quot;`\tMessage      string `json:&quot;message,omitempty&quot;`\tLastSpecHash string `json:&quot;lastSpecHash,omitempty&quot;`&#125;\n\n状态机phasephase用来标识 IP 状态阶段\n\n\n\nPhase\n含义说明\n\n\n\n​pending​\n初始化阶段（默认值）\n\n\n​active​\n已封禁状态\n\n\n​expired​\n已解封（自动或手动）\n\n\n​skipped​\n跳过（如白名单命中）\n\n\n​failed​\n操作失败，如封禁失败、非法配置等\n\n\n\n状态流转逻辑说明初始状态：pending​\n创建时设为 pending​\n\n若命中白名单 → 设为 skipped​\n\n若未命中白名单，尝试封禁：\n\n成功 → active​\n失败 → failed​\n\n\n\n白名单状态：skipped​\n表示已跳过处理\n幂等逻辑下跳过执行（不重复封禁）\n\n封禁中状态：active​\n表示 IP 当前已封禁\n若设定 Duration​ 且未过期 → 等待时间结束后由 goroutine 调用 scheduleAutoUnblock​ 解封 → expired​\n若手动触发 Spec.Trigger = true​ → 重新执行封禁流程\n若手动设置 Spec.Unblock = true​ → 执行解封操作 → expired​\n\n已解封状态：expired​\n解封成功后设置为 expired​\n表示资源生命周期已闭环（可留待 GC）\n\n失败状态：failed​\n封禁失败或非法配置（如 duration​ 解析失败）\n写入 .Status.Result = failed​，并通过 notifier 通知错误\n\nresult\n\n\nResult 值\n含义\n\n\n\n​success​\n操作成功（封禁或解封成功）\n\n\n​failed​\n操作失败（封禁或解封失败）\n\n\n​unblocked​\n已解封（通常对应解封动作完成）\n\n\n​skipped​\n跳过操作（如白名单、重复跳过）\n\n\n（空字符串）\n尚未执行操作或初始状态\n\n\n\nReconcile具体实现reconcile主要做了五件事：\n\n如果有手动解封，优先处理\n白名单跳过（只在非 trigger 情况下判断）\n手动强制封禁\n幂等判断（状态无变化 + 未触发）\n封禁操作\n\n手动解封处理\nspec​ 表示用户期望的状态，（想要什么）​status​ 表示控制器观察到的实际状态。（现在是什么）\n大致流程是：\n\n读取spec.Unblock == true​，表明用户意图要解封\n执行解封逻辑 ——&gt; 解封成功\n更新status.result == unblocked​，记录了解封完成\n然后要把spec.unblock​更新为false​，否则下次Reconcile又会重复解封，造成混乱\n\n\n主要在判断unblocked​是否为true​。\n预处理：\n首先需要通过r.Get​去集群中获取名为 req.Name​ 的 IPBlock​ 资源，看看有没有这个资源。\n有的话初始化phase​为pending​，使用r.Update​来更新。\n手动解封判断：\n首先判断当前资源的result​是不是unblocked​，是的话跳过。\n不是的话，就需要用r.Adapter.Unban(ip)​方法来解封ip，解封成功或失败都会用r.UpdateIPBlockStatus​来更新资源的状态（Status），然后通过 Notify 进行通知。\n最后利用r.Patch​更新ipblock.Spec.Unblock​为false​\n白名单跳过首先判断白名单是否存在 &amp;&amp; IP 是否在白名单中，命中（即phase == skipped​），则更新status.phase​和status.result​为skipped​\n手动强制封禁当 spec.trigger == true​时，控制器在 Reconcile 中会识别到这一标志，先将其重置为false​，并设置 triggered = true​。随后通过r.Update(ctx, &amp;ipblock)​更新资源对象，从而触发一次新的 Reconcile​。在下一轮 Reconcile 中，控制器由于检测到 triggered == true​（即使 spec 内容未变、哈希一致），也会跳过幂等判断，进入并执行 Step 5 中的封禁操作。\n这么做实现了一种 “按钮行为” ，不在这里直接调用Ban()​方法是出于幂等性 + 声明式控制 + 最小变更原理考虑。\n\n\n\n原因\n说明\n\n\n\n幂等性\n封禁操作必须避免重复执行 → 所以控制器通过 hash 和triggered​判断\n\n\n声明式控制\n控制器行为应由spec​驱动 → 用户设置trigger: true​即可，不需要命令式调用函数\n\n\n对齐 controller-runtime 模式\n所有资源状态变更由 Reconcile 驱动，包括 trigger 的清除\n\n\n幂等判断这里通过HashSpec()​来进行哈希，也就是对Spec整个部分进行哈希编码\nfunc HashSpec(spec opsv1.IPBlockSpec) (string, error) &#123;\tb, err := json.Marshal(spec)\tif err != nil &#123;\t\treturn &quot;&quot;, err\t&#125;\th := sha256.Sum256(b)\treturn fmt.Sprintf(&quot;%x&quot;, h), nil&#125;\n\n控制器通过计算当前 IPBlock 的 spec​ 字段的哈希值（HashSpec(ipblock.Spec)​），与之前记录在 status.lastSpecHash​ 中的哈希值进行对比，从而判断用户配置是否发生变更。\n\n如果一致（即 LastSpecHash == currentHash​），且 Phase != pending​，并且没有设置 trigger == true​，说明：\n\n用户未修改spec​\n控制器已经执行过封禁或跳过处理\n当前状态与期望一致\n\n因此就会跳过本次处理，保证了控制器的幂等性，避免了重复封禁\n\n若哈希值发生变化或设置了 trigger == true​，说明用户期望重新执行封禁逻辑，此时控制器将继续执行 Step 5 封禁操作。\n\n\n封禁操作在 Step 5 中，控制器根据 spec.Duration​ 字段判断封禁时长：\n\n如果 Duration​ 为空字符串，表示永久封禁，设置 isPermanent = true​，并不传递具体时长；\n\n如果 Duration​ 非空，则调用 time.ParseDuration​ 解析该字符串为具体时长 banSeconds​（秒数）；\n\n如果解析失败，则更新状态为失败，并返回错误；\n\n\n\n随后调用适配器接口 r.Adapter.Ban(ip, isPermanent, banSeconds)​ 执行封禁操作。\n\n若封禁失败，更新状态为失败，并通过Notify发送错误通知；\n若封禁成功，更新状态为成功，包括更新时间戳、记录最后的 spec 哈希等信息，并通过通知器发送封禁成功通知。\n\n最后，若为非永久封禁，且未曾自动解封，则启动一个协程异步执行自动解封计划。\n‍\n对于自动解封函数scheduleAutoUnblock​，主要思路就是time.Sleep(传入的时间)​阻塞当前协程等待这段时间（即封禁到期时间），然后执行r.Adapter.UnBan(ipblock.Spec.IP)​，对于成功&#x2F;失败，更新状态并通知。\n状态更新函数func (r *IPBlockReconciler) UpdateIPBlockStatus(ctx context.Context, ipblock *opsv1.IPBlock, updateFn func(*opsv1.IPBlock)) (*opsv1.IPBlock, error) &#123;&#125;\n\n目的是为了安全地更新Status字段，特别解决并发更新是的版本冲突问题\n\n定义一个变量 latest​，用来存储每次重试时获取的最新 IPBlock 对象。\n\n调用 retry.RetryOnConflict(retry.DefaultBackoff, func() error {...})​，该函数会捕获因为版本冲突导致的更新失败，并自动按照默认重试策略重试。\n\n在重试函数内部：\n\n先调用 r.Get(...)​ 从 API Server 获取该 IPBlock 的最新版本，避免基于过时版本更新造成冲突。\n调用传入的回调函数 updateFn(&amp;latest)​，让调用方修改 latest​ 对象的 Status 字段。\n调用 r.Status().Update(ctx, &amp;latest)​ 仅更新对象的 Status 子资源（符合 Kubernetes 设计，Status 和 Spec 可分开更新）。\n\n\n如果 Update​ 出错且是版本冲突错误，RetryOnConflict​ 会自动重试，直到成功或超过最大重试次数。\n\n如果最终失败，记录错误日志。\n\n返回最新状态的对象指针和错误。\n\n\n通知模板与动态参数替换这里主要使用的 Lark 的 card json 模板，将需要替换的部分写成变量形式。\nson...&quot;elements&quot;: [              &#123;                &quot;tag&quot;: &quot;markdown&quot;,                &quot;content&quot;: &quot;&lt;font color=\\&quot;grey\\&quot;&gt;告警内容&lt;/font&gt;\\n**$&#123;ip&#125;**\\n 发起请求过于频繁（1分钟内 **$&#123;count&#125;** 次）&quot;,                &quot;i18n_content&quot;: &#123;                  &quot;en_us&quot;: &quot;&lt;font color=\\&quot;grey\\&quot;&gt;Alert details&lt;/font&gt;\\nMobile client crash rate at 5%&quot;                &#125;,...\n\n然后在使用时，动态传入参数即可。（模板类型，传递参数）\nif r.Notifier != nil &#123;\t\t\tlogger.Info(&quot;Notifier found, sending ban notification&quot;, &quot;ip&quot;, ip)\t\t\terr := r.Notifier.Notify(ctx, &quot;ban&quot;, map[string]string&#123;\t\t\t\t&quot;alarm_time&quot;: time.Now().Format(&quot;2006-01-02 15:04:05&quot;),\t\t\t\t&quot;ip&quot;:         ip,\t\t\t\t&quot;count&quot;:      countExtra(ipblock.Spec.Reason), // 这里填实际count\t\t\t&#125;)\t\t\tif err != nil &#123;\t\t\t\tlogger.Error(err, &quot;发送封禁通知失败&quot;, &quot;ip&quot;, ip)\t\t\t&#125; else &#123;\t\t\t\tlogger.Info(&quot;发送封禁通知成功&quot;, &quot;ip&quot;, ip)\t\t\t&#125;\t\t&#125; else &#123;\t\t\tlogger.Info(&quot;Notifier is nil, skipping ban notification&quot;)\t\t&#125;\n\nConfigMap热更新在main.go​中watchConfigMap​实现。\nfunc watchConfigMap(ctx context.Context, mgr ctrl.Manager, reconciler *controller.IPBlockReconciler) &#123;&#125;\n\n\n通过 mgr.GetCache().GetInformer(ctx, &amp;corev1.ConfigMap{})​ 获取 ConfigMap 的 Informer\n\n\nInformer 是 controller-runtime 提供的机制，底层封装了 Kubernetes 的缓存和事件监听功能。\n\n它会监听集群中 ConfigMap 资源的新增、更新、删除事件，触发相应回调函数。\n\n\n\n通过 AddEventHandler​ 注册事件处理函数，即注册一个”当资源发生变化时需要执行的函数”\n\n\nAddFunc：当监听的 ConfigMap 新建时触发。\nUpdateFunc：当监听的 ConfigMap 更新时触发。\n\n\n具体处理逻辑：\n\na. 读取并更新关键配置字段\n\ngatewayHost​：网关地址，变更时调用 reconciler.UpdateGatewayHost​ 动态更新。\n\n白名单：调用 config.LoadWhitelistFromConfigMap​ 加载最新白名单，更新 reconciler.UpdateWhitelist​（并发安全更新）。\n\n适配器名称（engine​ 字段）：更新适配器实例。\n\n\nb. 触发器（Triggers）加载和重启\n\n从 ConfigMap 的 trigger​ 字段加载触发器配置。\n\n停止所有旧触发器，重新创建并注册新触发器，最后启动它们。\n\n\nc. 通知（Notify）配置加载\n\n读取通知类型、Webhook URL 和模板。\n\n动态创建新的通知实例（如飞书 LarkNotify），替换旧实例，实现通知配置的热切换。\n\n\n并发处理IPBlock-Operator-Plus在实际使用中可能会出现并发问题，主要会出现以下三种情况：\n\n场景举例：\n\n单 Trigger 重复报警：Grafana 配置有误，1分钟内对同一 IP 发出10次报警。\n多个 Trigger 同时触发：比如 Prometheus 和 Grafana 同时监控 TCP 和 HTTP 流量，触发了对同一 IP 的告警。\n多个 IP 并发告警：高峰期系统受到 SYN Flood 攻击，多个 IP 同时发起连接，产生大量告警。\n\n\n\n如果单Trigger频繁报警，POST了多个Webhook，可能会出现重复CR资源。\n\n如果有多个Trigger，多个Trigger通知了同一个 IP，也可能出现重复CR资源，或者出现冲突。\n\n多个Trigger通知不同 IP，可能出现资源竞争或冲突。\n\n\n针对这三种情况，在internal/utils​下添加了一些工具专门处理这几种并发问题。\n\ndebouncer.go​：定义 Debouncer 接口，拥有一个ShouldAllow(key string) bool​方法，用于决定是否允许本次处理。\nlru_debouncer.go​：Debouncer 的接口具体实现。利用LRU处理Webhook风暴，避免因为大量 Webhook 通知同一个 IP，导致出现大量重复的 CR 资源。\ngen_name.go​：对 IPBlock CR 名进行唯一性标识，避免多个 Trigger 通知同一个 IP，导致出现大量重复的 CR 资源。\nIPLock.go​：用于管理 IP 维度的并发锁，避免多个 Trigger 并发冲突或重复创建 CR ，在同一时间只允许一个 Trigger （或一个 goroutine）处理同一个 IP。\n\nLRU 防抖机制确保同一个 IP 只会有一个资源，防止 Webhook 风暴。\n\nLRU（Least Recently Used）缓存淘汰策略，是一种缓存淘汰机制。它会淘汰最近最少被访问的数据，腾出空间给新数据。\n具体是通过维护两个链表：活跃链表和非活跃链表实现的。\n有新数据访问时，把新数据插入非活跃链表的尾部。\n如果访问了非活跃链表中的数据，就把这个数据放在活跃链表的头部。\n如果访问了活跃链表中的数据，就把这个数据放在活跃链表的头部。\n如果缓存满了，再插入时，淘汰位于链表最后的数据。\n\n逻辑保存在utils/lru_debouncer.go​中，这里是对 Debouncer 接口的实现。主要有两个方法：\n// 使用&quot;k8s.io/utils/lru&quot;包，生成一个 LRU 防抖器。func NewLRUDebouncer(size int, ttl time.Duration) *lruDebouncer &#123;&#125;// 检查当前行为是否合法。func (d *lruDebouncer) ShouldAllow(ip string) bool &#123;&#125;\n\n核心方法是ShouldAllow​，这个方法做了下面几件事：\n\n给 cache 加上互斥锁，防止并发请求中对其访问和修改。\n在缓存中检查是否有该 IP 的记录\n有的话，就检查它上次触发时间 ts 和现在 time.Since(ts) 的间隔。\n如果间隔时间小于设定的 TTL，说明是重复触发了，就返回 false，拒绝本次操作。\n\n然后在internal/trigger/grafana.go​中应用防抖（在创建 CR 之前）\n// 防抖，防止重复创建 CRif !g.Debouncer.ShouldAllow(ip) &#123;\tlogger.Info(&quot;[grafana] Skip duplicate IPBlock within TTL&quot;, &quot;ip&quot;, ip)\treturn&#125;\n\n最后在main.go​中，注册防抖器。\n我们定义了防抖频率规则：\n\n对于不同 IP，LRU 中最多保存1000个IP，达到1000个后会自动淘汰最近最少使用的 IP。\n对于同一个IP，如果最近60s内发生过一次封禁，那么这60s内再次收到该IP的相同请求时，会被防抖识别为重复。\n\n// 选择触发器func CreateTriggerByConfig(cfg TriggerConfig, mgr ctrl.Manager) trigger.Trigger &#123;\tswitch cfg.Name &#123;\tcase &quot;grafana&quot;:\t\treturn &amp;trigger.GrafanaTrigger&#123;\t\t\tClient: mgr.GetClient(),\t\t\tAddr:   cfg.Addr,\t\t\tPath:   cfg.Path,\t\t\t// 1000 个 IP， 60 秒防抖\t\t\tDebouncer: utils.NewLRUDebouncer(1000, 60*time.Second),\t\t\tIPLocker:  utils.NewIPLock(),\t\t&#125;\t// TODO 其他触发器 ...\tdefault:\t\treturn nil\t&#125;&#125;\n\n唯一性 CR 名称确保同一个 IP 只会有一个资源。\n\n既然可能会出现重复的 CR 资源，那我们就通过一些特征来让每个 CR 唯一化，这样多个 Trigger 在上报相同 IP 时，生成的 CR 名也是相同的。K8s不会出现相同名的 CR ，这样就有效避免了大量相同 CR 消耗资源的并发问题。\n\n逻辑保存在utils/gen_name.go​中，主要是对 IP 进行 md5 hash，取8位，然后和ipblock-​进行拼接。\nfunc GenCRName(ip string) string &#123;\thash := md5.Sum([]byte(ip))\treturn &quot;ipblock-&quot; + hex.EncodeToString(hash[:8]) // 16位&#125;\n\n然后我们在internal/trigger/grafana.go​中应用，在创建 CR 之前，先判断 CR 是否已经存在（g.Client.Get()​ + apierrors.IsNotFound(err)​）。\ncrName := utils.GenCRName(ip)// 先判断 CR 是否已经存在var existing opsv1.IPBlockerr := g.Client.Get(context.Background(), client.ObjectKey&#123;\tName:      crName,\tNamespace: &quot;default&quot;,&#125;, &amp;existing)if err == nil &#123;\tlogger.Info(&quot;[grafana] IPBlock already exists, skip creation&quot;, &quot;ip&quot;, ip)\treturn&#125;\t// 如果 NotFound，继续创建if err != nil &amp;&amp; !apierrors.IsNotFound(err) &#123;\tlogger.Error(err, &quot;[grafana] Error checking IPBlock existence&quot;, &quot;ip&quot;, ip)\treturn&#125;\n\nIP锁机制确保同一时间只允许一个 Trigger （或一个 goroutine）处理同一个 IP 的封禁逻辑。\n\n假设同一时刻，这两个 trigger 几乎同时收到了针对 IP 1.2.3.4​ 的 webhook：\n\nTriggerA 开始处理：打算创建一个 IPBlock​ 资源\nTriggerB 也收到相同 IP 的告警，也想创建一个同样的 IPBlock​\n\n如果没有加锁：\n\n两个 trigger 可能几乎同时进入了 Client.Create(...)​\n结果 Kubernetes 可能报错 AlreadyExists​\n状态可能冲突，日志也会乱\n\n\n逻辑保存在utils/IPLock.go​中，主要做三件事：\ntype IPLock struct &#123;\tglobal sync.Mutex // 保护 locks map\tlocks  map[string]*sync.Mutex // 全局锁，用来保护 locks 这个 map，防止并发读写它时发生竞态。&#125;\n\n// 新建一个锁管理器func NewIPLock() *IPLock &#123;&#125;// 获取某个 IP 的锁func (i *IPLock) Lock(ip string) &#123;&#125;// 释放某个 IP 的锁func (i *IPLock) Unlock(ip string) &#123;&#125;\n\n核心是Lock​，它做了这几件事：\n\n新建一个全局锁，防止多个 goroutine 访问 map 出现问题。\n查看该 IP 是否有对应的锁，没有就新建一个锁，并加入map中。\n再释放全局锁。\n最后调用具体 IP 的lock.Lock()​，阻塞等待获取锁。\n\n然后应用在internal/trigger/grafana.go​，对预警循环时，会循环获取到每个 IP，这里我们使用匿名函数，为了确保每个 IP 的锁在处理完成后能及时释放，避免因 defer​ 延迟释放导致的锁阻塞，我们为每个 IP 的处理逻辑包裹一个匿名函数：\n// 避免并发时的竞争和死锁func(ip string) &#123;\tg.IPLocker.Lock(ip)    defer g.IPLocker.Unlock(ip)\t...&#125;(ip)\n\n这样设计的好处是每轮循环的 defer 都会在当前匿名函数结束时释放锁，避免多个 IP 的处理互相阻塞，保证并发安全。\n","categories":["Kubernetes"],"tags":["Achievements","Kubernetes","Operator"]},{"title":"关于GZCTF反向代理配置不当导致平台在高并发下崩溃","url":"/posts/23988.html","content":"\n## 环境\n\nGZCTF单实例部署在172.20.14.20、172.20.14.110、172.20.14.111三台机器的K8s集群上，使用LoadBalancer对外暴露服务，LoadBalancer IP由MetalLB分得：172.20.14.118\n问题描述2024年11月16日，2024 Redrock CTF开赛，这次比赛使用了新的比赛平台GZ::CTF - GZ::CTF Docs。\n由于反向代理配置不当，导致平台在刚开赛时就因为高并发而立即崩溃，报错显示错误代码429，并且平台响应很慢，无法正常刷新。\n\n\n排查过程在遇到429问题后，首先是检查了配置文件中和访问限制有关的参数配置，都适当的调大，发现并没有什么改善，没有解决问题。\n\nGZCTF是单实例部署的，为了让流量分流，我将Pod扩容，但是单实例部署与多实例部署存在差别，并且据官方介绍单实例已经能满足大部分的一般比赛（满足2000人200题），并且是官方最为推荐的部署方式。多实例部署需要配置对象存储，否则会导致数据不一致的问题。\n\n最后在咨询开发者以及在检查日志的过程中发现了问题：\n配置文件中的ForwardedOptions的TrustedNetworks和TrustedProxies字段使用的是默认值，反向代理并没有生效，logs里客户端的ip全是一致的，大量的相同ip可能超过了平台的访问阈值，最后平台很容易就崩溃了。\nplaintext[24-11-16 08:14:30.307 DBG] RequestLoggingMiddleware: [200]     7.10ms HTTP GET    /api/account/profile @ ::ffff:172.20.14.20 [24-11-16 08:14:30.704 DBG] RequestLoggingMiddleware: [404]     0.22ms HTTP GET    /hub/user @ ::ffff:172.20.14.20 [24-11-16 08:14:30.986 DBG] RequestLoggingMiddleware: [404]     1.32ms HTTP GET    /hub/user @ ::ffff:172.20.14.20\n\n\n解决注意将ForwardedOptions的\n\nTrustedNetworks值为172.20.14.20&#x2F;24\nTrustedProxies值为172.20.14.118，即GZCTF的LoadBalancer IP\n\n整个proxy chain\nplaintext172.20.14.2（网关机）-&gt; 172.20.14.118（LoadBalancer） -&gt; 172.20.14.20 （master1）                                                    -&gt; 172.20.14.110（master2）                                                    -&gt; 172.20.14.111 （master3）","categories":["Trouble Shooting"],"tags":["Trouble Shooting"]},{"title":"工作负载与服务发现","url":"/posts/31706.html","content":"\n在没有工作负载之前，我们都是直接手动创建一个一个的Pod。但是如果当我们需要将一个复杂的服务部署到庞大的集群上并且实现高可用，我们总不能每个节点上都部署一遍吧。况且如果你可以将所有的Pod手动都创建好，但是如果镜像发生了更新，我们也需要将一个一个的Pod手动进行更新。太麻烦了，这个时候我们就可以使用工作负载，它可以帮助我们更高的管理Pod。\n\n工作负载\n工作负载 | Kubernetes\n\n工作负载是在 Kubernetes 上运行的应用程序。\n为了减轻用户的使用负担，通常不需要用户直接管理每个 Pod。 而是使用负载资源来替用户管理一组 Pod。 这些负载资源通过配置控制器来确保正确类型的、处于运行状态的 Pod 个数是正确的，与用户所指定的状态相一致。\nKubernetes 提供若干种内置的工作负载资源\n\nDeployment和ReplicaSet\nStatefulSet：有状态服务，为 Pod 提供持久存储和持久标识符。\nDaemonSet：守护型应用部署，如日志、监控组件。\nJob和CronJob：定时任务部署，指定时间运行。\n\nDeploymentDeployment是Kubernetes中用于定义Pod副本集的对象，它负责管理应用的部署和更新。通过Deployment，可以实现应用的滚动更新、回滚以及自动修复。\n工作模式Deployment并不直接管理Pod，而是管理ReplicaSet，ReplicaSet再管理Pod。在我们创建 Deployment 的时候，它会用自己的 Pod 规范创建一个 ReplicaSet。当更新一个 Deployment 并修改副本数量时，它会把更新内容传递给下游的 ReplicaSet。\n一个 ReplicaSet 对象就是由副本数目的定义和一个 Pod 模板组成的, 它的定义就是 Deployment 的一个子集。它主要关注的是Pod实例的数量，并在Pod出现问题时，维持所需副本数量。\nDeployment则是建立在ReplicaSet之上的更高级别的抽象，用于管理Pod副本集和应用程序的部署。然后它多了滚动更新、回滚以及自动修复这些功能。\n\n主要组成部分\n模板（Template） ：定义了要创建的Pod的规范，包括镜像、环境变量、卷等。当副本数不足时会根据模板新建Pod。\n副本数（Replica Count） ：指定了希望运行的Pod副本数量。\n升级策略（Update Strategy） ：定义了应用更新的策略，包括滚动更新、Recreate等。\n标签选择器（Label Selector） ：用于选择要进行管理的Pod副本集。\n滚动升级（Rolling Update） ：一种升级策略，通过逐步替换旧的Pod实例来实现平滑的升级。\n\n特点\n滚动更新：保证更新过程不中断服务。\n副本管理：保证始终有指定数量的实例在运行。\n自动修复：当Pod发生故障时会自动替换为新的Pod。\n版本回滚：允许用户回滚到先前的版本，以应对更新带来的问题\n\n示例：使用Deployment部署NginxapiVersion: apps/v1 # API版本kind: Deploymentmetadata: # 元数据部分  name: nginx-deployment # 资源名称  namespace:   labels: # 给deployment打的标签    app: nginx # 名为app值为nginx的标签spec: # 定义部署对象的规范  replicas: 3 # 副本数量  selector: # 选择器部分，用于指定哪些Pod副本属于此部署    matchLabels: # 匹配标签部分。Service通过这里来匹配Pod      app: nginx # 指定要匹配的标签为app=nginx，用于选取具有这个标签的Pod副本  template: # 模板部分    metadata:      labels:        app: nginx # 需要与selector设置的标签一致    spec:      containers: # 用于定义Pod中的容器      - name: nginx # 容器名称        image: reg.redrock.team/library/nginx:latest        ports:        - containerPort: 80 # 暴露容器的80端口供外部访问\n\nkubectl apply -f deploy.yaml\n\n可以看到RS的NAME是在Deployment之后加了一段字符串，而Pod是在RS后又加了一段字符串，说明Pod其实是由RS直接管理的。\n副本伸缩方式一：命令行\nkubectl scale deploy NAME --replicas=pod数量 -n NAMESPACE\n\n方式二：编辑yaml文件\n修改replicas值即可。\nPod为什么要有副本数？\n\n使用多个副本来运行同一个服务，可以提高应用的可用性。如果一个Pod因为故障而被杀死，其他Pod会立即来替代它，从而保证了服务的稳定性。\n多个副本可以将流量分散到不同的Pod，从而实现负载均衡，避免所有流量集中在一个Pod上导致负载过高。流量高时，我们可以增加副本数，少时可以减少副本数，从而节省资源。\n多个Pod可以实现平滑的滚动更新。\n\n镜像更新Deployment支持两种镜像更新策略：重建更新（Recreate）​和​滚动更新（默认）（RollingUpdate） ，可以通过strategy选项进行配置。\n重建更新（Recreate）\n在spec下增加如下字段\nspec:  strategy:  # 策略    type: Recreate  # 重建更新策略\n\n滚动更新（RollingUpdate）\nstrategy:  type: RollingUpdate # 滚动更新策略，可以不加，因为默认就是滚动更新  rollingUpdate:    maxUnavailable: 25%    maxSurge: 25%\n\nMaxSurge和MaxUnavailable，这两个参数决定了更新过程的速度。这两个参数可以是 Pod 数量，也可以是 Deployment 的实例数量百分比。\n为什么要有这两个参数？为什么要设置更新过程的速度？\n\n我们的服务很重要，对服务的变更需要非常谨慎。因此决定在关闭旧版本Pod之前，需要首先启动新Pod。只有新 Pod 启动、运行并就绪之后，才能杀死旧 Pod。\n假如我们的集群已经满载，无法负担多余的Pod消耗，我们自然需要先关掉旧Pod，然后才启动新Pod。\n\n当MaxUnavailable设置为 0 意味着：“在新 Pod 启动并就绪之前，不要关闭任何旧 Pod”。\n当MaxSurge设置为 100% 的意思是：“立即启动所有新 Pod”，也就是说我们有足够的资源，我们希望尽快完成更新。\n这两个默认值是25%，假如我们有3个副本，意味着更新时允许 25%（0.75个取整为1） 的Pod处于不可用状态，允许最多增加1个新Pod。\n版本回退Deployment支持版本升级过程中的暂停，继续功能以及版本回退等诸多功能。\nkubectl rollout option NAME -n NAMESPACEkubectl rollout：版本升级相关功能，支持下面的选项：status：显示当前升级状态history：显示升级历史记录pause：暂停版本升级过程resume：继续已经暂停的版本升级过程restart：重启版本升级过程undo：回滚到上一级版本（可以使用--to-revision回滚到指定版本）\n\n金丝雀发布Deployment支持更新过程中的控制，如”暂停（pause）”或”继续（resume）”更新操作。\n有时我们不想让测试版本影响所有用户，即使是短时间也不行。所以我们可以部分推出新版本。例如我们部署新旧两组实例，1% 的流量发送给新版本，先筛选一小部分的用户请求路由到新的pod应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的pod资源滚动更新，否则立即回滚更新操作。\n现在的镜像版本是latest，切换版本到1.19.2，变更途中使用pause\nkubectl set image deploy nginx-deployment nginx=reg.redrock.team/library/nginx:1.19.2 -n lesson-demo&amp;&amp; kubectl rollout pause deploy nginx-deployment -n lesson-demo\n\n发现1.19.2版本只增加了一个RS，而latest版本没有减少\n\n查看更新状态，发现deployment正在等待更新，并且一个已经更新完成。\n\n然后继续更新\nkubectl rollout resume deploy nginx-deployment -n lesson-demo\n\n全部更新完成\n\n服务发现与负载均衡\n服务（Service） | Kubernetes\n\n为什么需要服务发现？传统的部署应用服务方式都是直接部署在给定的机器上，访问服务时，我们只需要访问该机器的IP即可。但K8s集群中的应用都是通过Pod去部署的，而 pod 生命周期是短暂的。在 Pod 的生命周期过程中，比如它创建或销毁，它的 IP 地址都会发生变化，这样就不能使用传统的部署方式，不能指定 IP 去访问指定的应用。\nService前面我们通过Deployment创建了一组Pod，这些Pod组需要提供一个统一的访问入口，以及怎么去控制流量负载均衡到这个组里面。其实我们可以在部署时就提供一个模板以及访问方式，使应用服务暴露在外部。这时就需要服务发现，也就是K8s中的Service。\n示例：通过Service暴露Nginx服务apiVersion: v1kind: Servicemetadata:  name: nginx-service  namespace: lesson-demospec:  selector:    app: nginx # 匹配具有该标签的Pod  ports:    - protocol: TCP       # 服务监听的端口号，当其他服务或外部客户端与该服务通信时，它们将使用这个端口号进行通信，该端口号是服务对外部暴露的入口。即通过clusterIP: port可以访问到某个service      port: 9376      # targetPort可以直接指定Pod的端口，也可以指定Pod端口所对应的名称。      # targetPort一定要与containerPort暴露出的端口对应      targetPort: 80\n\nService的类型ClusterIP\n\n只在集群内部生效的IP，集群内所有节点都能访问到它。\nClusterIP类型也是我们不指定类型时的默认Service类型。\n\napply之后，我们就可以通过访问ClusterIP:PORT的形式来访问到我们的服务。\n访问方式：ClusterIP:PORT\n\n这个 IP 地址就是 Service 的 IP 地址，这个 IP 地址在集群里面可以被其它 pod 所访问，相当于通过这个 IP 地址提供了统一的一个 pod 的访问入口，以及服务发现。而Endpoints是每个Pod的后端IP。\nNodePort\n当需要从集群外部访问Service时，就可以使用NodePort。\nNodePort会在每个NodeIP上启用一个相同的端口来暴露服务。NodeIP就是我们在创建集群时指定的节点IP，端口限定范围为30000-32767。\n当我们访问NodeIP:PORT时，流量会由 kube-proxy 网络组件进一步到给对应的 Pod。\n访问方式：NodeIP:PORT\napiVersion: v1kind: Servicemetadata:  name: nginx-service  namespace: lesson-demospec:  type: NodePort  selector:    app: nginx   ports:    - port: 80      targetPort: 80      nodePort: 30007\n\nLoadBalancer\nNodePort会在每台Node上监听端口接收用户流量，但在实际情况下，对用户暴露的只会有一个IP和端口，那这么多台Node该使用哪台让用户来访问呢？而且如果只访问一个NodeIP，那么这个节点压力会很大。\n这时就需要前面加一个公网负载均衡器为项目提供统一访问入口了。\nLoadBalancer 通常是云服务厂商提供的负载均衡器，我们通过外部的负载均衡器将流量路由到创建的服务商。\n将Service的类型设置为LoadBalancer后，我们有两个选择，一个是设置负载均衡器的IP，另一种负载均衡器会自动为我们分配一个ExternalIP作为出口。\n\n访问方式：EXTERNALIP:PORT\napiVersion: v1kind: Servicemetadata:  name: nginx-servicespec:  type: LoadBalancer  selector:    app: nginx   ports:    - port: 80      targetPort: 80\n\nexternalIPs\n一般我们的服务只暴露了ClusterIP，而NodePort的端口又有30000-32767的范围限制。这时我们就可以配置上externalIP，表示来自externalIP的流量能进入我们的服务，\nexternalIP一般填负载均衡器的IP。\n注意：externalIPs一定不能填NodeIP，不然会导致该节点直接崩溃（calico、kubelet、kube-proxy等组件无法与apiserver进行通信），出现The connection to the server apiserver.cluster.local:6443 was refused - did you specify the right host or port?\n原因分析可以看k8s IPVS模式下externalIP导致节点故障浅析_external-ip-CSDN博客\napiVersion: v1kind: Servicemetadata:  name: nginx-servicespec:  externalIPs:  - 1.2.3.4   selector:    app: nginx   ports:    - port: 80      targetPort: 80\n\nService四种类型的对比\n\n\n类型\n用途介绍\n\n\n\nClusterIP\n默认类型，自动分配一个仅clusterIp内部可以访问的虚拟ip地址\n\n\nNodePort\n在ClusterIP基础上为Service机器上绑定一个端口，这样就可以通过NodePort来访问服务\n\n\nLoadBalancer\n在NodePort基础上借助云服务商创建一个外部负载均衡器，并将请求转发到NodePort来访问服务\n\n\nexternalIPs\n把集群外部的服务引入到集群内部来，在集群内部直接使用\n\n\nIngress和IngressRoute没有Ingress之前，我们使用NodePort对外暴露服务，但当服务多了以后，会存在一些弊端，比如暴露太多的的端口，无法实现域名转发等等。\nIngress其实就是从Kubernetes集群外部访问的一个入口，它可以帮助我们通过不同域名来将流量匹配到对应的服务，类似于Nginx。\nIngress在Service上面一层，可以定义集群外部到集群内Service的HTTP和HTTPS的路由。\n\n使用方法：先自行部署IngressController → 再部署Ingress资源。\nIngressController的功能：\n\n接受外部流量，并将请求负载均衡到内部Pod\n部署Ingress路由转发规则\n监控Pod，并在添加或删除Pod后自动更新负载均衡规则。\n\n可以把Ingress controller理解为一个监听器，它可以不断地与 kube-apiserver 打交道，实时的感知后端 service、pod 的变化，当得到这些变化信息后，Ingress controller 再结合 Ingress 的配置，更新反向代理负载均衡器，达到服务发现的作用。\nIngress Controller的实现有多种，比如Ingress-Nginx、Traefik等，由于Ingress-Nginx的配置比较麻烦，所以我们一般使用它的替代品Traefik。\nTraefik Ingress Controller优势：\n\n部署过程简单\n使用Go语言开发，完美贴合K8s\nTraefik衍生的IngressRoute配置更加简洁，我们一般使用IngressRoute\n拥有多种中间件，可以用于将请求路由到目的地之前或之后来调整请求。\n\n安装使用Helm安装（Helm是K8s中的一个包管理器）\n# helm安装$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3$ chmod 700 get_helm.sh$ ./get_helm.sh\n\nhelm repo add traefik https://traefik.github.io/chartshelm repo listhelm repo updatehelm pull traefik/traefikkubectl create namespace traefikhelm install traefik traefik/traefik -n traefik\n\n部署完成后，我们就可以把某个域名解析到我们的集群（下面把k8s.yiiong.top解析到172.20.14.20），然后我们可以配置IngressRoute规则，Traefik就可以根据IngressRoute规则来将流量路由到对应的服务，并为每个服务提供独立的域名。\n路由实现Traefik有两个服务入口（entryPoint）\n\n80，对应entryPoint:web\n443，对应entryPoint:websecure\n\nTraefik默认以NodePort形式暴露，对于如下的Traefik来说，它的外部入口就是30350和31971，从30350进入80入口，从31971进入443入口，具体规则可以在values.yaml中修改。\n如果我们不想通过域名:端口（k8s.yiiong.top:30350）,我们可以通过配置Nginx的反向代理，实现直接访问域名到我们对应的服务。\nNAME              TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)                      AGEservice/traefik   NodePort   10.96.1.38   &lt;none&gt;        80:30350/TCP,443:31971/TCP   174d\n\n路由实现\n\nIngressRouteIngress是官方的东西，而IngressRoute则是Traefik Ingress Controller封装好的一种特殊Ingress资源。\n配置一个HTTP的IngressRoute\napiVersion: traefik.containo.us/v1alpha1kind: IngressRoutemetadata:  name: nginx-http  namespace: lesson-demospec:  entryPoints: # 入口点，入口点是在我们Ingress控制器中定义的入口端口，表示进入该端口的服务才遵循该IngressRoute的路由  - web  routes:  - match: Host(`k8s.yiiong.top`) # 匹配这个域名    kind: Rule # 路由类型的规则    services:    - name: nginx-service # 匹配名为nginx-service的svc      port: 9376 # 这个svc暴露在9376，二者要相同\n\n接下来访问http://k8s.yiiong.top:30350即可访问到我们对应的服务。\n\n配置HTTPS的IngressRoute\n我们这里先使用openssl来生成一个自签证书。\nopenssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -sha512 -days 36500 \\  -subj &quot;/C=CN/ST=Chongqing/L=Chongqing/O=Redrock/OU=Personal/CN=k8s.yiiong.top&quot; \\  -key ca.key \\  -out ca.crt    openssl genrsa -out tls.key 4096 openssl req -sha512 -new \\ -subj &quot;/C=CN/ST=Chongqing/L=Chongqing/O=Redrock/OU=Personal/CN=k8s.yiiong.top&quot; \\ -key tls.key \\ -out tls.csr openssl x509 -req -sha512 -days 3650 \\    -CA ca.crt -CAkey ca.key -CAcreateserial \\    -in tls.csr \\    -out tls.crt\n\n创建证书的Secret（K8s中一种存储和管理敏感数据的资源类型）\nkubectl create secret -n lesson-demo generic traefik-tls --from-file=tls.crt --from-file=tls.key\n\n配置Yaml\napiVersion: traefik.containo.us/v1alpha1kind: IngressRoutemetadata:  name: nginx-https  namespace: lesson-demospec:  # 这里指定https只能走443端口  entryPoints:  # 使用websecure作入口    - websecure  routes:    - kind: Rule      match: Host(`k8s.yiiong.top`)      services:        - name: nginx-service          port: 9376  # tls相关配置  tls:  # 指定证书，是我们前面创建的证书    secretName: traefik-tls\n\n接下来访问https://k8s.yiiong.top:31971\n\n\n不在集群里使用HTTPS\n\n在集群中使用HTTPS太麻烦了，要配这么多东西，有没有什么更好的办法可以把它绕开，然后我们也能使用HTTPS请求呢？\n答案是在集群中配置一个网关机，让这个网关机来控制流量的进入，我们只需要关注流量进入网关后，把tls给卸载掉，最后用http的流量进入我们的集群，这种方案即安全，还方便，不过缺点是只适合在多节点的集群中使用\n\n中间件middleware主要用来对HTTP请求做一些修改\n\nHTTP强制跳转HTTPS\n实现效果：访问http://k8s.yiiong.top:30350自动跳转至https://k8s.yiiong.top:31971\n先apply刚才的HTTPS的IngressRoute。\n然后配置一个中间件\n这里会用到RedirectScheme的内置中间件，配置如下：\napiVersion: traefik.containo.us/v1alpha1kind: Middlewaremetadata:  name: redirect-https-middleware  namespace: lesson-demospec:  redirectScheme:    scheme: https    port: &quot;31971&quot; # 由于我的HTTPS在31971端口，指定一下端口，让其重定向到31971\n\n然后在HTTP的Yaml文件里最后添加刚刚创建的中间件。\nmiddlewares:  - name: redirect-https-middleware\n\n可见HTTP被成功重定向到HTTPS\n\n去除请求前缀\n有时候我们只有一个域名，但我们想通过这一个域名访问不同的应用。\n在Nginx中，我们可以配置Location来匹配流量，Traefik也可以这么做。\n但是定制不同的前缀后，由于应用本身并没有这些前缀，导致请求返回404，这时候我们就需要对请求的path进行处理。\n先创建一个带前缀的IngressRoute\napiVersion: traefik.containo.us/v1alpha1kind: IngressRoutemetadata:  name: nginx-http  namespace: lesson-demospec:  entryPoints:  - web  routes:  - match: Host(`k8s.yiiong.top`) &amp;&amp; PathPrefix(`/nginx`)    kind: Rule    services:    - name: nginx-service      port: 9376\n\n直接访问会返回404\n\n配置中间件stripPrefix\napiVersion: traefik.containo.us/v1alpha1kind: Middlewaremetadata:  name: prefix-url-middleware  namespace: lesson-demospec:  stripPrefix:    prefixes:      - /nginx\n\n修改IngressRoute，使用中间件\nmiddlewares:   - name: prefix-url-middleware\n\n\n灰度发布\n灰度发布我们有时候也会称为金丝雀发布（Canary），主要就是让一部分测试的服务也参与到线上去，经过测试观察看是否符合上线要求。\n假设一个应用现在运行着V1版本，新的V2版本需要上线，这时候我们需要在集群中部署好V2版本，然后通过Traefik提供的带权重的轮询(WRR)来实现该功能。\n这和我们之前讲Nginx带权重的负载均衡很类似。\n需要两个及以上的Service，然后再加一个TraefikService\napiVersion: traefik.containo.us/v1alpha1kind: TraefikServicemetadata:  name: appspec:  weighted:    services:      - name: appv1 # 服务1        weight: 3        port: 80        kind: Service      - name: appv2 # 服务2        weight: 1        port: 80        kind: Service\n\n我们给v1版本配置权重3，给要上线的v2版本配置权重1.也就是说我们3次访问domain.com都会是v1版本，第4次就是v2版本。和我们当时Nginx举的8881和8882的那个例子一模一样。\n创建TraefikService类型后，我们在IngressRoute类型中将service写为traefikService即可\napiVersion: traefik.containo.us/v1alpha1kind: IngressRoutemetadata:  name: app-ingressroute-canary spec:  entryPoints:    - web  routes:  - match: Host(`domain.com`)    kind: Rule    services:    - name: app      kind: TraefikService\n\n待v2版本测试没问题后，就可以将流量全切到v2了。\nTraefik的中间件还有很多，比如Basic Auth（用户身份认证）、ipWhiteList（定义IP白名单）、mirroring（流量复制）等等\n大家可以通过Traefik Proxy Middleware Overview - Traefik进行学习更多的中间件，甚至可以自行开发自定义中间件。\n总结：服务暴露模式\n\n\n暴露模式\n访问方式\n\n\n\nService (NodePort) + Deployment\nNodeIP:NodePort\n\n\nIngressRoute + Service（CluserIP） + Deployment\n域名\n\n\nLoadBalancer + Service + Deployment\n负载均衡器IP:PORT\n\n\n第二种方式是最常用的，之所以Service被设置成ClusterIP，是因为流量进入集群后直接就由Ingress Controller管理，不再通过kube-proxy管理。我们常常将域名解析到集群主机，然后IngressRoute设置相同的主机名，就能实现流量匹配\n作业：\n1.将第三节课课后作业使用FastAPI开发的Web项目部署在K8s上，并通过IngressRoute + Service（CluserIP） + Deployment + Middleware（去前缀） 的方式暴露服务。\n2.部署2个以上的服务在K8s上，利用IngressRoute用域名区分访问。\n最终将服务地址（要求能访问到）和整个部署流程记录文档提交到yiiong@redrock.team\n截止时间5月5日\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"调度与Helm","url":"/posts/55660.html","content":"\n## 调度\n\nKubernetes允许你去影响pod被调度到哪个节点。起初，只能通过在pod规范⾥指定节点选择器来实现，后⾯通过其他机制的逐渐加⼊来扩容这项功能。\n固定节点（NodeName）指定Pod调度到某个节点上并且固定\napiVersion: apps/v1kind: Deploymentmetadata:  name: node-name-test  labels:    app: node-name-testspec:  replicas: 5  selector:    matchLabels:      app: node-name-test  template:    metadata:      labels:        app: node-name-test    spec:      nodeName: master # 指定节点名即可      containers:      - image: nginx        name: nginx\n\n标签选择器根据标签选择可以调度的节点\n给节点打标签\nkubect label nodes master disk-type=ssd # 给名为master的节点打上名为disk-type，值为ssd的标签\n\n去除标签\nkubectl label nodes master disk-type- # 最后一个杠表示取消节点上的特定标签\n\n只有具备这个标签的节点才会被 Deployment 选择来部署 Pod。\napiVersion: apps/v1kind: Deploymentmetadata:  name: node-name-test  labels:    app: node-name-testspec:  replicas: 5  selector:    matchLabels:      app: node-name-test  template:    metadata:      labels:        app: node-name-test    spec:      containers:      - image: nginx        name: nginx      nodeSelector:        disk-type: ssd\n\n污点（Taints）和容忍度（Tolerations）节点污点以及pod对于污点的容忍度⽤于限制哪些pod可以被调度到某⼀个节点。只有当⼀个pod容忍某个节点的污点，这个pod才能被调度到该节点。\n容忍度是通过明确的在pod中添加的信息，来决定⼀个pod可以或者不可以被调度到哪些节点上。\n污点则是在不修改已有pod信息的前提下，通过在节点上添加污点信息，来拒绝pod在某些节点上的部署。\n使用kubectl describe node &lt;node-name&gt;来查看污点Taints\n污点的表现形式为&lt;key&gt;=&lt;value&gt;:&lt;effect&gt;\n在节点上打上污点如果我想给节点打上一个污点\nkubectl taint node master node-role.kubernetes.io/master=:NoSchedule # node-role.kubernetes.io/master是社区推荐的标签命名约定kubectl taint node master node-role.kubernetes.io/master- # 在键名后加-可以去除污点\n\n这会给master节点打上污点，阻止一般Pod被调度到该节点上。一般可以容忍这个污点的Pod都是系统级别的Pod（比如某些系统组件以Pod形式运行）\n而effect有三种\n\nNoSchedule表⽰如果pod没有容忍这些污点，pod则不能被调度到包含这些污点的节点上。\nPreferNoSchedule是NoSchedule的⼀个宽松的版本，表⽰尽量阻⽌pod被调度到这个节点上，但是如果没有其他节点可以调度，pod依然会被调度到这个节点上。\nNoExecute不同于NoSchedule以及PreferNoSchedule，后两者只在调度期间起作⽤，⽽NoExecute也会影响正在节点上运⾏着的pod。如果在⼀个节点上添加了NoExecute污点，那些在该节点上运⾏着的pod，如果没有容忍这个NoExecute污点，将会从这个节点去除。\n\n污点有什么用？\n比如有⼀个单独的Kubernetes集群，上⾯同时有⽣产环境和⾮⽣产环境的流量。其中最重要的⼀点是，⾮⽣产环境的pod不能运⾏在⽣产环境的节点上。我们就可以通过在⽣产环境的节点上添加污点来满⾜这个要求。\n在Pod上添加污点容忍度为了将⽣产环境的Pod部署到生产环境节点上，Pod需要能容忍那些你添加在节点上的污点。\napiVersion: apps/v1kind: Deploymentmetadata:  name: taint-test  labels:    app: taint-testspec:  replicas: 5  selector:    matchLabels:      app: taint-test  template:    metadata:      labels:        app: taint-test    spec:      containers:      - image: nginx        name: nginx      tolerations: # 添加容忍度      - key: node-type        operator: Equal        value: production        effect: NoSchedule\n\nEqual操作符要求污点的键和值必须与容忍度规则中指定的键和值完全匹配。也就是说只有当节点上设置了 key 为 node-type，并且这个 key 对应的值为 production时，才会使得 Pod 具有容忍度，并且可以被调度到带有对应污点的节点上。其他任何不符合该要求的污点都将不会影响 Pod 的调度。\n节点亲和性污点可以⽤来让pod远离特定的节点。而节点亲和性（node affinity）这种机制允许你通知Kubernetes将pod只调度到某个⼏点⼦集上⾯。\n使用首选的节点亲和性调度 Pod本清单描述了一个 Pod，它有一个节点亲和性设置 preferredDuringSchedulingIgnoredDuringExecution，disktype: ssd。 这意味着 Pod 将首选具有 disktype=ssd 标签的节点。pods/pod-nginx-preferred-affinity.yaml 复制 pods/pod-nginx-preferred-affinity.yaml 到剪贴板apiVersion: v1kind: Podmetadata:  name: nginxspec:  affinity:    nodeAffinity:      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: disktype            operator: In            values:            - ssd            containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent\n\n探针Kubernetes 为检查应用状态定义了三种探针，它们分别对应容器不同的状态：\n\nStartup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。\nLiveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。\nReadiness，就绪探针，用来检查应用是否可以接收流量，是否能够对外提供服务。\n\n如果一个 Pod 里的容器配置了探针，Kubernetes 在启动容器后就会不断地调用探针来检查容器的状态：\n\n如果 Startup 探针失败，Kubernetes 会认为容器没有正常启动，就会尝试反复重启，当然其后面的 Liveness 探针和 Readiness 探针也不会启动。\n如果 Liveness 探针失败，Kubernetes 就会认为容器发生了异常，也会重启容器。\n如果 Readiness 探针失败，Kubernetes 会认为容器虽然在运行，但内部有错误，不能正常提供服务，就会把容器从 Service 对象的负载均衡集合中排除，不会给它分配流量。\n\n复制\n# kubectl explain pod.spec.containers.startupProbe# kubectl explain pod.spec.containers.livenessProbe# kubectl explain pod.spec.containers.readinessProbe## kubectl logs ngx-pod-probe -f---# this cm will be mounted to /etc/nginx/conf.dapiVersion: v1kind: ConfigMapmetadata:  name: ngx-confdata:  default.conf: |    server &#123;      listen 80;      location = /ready &#123;        return 200 &#x27;I am ready&#x27;;        #return 500 &#x27;I am not ready&#x27;;      &#125;      location / &#123;        default_type text/plain;        return 200 &quot;Nginx OK&quot;;      &#125;    &#125;---apiVersion: v1kind: Podmetadata:  name: ngx-pod-probespec:  volumes:  - name: ngx-conf-vol    configMap:      name: ngx-conf  containers:  - image: nginx:alpine    name: ngx    ports:    - containerPort: 80    volumeMounts:    - mountPath: /etc/nginx/conf.d      name: ngx-conf-vol    # probes are here    startupProbe:      periodSeconds: 1      timeoutSeconds: 1      exec:        command: [&quot;cat&quot;, &quot;/var/run/nginx.pid&quot;]        #command: [&quot;cat&quot;, &quot;nginx.pid&quot;]  # wrong pid file    livenessProbe:      periodSeconds: 10      timeoutSeconds: 1      #failureThreshold: 1      tcpSocket:        #port: 80        port: 8080    readinessProbe:      periodSeconds: 5      timeoutSeconds: 1      httpGet:        path: /ready        port: 80---\n\n资源配额有了名字空间，我们就可以像管理容器一样，给名字空间设定配额，把整个集群的计算资源分割成不同的大小，按需分配给团队或项目使用。\n名字空间的资源配额需要使用一个专门的 API 对象，叫做 ResourceQuota​，因为资源配额对象必须依附在某个名字空间上，所以在它的 metadata​ 字段里必须明确写出 namespace​（否则就会应用到 default 名字空间）。\n复制\n# kubectl create ns dev-ns $out# kubectl create quota dev-qt $out## kubectl explain quota.spec# kubectl describe -n dev-ns quota dev-qt## kubectl explain limits.spec.limits## kubectl run ngx --image=nginx:alpine -n dev-ns# kubectl describe pod ngx -n dev-ns---apiVersion: v1kind: Namespacemetadata:  name: dev-ns---apiVersion: v1kind: ResourceQuotametadata:  name: dev-qt  namespace: dev-nsspec:  hard:    requests.cpu: 10    requests.memory: 10Gi    limits.cpu: 10    limits.memory: 20Gi    requests.storage: 100Gi    persistentvolumeclaims: 100    pods: 100    configmaps: 100    secrets: 100    services: 10    services.nodeports: 5    count/jobs.batch: 1    count/cronjobs.batch: 1    count/deployments.apps: 1---apiVersion: v1kind: LimitRangemetadata:  name: dev-limits  namespace: dev-nsspec:  limits:  - type: Container    defaultRequest:      cpu: 200m      memory: 50Mi    default:      cpu: 500m      memory: 100Mi  - type: Pod    max:      cpu: 800m      memory: 200Mi---\n\n它需要在 spec​ 里使用 hard​ 字段，意思就是“硬性全局限制”。\n\nCPU 和内存配额，使用 request.*​、limits.*​，这是和容器资源限制是一样的。\n存储容量配额，使 requests.storage​ 限制的是 PVC 的存储总量，也可以用 persistentvolumeclaims​ 限制 PVC 的个数。\n核心对象配额，使用对象的名字（英语复数形式），比如 pods​、configmaps​、secrets​、services​。\n其他 API 对象配额，使用 count/name.group​ 的形式，比如 count/jobs.batch​、count/deployments.apps​。\n\n在名字空间加上了资源配额限制之后，它会有一个合理但比较“烦人”的约束：要求所有在里面运行的 Pod 都必须用字段 resources​ 声明资源需求，否则就无法创建。这个时候就要用到一个很小但很有用的辅助对象了——  LimitRange​ ，简称是 limits​ ，它能为 API 对象添加默认的资源配额限制。\n\nspec.limits​ 是它的核心属性，描述了默认的资源限制。\ntype​ 是要限制的对象类型，可以是 Container​、Pod​、PersistentVolumeClaim​。\ndefault​ 是默认的资源上限，对应容器里的 resources.limits​，只适用于 Container​。\ndefaultRequest​ 默认申请的资源，对应容器里的 resources.requests​，同样也只适用于 Container​。\nmax​、min​ 是对象能使用的资源的最大最小值。\n\nHelm\nHelm | Docs\n\nHelm是一款开源的K8s包管理器，类似于apt，yum或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。\n为什么要使用Helm？\n假如现在需要部署一个复杂且庞大的服务，我需要很多个yaml文件一个一个配好每一种资源对象，这样做很麻烦。Helm很好地解决了这个问题，它能够把这些零零散散的应用资源文件放在一起进行统一配置，从而方便其他人地部署与使用，就和apt install一样，仅仅一句helm install就可以部署好一个现成的服务了。\n安装使用脚本安装，它可以让你知道在执行之前脚本做了什么\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3chmod 700 get_helm.sh./get_helm.sh\n\n或者直接执行安装\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n基本概念Chart 代表着 Helm 包。它包含在 Kubernetes 集群内部运行应用程序，工具或服务所需的所有资源定义。你可以把它看作是 Apt dpkg，或 Yum RPM 在Kubernetes 中的等价物。\nRepository（仓库）  是用来存放和共享 charts 的地方。官方仓库是hub.helm.sh，你也可以用Harbor来存放charts。\nRelease 是运行在 Kubernetes 集群中的 chart 的实例。一个 chart 通常可以在同一个集群中安装多次，每一个release指定不同的名称即可。每一次安装都会创建一个新的 release。\nHelm的工作流程：Helm 安装 charts 到 Kubernetes 集群中，每次安装都会创建一个新的 release。你可以在 Helm 的 chart repositories 中寻找新的 chart。\n基本命令使用helm search用来查找Charts\nhelm search hub wordpress查找Artifact Hub中搜索到所有的wordpress charts\n\nhelm search repo从你添加的仓库中查找chart\n\nhelm install用来安装helm包\nhelm search repo wordpresshelm install wordpress bitnami/wordpress # 从bitnami这个仓库中安装wordpress的chart，把这个release命名为wordpress，它被安装在default下\n\nhelm status可以用来追踪release的状态\n使用helm uninstall -n &lt;namespace&gt; 来卸载release\nhelm uninstall wordpress # 指定release名称helm list # 查看当前命名空间部署的所有release\n\n自定义安装配置很多时候我们需要自定义chart来指定我们想要的配置。\n使用 helm show values 可以查看 chart 中的可配置选项\n你可以helm pull  &lt;chart.tgz&gt;手动拉取chart包，解压后手动修改其中的values.yaml中的任意配置项，然后在安装时指定我们自定义的配置\nhelm install -f values.yaml bitnami/wordpress\n\n如果配置不多，可以直接使用命令行-- set来对指定项进行覆盖\n--set有一些格式限制：\n最简单用法（键值对形式）：–set name&#x3D;value，等价于如下yaml格式\nname: value\n\n多个值用逗号分隔：–set outer.inner&#x3D;value，他被转化为\nouter:  inner: value\n\n列表使用花括号：–set name&#x3D;{a, b, c}，等价于\nname:  - a  - b  - c\n\n某些键值可以设置为null或空：–set name&#x3D;[],a&#x3D;null\nname: []a: null\n\n从2.5.0版本开始，还可以使用数组下标的方式访问列表中的元素。例如：–set servers[0].port&#x3D;80\nservers:  - port: 80\n\n多个值（逗号分隔）：–set servers[0].port&#x3D;80,servers[0].host&#x3D;example\nservers:  - port: 80    host: example\n\n如果要使用特殊字符，使用反斜线来转义：–set name&#x3D;value1,value2\nname: &quot;value1,value2&quot;\n\n更复杂的话就不要使用–set了，直接修改values.yaml\nhelm upgrade和helm rollback用来升级release和失败时恢复\n一次升级操作会使用已有的 release 并根据你提供的信息对其进行升级。由于 Kubernetes 的 chart 可能会很大而且很复杂，Helm 会尝试执行最小侵入式升级。即它只会更新自上次发布以来发生了更改的内容。\n$ helm upgrade -f panda.yaml happy-panda bitnami/wordpress\n\n在上面的例子中，happy-panda 这个 release 使用相同的 chart 进行升级，但是使用了一个新的 YAML 文件：\nmariadb.auth.username: user1\n\n这样配置中会更新这一项配置\n如果发布过程中出现不符合预期的事情，helm可以像deployment一样进行回滚到之前的版本\nhelm repo可以查看仓库\nhelm repo list列出当前已经添加的仓库\nhelm repo add添加新的仓库\nhelm repo update更新仓库\n创建自己的charts快速开始你可以通过helm create来快速生成一个chart模板\n在编辑chart时，可以通过helm lint验证格式是否正确\n编辑完成后，使用helm package来打包chart，然后这个chart就可以install安装，还可以上传到chart仓库中。\n当你想测试模板渲染的内容但又不想安装任何实际应用时，可以使用helm install --debug --dry-run goodly-guppy ./mynginx。这样不会安装应用(chart)到你的kubenetes集群中，只会渲染模板内容到控制台（用于测试）。\n编写相关文件Helm chart的结构如下：\nmychart/  Chart.yaml  values.yaml  charts/  templates/  ...\n\ntemplates/ 目录包括了模板文件。也就是Deployment、Service、ConfigMap等的yaml文件，helm会将这些发送给Kubernetes，Kubernetes根据这些创建资源。和一般的创建资源方式不同的是，这些yaml文件中的一些参数和变量会在安装时由 Helm 进行替换或填充，然后在values.yaml集中设置。\nvalues.yaml 文件也导入到了模板。这个文件包含了chart的 默认值 。\nChart.yaml 文件包含了该chart的描述。\n创建一个自己的nginx chart\nhelm create mynginx\n\n可以看到mynginx/templates/ 目录下一些文件已经存在了：\n\nNOTES.txt: chart的”帮助文本”。这会在你的用户执行helm install时展示给他们。\ndeployment.yaml: 创建Kubernetes 工作负载的基本清单\nservice.yaml: 为你的工作负载创建一个 service终端基本清单。\n_helpers.tpl: 放置可以通过chart复用的模板辅助对象\n\n我们先把它们都删掉，然后自己创作相关文件。\n我们的templates/下主要需要deployment.yaml和service.yaml\n模板文件可以像平时手动编写资源清单yaml文件的格式一样，但是将某些值硬编码到一个键中不是一个很好的方式。helm提倡使用插入的方式来生成某些字段，这样可以提高配置的灵活性和可维护性。\nhelm中以&#123;&#123; xxx &#125;&#125;的形式来进行插入。\n我们以这两个正常的资源文件作为基础，然后按照插入的方式来对某些字段进行修改，使它们变得有helm的风格。\n内置对象\n对象可以通过模板引擎传递到模板中。以下是一些内置的对象。\n\nRelease：Release对象描述了版本发布本身。比如常用的Release.Name，代表了release的名称。\nValues：Values对象是从values.yaml文件和用户提供的文件传进模板的。默认为空。\nChart：Chart.yaml文件内容。 Chart.yaml里的所有数据在这里都可以可访问的。比如 &#123;&#123; .Chart.Name &#125;&#125;-&#123;&#123; .Chart.Version &#125;&#125; 会打印出 mychart-0.1.0\n\nValues文件\n它的内容可以来自多个位置\n\nchart中的values.yaml文件\n使用-f参数(helm install -f myvals.yaml ./mychart)传递到 helm install 或 helm upgrade的values文件\n使用--set (比如helm install --set foo=bar ./mychart)传递的单个参数\n\n以上列表有明确顺序：默认使用values.yaml，优先级为values.yaml最低，--set参数最高。\n模板函数\n到目前为止，我们已经知道了如何将信息传到模板中。 但是传入的信息并不能被修改。 有时我们希望以一种更有用的方式来转换所提供的数据。\nhelm中通常倒置命令，借用管道符来使用函数，这种感觉就像使用管道符把参数“发送”给函数。\n比如quote函数把.Values对象中的字符串属性用引号引起来，然后放到模板中\n本来应该这样写\n首先在values.yaml中定义\nfavourite: rice\n\n&#123;&#123; quote .Values.favorite.food &#125;&#125;  # 转换为&quot;rice&quot;，就是加了个引号\n\n但helm提倡这样写\n&#123;&#123; .Values.favorite.food | quote &#125;&#125;\n\n模板中频繁使用的一个函数是default，这个函数允许你在模板中指定一个默认值，以防这个值被忽略。\nnamespace: &#123;&#123; .Values.namespace | default &quot;default&quot; &#125;&#125;# 指定命名空间，如果values.yaml里没有指定（没有namespace这个键），就默认指定default\n\n流控制\n有时我们在values.yaml中会利用enable: true/false来开启或关闭某些功能这是可以利用控制结构if/else块来实现。values.yaml中开启NodePort\nNodePort:  enable: true # or flase 使用布尔值  type: NodePort  nodePort: 30001  port: 9376\n\ntemplates/NodePort.yaml首尾加上&#123;&#123; if .Values.NodePort.enable &#125;&#125;和&#123;&#123; end &#125;&#125;即可\n还有其他的控制结构\n\nwith：用来指定变量范围\nrange：用做循环\n\n命名模板\n在编写模板细节之前，文件的命名惯例需要注意：\n\ntemplates/中的大多数文件被视为包含Kubernetes清单\nNOTES.txt是个例外\n命名以下划线(_)开始的文件则假定 没有 包含清单内容。这些文件不会渲染为Kubernetes对象定义，但在其他chart模板中都可用。\n\n当我们第一次创建mynginx时，会看到一个名为_helpers.tpl的文件，这个文件是模板局部的默认位置。\ndefine和template\n我们可以使用define和template来声明和使用模板\n比如我在_helpers.tpl中创建一个模板。按照惯例define方法会有个简单的文档块(&#123;&#123;/* ... */&#125;&#125;)来描述要做的事。\n&#123;&#123;- define &quot;MY.NAME&quot; &#125;&#125;  # body of template here&#123;&#123;- end &#125;&#125;&#123;&#123;/* Generate basic labels */&#125;&#125;&#123;&#123;- define &quot;mynginx.labels&quot; &#125;&#125;  labels:    app: nginx&#123;&#123;- end &#125;&#125;\n\n然后使用template来使用模板\nname: &#123;&#123; .Release.Name &#125;&#125;-deployment   namespace: &#123;&#123; .Values.namespace | default &quot;default&quot; &#125;&#125;  &#123;&#123;- template &quot;mynginx.labels&quot; &#125;&#125;\n\ninclude方法\n假设我们刚才的模板是这样的\n&#123;&#123;- define &quot;mynginx.labels&quot; &#125;&#125;app: nginx # 顶格没有空格&#123;&#123;- end &#125;&#125;\n\n这时插入模板文件\nname: &#123;&#123; .Release.Name &#125;&#125;-deployment   namespace: &#123;&#123; .Values.namespace | default &quot;default&quot; &#125;&#125;  labels:  \t&#123;&#123;- template &quot;mynginx.labels&quot; &#125;&#125;\n\n这时它的输出如下\nname: &#123;&#123; .Release.Name &#125;&#125;-deployment   namespace: &#123;&#123; .Values.namespace | default &quot;default&quot; &#125;&#125;  labels:app: nginx  \t\n\n注意到app: nginx的缩进不对。因为被替换的模板中文本是左对齐的。由于template是一个行为，不是方法，无法将 template调用的输出传给其他方法，数据只是简单地按行插入。\n为了处理这个问题，Helm提供了一个template的可选项，可以将模板内容导入当前管道，然后传递给管道中的其他方法。\n使用indent正确地缩进\nname: &#123;&#123; .Release.Name &#125;&#125;-deployment   namespace: &#123;&#123; .Values.namespace | default &quot;default&quot; &#125;&#125;  labels:&#123;&#123;include &quot;mynginx.labels&quot; | indent 4 &#125;&#125;\n\nNOTES.txt文件\n在helm install 或 helm upgrade命令的最后，Helm会打印出对用户有用的信息。这些信息保存在NOTE.txt文件中\nThank you for installing &#123;&#123; .Chart.Name &#125;&#125;.Your release is named &#123;&#123; .Release.Name &#125;&#125;.To learn more about the release, try:  $ helm status &#123;&#123; .Release.Name &#125;&#125;  $ helm get all &#123;&#123; .Release.Name &#125;&#125;\n\n输出如下\nThank you for installing mynginx.Your release is named nginx1.To learn more about the release, try:  $ helm status nginx1  $ helm get all nginx1\n\nChart.yaml\n这个文件声明了当前 Chart 的名称、版本等基本信息，这些信息会在该 Chart 被放入仓库后，供用户浏览检索。\n在 Chart.yaml 里有两个跟版本相关的字段，其中 version 指明的是 Chart 的版本，也就是我们应用包的版本，而 appVersion 指明的是内部实际使用的应用版本。这两个字段互不相关，此字段仅供参考，对chart版本计算没有影响。我们可以不关心它。\n比如 nginx chart的版本字段version: 1.2.3按照名称被设置为：\nnginx-1.2.3.tgz\n\n打包chart并上传到Harbor# 打包charthelm package CHART_PATH# 登录harborhelm registry login reg.redrock.team -u admin -p xxx# 推送charthelm push CHART_PACKAGE oci://reg.redrock.team/library\n\n也可以把打包好的chart上传到Github上，然后通过helm pull解压安装\n其他Kubeconfig使用 kubeconfig 文件组织集群访问 | Kubernetes\nkubectl 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息（指定要操作的集群），并与集群的 API 服务器进行通信。Helm同样使用kubeconfig来指定操作的集群。\n用于配置集群访问的文件称为 kubeconfig 文件。 这是引用到配置文件的通用方法，并不意味着有一个名为 kubeconfig 的文件。\n默认情况下，kubectl 在 $HOME/.kube 目录下查找名为 config 的文件。 你可以通过设置 KUBECONFIG 环境变量或者设置 --kubeconfig参数来指定其他 kubeconfig 文件。\n配置对多集群的访问 | Kubernetes\n查看config签发日期\nopenssl x509 -in /etc/kubernetes/pkki/apiserver.crt -noout -text | grep Not\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"O2O用户优惠券预测记录","url":"/posts/59602.html","content":"\n# 数据挖掘汇报\n\n大家好，今天由我向大家汇报一下我们组的项目。我们组做的项目题目是：基于XGBoost模型的O2O用户优惠券使用行为分析与预测方法研究。相信一些同学应该对这个题目很熟悉，因为这个题目来源于CSA数据工程实践的考核任务。之所以选择这个题目，一方面是因为它涵盖了许多我们在数据挖掘课程中学习过的机器学习相关知识点，另一方面也是因为我在这个项目上投入了大量时间和精力，并从中真正获得了一些实质性的收获。\n因此，我希望借此机会向大家分享我的学习成果和一些心得体会。\n下面我会从这几个方面来具体展开整个项目流程，最后还会讲讲K-Means的代码实现。\n项目介绍首先是项目介绍。\n可能很多同学不知道O2O是做什么的，这里我对O2O做一个解释。O2O是“Online to Offline”的缩写，它是一种商业模式，指的是通过线上平台将用户引导到线下实体店进行消费的方式。比如美团外卖就采用的这种模式，也就是用户线上先下单，商家线下制作并且送餐这种。\n那我们为什么要对优惠券进行预测呢？因为优惠券是O2O的一种重要营销手段，反映到我们的生活上也是，哪个商家发优惠券我们就比较喜欢去这个商家去消费。但是发优惠券也不能随便发，要根据用户的喜好发，个性化地发，这样才能吸引更多的用户来促进消费。\n接下来是样本来源。\n那整个项目是在做什么？就是围绕用户过去怎么领取、消费优惠券，提取一些关键特征，最后预测用户在未来会怎么消费优惠券。\n最后的评测目标是预测投放的优惠券是否核销，我们使用优惠券核销的平均AUC来作为评分标准。\n这里简单介绍一下AUC，介绍AUC得先知道ROC是什么，ROC纵轴是“真正例率TPR”，横轴是“假正例率FPR”。AUC就是ROC曲线下的面积。\n\n\n整个O2O优惠券预测分为了如上的六个步骤\n对于特征工程，我仅考虑了线下数据集，主要从历史区间和标签区间两个部分进行考虑，提取用户（User）、商家（Merchant）、优惠券（Coupon），同时结合一些交互特征，最后还将少量贡献度高的特征进行排列组合构造新的特征。\n特征提取完毕后，使用XGBoost模型预测优惠券使用概率，调参得到最优解。\n最后我将特征的重要性进行输出，并且通过一个自定义的auc函数来模拟比赛的auc结果，辅助评判模型。\n数据处理首先我先查看了属性列有哪些。左侧是比赛官方提供的预测样本的属性列说明，对于领取行为，多了一个Date列用于记录用户什么时候核销了优惠券。\n接下来我对数据进行一个整体预览。具体就是进行一些计数，看看有多少条记录，统计下最早最晚的领取时间。\n接着结合isnull()和any()对数据进行缺失值的检查。可以看到distance存在缺失值。\n然后对数据进行预处理。主要做了如上这些事情。空距离缺失我用-1来进行简单的填充。在提取特征前也要对数据进行一个预处理。\n‍\n预处理完毕后就可以对数据进行可视化了。这里我使用pyecharts生成了如上几个统计图。每个统计图其实可以分析出一些用户偏好行为。就拿用户消费距离统计来说，大部分用户的消费距离为0，即领券后直接在领取地点消费的用户最多，数量为43867。说明很多用户更倾向于在领取优惠券的门店内直接使用，而不会去其他更远的地方消费。随着距离的增加，领取用户数量逐渐减少。说明大多数用户更倾向于在较近的范围内消费。\n在 10km 时出现异常高峰，推测有以下原因：\n\n特定商圈或门店 在 10km 位置，有大量用户前往。\n用户误差：数据采集时，部分 10km 以上的用户被归为 10km\n\n‍\n由于最终的目标是要确定优惠券是否在15天内要被核销，所以我们需要对数据进行打标。具体打标方式就是新加label列，算一下date与date_received的差，如果在15天内就说明核销率，记录为1，否则为0。\n‍\n由于数据有明显的时间性，所以可以采用时间窗口划分法来划分数据集。这种划分方法把每个数据集划分为历史区间、中间区间和标签区间。历史区间上存在用户过往领券行为，中间区间作为一个缓冲，增加时间隔离，避免数据泄露，标签区间则用来模拟真实的目标，用来预测用户未来是否核销优惠券。\n特征工程整个特征工程一共提取了111个特征，分为三个部分。每个部分提取的特征对象都类似，也就是用户、商家、以及一些交互特征。\n对于交叉特征，主要逻辑是通过两层循环，遍历传入的需要进行交叉组合的高贡献度特征列，两两组合，构造交叉特征。\n这里简单说下特征是怎么提取的：使用的是pivot_table()透视表或者groupby()，按照一个分组键，在cnt列或者其他列应用一个方法，也就是aggfunc，最后把结果重命名成特征名，最后合并进主特征表。如果有缺失值，就补充为0.\n交叉特征不再赘述。\n‍\n特征提取好以后，我们把所有提取出的特征合并到历史区间上，然后去除公共列，最后使用concat()合并进数据集。\n接着对数据集进行修正：\n对于训练集和验证集，将不需要训练的数据列使用drop()方法删除，并将label放在最后。同时加入交叉特征。\n对于测试集，除了删除不需要的列，我还将数据某些列转为int型，便于 xgboost 识别。同样地也加入交叉特征。\n模型构建与评估模型构建的话主要是这几个步骤。\n首先得修正下数据集，因为xgboost需要数值型特征数据，而User_id、Coupon_id不是训练数据的一部分，它们不能提供有用的信息，因此需要删除。另一方面，如果将label传入，模型可能会通过目标变量直接猜测结果，这样会导致模型训练时的“数据泄漏”问题，从而影响模型的泛化能力。\n最后我们得到训练集、验证集和测试集。监控训练集和验证集，开始训练，最后按照比赛要求输出csv。\n‍\n下面讲下xgboost的参数。左图的这些参数可以分为如上6类。这些参数就不一一展开说了，我认为值得关注的是eta学习率（越小训练越慢但稳定）、max_depth树的最大深度、最小损失函数下降值（控制节点是否分裂）gamma和L2 正则化权重（对叶子权重的惩罚）lambda：大了可以防止过拟合。\n‍\n下面讲讲我的提分曲线\n整个提分过程出现了4次瓶颈。\n从0.58到0.61：在baseline的基础上，添加更多特征，包括历史区间和标签区间。\n从0.61到0.71：得益于用户标签区间的leakage，让AUC整整提高了0.1\n\nleakage是一种数据泄露，比如标签区间上可以出现这种情况，就是用户在7月10日领取了优惠券，而在14号，16号又领取了同一种优惠券，说明7月10号领取的优惠券被核销的可能性很大。但是实际生产生活中是获取不到这种特征的。\n\n从0.71到0.74：挖掘了用户标签区间上的更多特征，其中7天内用户是否多次领取同一优惠券贡献最大。\n从0.74到0.78：在用户标签区间上继续挖掘与时间强相关的特征。其中15天、30天内的领券次数的贡献最大。\n‍\n为了方便知道特征的重要性，我利用get_score()方法对所有特征进行gain重要性评分，最后按贡献度排序，方便剪除贡献度低的特征。\n‍\n我还利用了验证集，并且搭配early_stop_rounds让模型提前收敛，确定最佳训练轮数。可以看到在1000次训练轮数的前提下，模型在846轮性能不再提升，实现早停，避免了过拟合。\n‍\n对于AUC的提升，我还做过一些尝试。\n1、尝试使用LightGBM和XGBoost模型融合，加权输出结果。但是AUC反而没有XGBoost单独训练的AUC高，遂仅使用XGBoost。\n2、XGBoost调整参数陷入瓶颈，几乎所有参数的微调都会导致 AUC 指标下降，模型效果反而变差。最终我放弃了通过调参提升AUC 的方向，转而从特征工程角度优化模型表现。\n3、我还尝试过合并过线上数据集，但是会出现大量缺失值，甚至造成“数据泄露”。导致AUC严重下降。\n‍\n‍\nK-Means\n参数x代表数据集、k代表簇的数量、iter代表最大迭代次数、tol代表簇中心的容忍误差范围。\n整个函数主要分为4步。\n首先使用random.sample()方法，从数据集x中，随机选取k个点作为初始簇中心。\n接着初始化一个标签列表，全部即为None。然后利用get_distance分别计算每个点到初始簇中心的距离，将所有距离中最小的一个保存在新标签列表中，然后不断迭代，直到新旧标签完全相同。然后更新簇中心，更新方法是计算每个簇中所有点的均值作为新的簇中心。具体算法是初始化一个中心列表用来保存新的簇中心。然后获取当前簇中所有的点，接着计算这些点横纵坐标的均值，把计算结果保存在中心列表中。利用allclose()方法判断新的簇中心与原来簇中心之间的误差是否不大于tol，满足条件就终止迭代。\n最后返回簇中心与距离数组。\n‍\n最后说下心得体会\n在上这门课之前，我一直专注于云原生容器化领域，对机器学习、人工智能的认识也仅仅停留在“问问AI问题阶段”，从来没有深入研究过背后的实现机制。这次的O2O优惠券预测项目让我深刻体会到了数据挖掘和机器学习的魅力，最让我振奋的是我的提分历程，其中，受到 wepe 提示的 leakage 特征启发，我成功将 AUC 提升了整整 0.1。\n在从 0.71 到 0.78 的关键阶段，我记得从周五到周日我看了整整3天，尝试了多种方法，包括合并线上特征、构造交叉特征、网格搜索参数调优和模型融合等，但效果都不显著。这一度让我怀疑自己的方向，甚至开始觉得机器学习就是一场“黑盒测试”。为此，我主动寻求学长的指导，并查阅大量博客和大模型的建议，最终意识到问题出在特征工程上还不够深入。\n通过不断尝试与调整，我逐步解决了一部分问题，但也意识到自身技术能力仍有不足，对某些问题的根源仍未完全理解，尚未找到有效的解决方案。目前模型的AUC达到了 0.78，仍有进一步提升的空间，比如加入用户线上行为特征、针对性调整模型参数、挖掘与星期相关的时间特征等。\n说到最后，其实数据挖掘挺有趣的，特别是你可以从一堆静态的数据里，发现一些有意思的东西，这也许就是大数据的魅力。\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"wasm-plugin-github action","url":"/posts/46148.html","content":"\n## 功能描述\n\n功能：添加了利用 GitHub Actions 来自动完成相应的镜像构建和发布工作的Workflow。支持通过push tag和手动触发两种方式。同时也遵循使用oras打包工具。特点：完全按照Wasm 插件镜像规范进行镜像打包。\n更新：\n\n添加了查找文件的逻辑，会在插件目录中查找spec.yaml​、README.md​和README-{lang}.md​三个文件，有则在打包推送镜像时设置相应的media type​。\n修改了review后存在的问题。\n修改了builder​容器的启动命令为 docker run -itd --name builder xxx /bin/bash​，之前使用sleep 99999​是希望容器保持后台持续运行，这样只适合测试容器，不适合在生产环境中使用。\n重新提交PR​是因为错误地提交了一些没用的commit​，不太会删除，担心破坏仓库并且为了保持PR​整洁所以重新提交。\n\n1.准备工作添加Repository Secrets和 Repository variables。​​2.通过push tag​触发，先查找相关文件，然后将特定的插件打包成镜像并推送至指定仓库。​\n\n​3.人工触发，指定插件名和版本号​\n本次贡献借助了通义灵码以及通义千问。​​​​\n‍\n验证\n纠正了打包容器的TinyGO​版本为0.28.1​，确保镜像可用。\n修改了编译命令为tinygo build -o ./plugin.wasm -scheduler=none -target=wasi -gc=custom -tags=&#39;custommalloc nottinygc_finalizer&#39; ./main.go​，确保plugin.wasm​可用。\n修改push_command​，最后设置plugin.tar.gz​的media type​，确保其位于镜像最后一层。\n\n验证\n环境介绍\n\nKubernetes部署Higress，Gateway位于32459端口。\nhigress.yiiong.top​解析至服务器。\nKubernetes部署了一个Nginx测试服务，增加一个hello.html​，访问会返回Hello World!​\n增加一个ingress​\n\n\n\n经Github Action​打包出镜像。\n\n途径一：通过配置清单部署打包好的request-block​插件，拒绝访问/hello.html​，可以看到成功被拒绝访问。\n\n途径二：通过控制台新增插件，同样可以拒绝访问。\n\n\n\n\nGithub Actionname: Build and Push Wasm Plugin Imageon:  push:    tags:    - &quot;wasm-go-*-v*.*.*&quot; # 匹配 wasm-go-&#123;pluginName&#125;-vX.Y.Z 格式的标签  workflow_dispatch:    inputs:      plugin_name:        description: &#x27;Name of the plugin&#x27;        required: true        type: string      version:        description: &#x27;Version of the plugin (optional, without leading v)&#x27;        required: false        type: stringjobs:  build-and-push-image:    runs-on: ubuntu-latest    environment:      name: image-registry-msg    env:      IMAGE_REGISTRY_SERVICE: $&#123;&#123; vars.IMAGE_REGISTRY_SERVICE || &#x27;higress-registry.cn-hangzhou.cr.aliyuncs.com&#x27; &#125;&#125;      IMAGE_REPOSITORY: $&#123;&#123; vars.IMAGE_REPOSITORY || &#x27;plugins&#x27; &#125;&#125;      GO_VERSION: 1.19      TINYGO_VERSION: 0.28.1      ORAS_VERSION: 1.0.0    steps:      - name: Set plugin_name and version from inputs or ref_name        id: set_vars        run: |          if [[ &quot;$&#123;&#123; github.event_name &#125;&#125;&quot; == &quot;workflow_dispatch&quot; ]]; then            plugin_name=&quot;$&#123;&#123; github.event.inputs.plugin_name &#125;&#125;&quot;            version=&quot;$&#123;&#123; github.event.inputs.version &#125;&#125;&quot;          else            ref_name=$&#123;&#123; github.ref_name &#125;&#125;            plugin_name=$&#123;ref_name#*-*-&#125; # 删除插件名前面的字段(wasm-go-)            plugin_name=$&#123;plugin_name%-*&#125; # 删除插件名后面的字段(-vX.Y.Z)            version=$(echo &quot;$ref_name&quot; | awk -F&#x27;v&#x27; &#x27;&#123;print $2&#125;&#x27;)          fi          echo &quot;PLUGIN_NAME=$plugin_name&quot; &gt;&gt; $GITHUB_ENV          echo &quot;VERSION=$version&quot; &gt;&gt; $GITHUB_ENV      - name: Checkout code        uses: actions/checkout@v3      - name: File Check        run: |           workspace=$&#123;&#123; github.workspace &#125;&#125;/plugins/wasm-go/extensions/$&#123;PLUGIN_NAME&#125;          push_command=&quot;./plugin.tar.gz:application/vnd.oci.image.layer.v1.tar+gzip&quot;          # 查找spec.yaml          if [ -f &quot;$&#123;workspace&#125;/spec.yaml&quot; ]; then            echo &quot;spec.yaml exists&quot;            push_command=&quot;./spec.yaml:application/vnd.module.wasm.spec.v1+yaml $push_command &quot;          fi          # 查找README.md          if [ -f &quot;$&#123;workspace&#125;/README.md&quot; ];then              echo &quot;README.md exists&quot;              push_command=&quot;./README.md:application/vnd.module.wasm.doc.v1+markdown $push_command &quot;          fi                  # 查找README_&#123;lang&#125;.md          for file in $&#123;workspace&#125;/README_*.md; do            if [ -f &quot;$file&quot; ]; then              file_name=$(basename $file)              echo &quot;$file_name exists&quot;              lang=$(basename $file | sed &#x27;s/README_//; s/.md//&#x27;)              push_command=&quot;./$file_name:application/vnd.module.wasm.doc.v1.$lang+markdown $push_command &quot;            fi          done          echo &quot;PUSH_COMMAND=\\&quot;$push_command\\&quot;&quot; &gt;&gt; $GITHUB_ENV            - name: Run a wasm-go-builder        env:           PLUGIN_NAME: $&#123;&#123; env.PLUGIN_NAME &#125;&#125;          BUILDER_IMAGE: higress-registry.cn-hangzhou.cr.aliyuncs.com/plugins/wasm-go-builder:go$&#123;&#123; env.GO_VERSION &#125;&#125;-tinygo$&#123;&#123; env.TINYGO_VERSION &#125;&#125;-oras$&#123;&#123; env.ORAS_VERSION &#125;&#125;        run: |          docker run -itd --name builder -v $&#123;&#123; github.workspace &#125;&#125;:/workspace -e PLUGIN_NAME=$&#123;&#123; env.PLUGIN_NAME &#125;&#125; --rm $&#123;&#123; env.BUILDER_IMAGE &#125;&#125; /bin/bash      - name: Build Image and Push        run: |                  push_command=$&#123;&#123; env.PUSH_COMMAND &#125;&#125;          push_command=$&#123;push_command#\\&quot;&#125;          push_command=$&#123;push_command%\\&quot;&#125; # 删除PUSH_COMMAND中的双引号，确保oras push正常解析          command=&quot;          cd /workspace/plugins/wasm-go/extensions/$&#123;PLUGIN_NAME&#125;          go mod tidy          tinygo build -o ./plugin.wasm -scheduler=none -target=wasi -gc=custom -tags=&#x27;custommalloc nottinygc_finalizer&#x27; ./main.go          tar czvf plugin.tar.gz plugin.wasm          echo $&#123;&#123; secrets.REGISTRY_PASSWORD &#125;&#125; | oras login -u $&#123;&#123; secrets.REGISTRY_USERNAME &#125;&#125; --password-stdin $&#123;&#123; env.IMAGE_REGISTRY_SERVICE &#125;&#125;          oras push $&#123;IMAGE_REGISTRY_SERVICE&#125;/$&#123;IMAGE_REPOSITORY&#125;:$&#123;VERSION&#125; $&#123;push_command&#125;          &quot;          docker exec builder bash -c &quot;$command&quot;\n","categories":["Achievements"],"tags":["CICD"]},{"title":"Operator介绍","url":"/posts/58647.html","content":"\n# Operator\n\n为什么要用？当K8s中的原生资源无法满足业务要求时，这时候就需要Operator来创建自己的CRD以及对应Controller。利用Controller实现CRD所需要的业务逻辑。\n比如，我们想在Kubernetes中部署一个Nginx并让外界可以访问到，这时我们首先需要创建一个Deployment用来部署Nginx，然后创建一个Service用来将Deployment的Pod的容器端口映射到主机端口。我们觉得太麻烦了，想要创建一次资源就可以同时包含Deployment和Service。\n是什么？Operator是一种通过自定义控制器（Customer Controller）扩展K8s API的模式。它的核心思想是将运维知识代码化。允许开发者将应用的管理逻辑（如部署、升级、备份、恢复）封装成代码，使K8s能够像管理原生资源一样管理复杂的有状态应用（如数据库、消息队列）\n解决了什么？\n管理有状态应用的复杂性：例如pg集群的自动故障转移、redis的数据持久化\n自动化运维：通过代码替代人工操作\n统一生命周期管理：标准化应用的安装、配置、扩缩容流程等\n\n什么时候用？\n数据库管理：MariaDB Operator\n中间件管理：Kafka Operator\nCI&#x2F;CD工具：Argo CD Operator\n\n核心组件Customer Resource Definitions（CRD）CRD是K8s中扩展API资源的方式。\nController\n职责：监听资源状态变化、驱动集群向期望状态收敛\n核心逻辑：通过Reconcile()函数实现调谐循环\n\nReconcile Loop调谐循环机制\n\n获取资源的当前状态\n对比实际状态（集群中运行的Pod数量）\n执行操作（增减Pod）\n更新资源状态（status字段）\n\n和Helm对比？区别与场景\n\nHelm：适合静态配置的包管理工具（如一次性部署应用）\nOperator：适合的动态、持续管理的场景（如自动修复故障节点）\n\n","categories":["Kubernetes"],"tags":["Kubernetes","Operator"]},{"title":"存储与配置","url":"/posts/37331.html","content":"\n## 存储\n\n持久化Pod是由多个容器组成的，在了解Pod的文件系统结构时，我们可以先看看一个Docker容器是如何被制作的。\n容器根据镜像启动。我们在Dockerfile中的每一条RUN命令，会生成一层一层的镜像层，这些镜像层从下向上以栈的形式组成了一个镜像。而容器和镜像都是由多个层组成的，他们之间最大的区别就是容器的最上面的一层是读写层，叫做容器层。但镜像的所有层都是只读层，叫做镜像层。\n容器启动后，Docker会在容器使用的镜像上添加一个容器层。容器运行时，所有和数据变化相关的操作都是在这个读写层中完成的，如新建文件，修改文件等。删除容器时，Docker同时会删除这个容器层。\n每个容器运行时，都有自己的容器层，并在容器层中保存容器运行相关的数据。容器层之下的所有镜像层都是只读的，因此多个容器可以共享同一个镜像。\n\nPod中有若干个容器，每个容器的文件系统都是独立且一次性的。当容器因为某些原因重启时（崩溃、调度等情况），容器层就会被销毁并重新创建。\n假如我们运行了一个MySQL的Pod，如果你没有对它做持久化存储，一旦这个Pod因为某些原因被销毁，那么这个Pod里的数据就会全部丢失，然后工作负载就会另起一个全新的MySQL的Pod，这个Pod是一干二净的，原来Pod的数据不会被保留也不会被恢复，~~这样你就得跑路了~~~\n因此，对数据的持久化是非常有必要的！\n存储卷Volume存储 | Kubernetes\nVolume 的生命周期独立于容器，Pod 中的容器可能被销毁和重建，但 Volume 会被保留。\n在Docker中，我们可以通过-v参数来指定挂载目录。而在K8s中，我们需要通过定义存储卷（Volume）来满足数据持久化的需求。\nKubernetes支持很多类型的卷。同一个pod也可以挂载多个多种不同类型的卷。\n\n总体来看可以分为四大类：\n\n本地卷：hostPath-直接挂载到宿主机文件类型、emptyDir-数据的存储取决于宿主机所使用的介质，比如磁盘或 SSD 或网络存储。（这种卷的数据直接保存在宿主机的本地磁盘上，并且Pod只能在该特定节点上访问这些数据）\n网络数据卷：比如Nfs、ClusterFs、Ceph，这些都是外部的存储都可以挂载到k8s上。\n云盘：云服务商自行提供的存储方案。\nK8s的自身资源：比如Secret、ConfigMap等。\n\nemptyDir用于存储临时数据的简单空目录。\n工作流程：\n当 Pod 被分派到某个 Node 上时，emptyDir 卷会被自动创建在该Pod中，并且在 Pod 在该节点上运行期间，卷一直存在。当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。但是容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。\n用途：\n\n临时空间，例如用于某些应用程序运行时所需要的临时目录，且无需永久保留\n同一个Pod中的 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）\n\n缺点：\nemptyDir不能提供数据持久化，也无法跨节点同步数据，所以仅在容器临时存放数据时使用它。\nemptyDir存储卷的配置示例：\nspec:  volumes:  - name: volume1    emptyDir: &#123;&#125;  - name: volume2    emptyDir:      medium: Memory      sizeLimit: 100Mi\n\n\nmedium\n作为卷来使⽤的emptyDir是在承载Pod的⼯作节点的实际磁盘上创建的， 因此其性能取决于节点的磁盘类型。如果需要高速的读写性能，我们可以通知Kubernetes在tmpfs⽂件系统（储存在内存⽽⾮硬盘）上创建emptyDir。因此，将emptyDir的medium设置为Memory。\n\nsizeLimit\n可以限制emptyDir被容器填充的大小，则填写此项。如：100Mi，20Gi。此项在大多数卷中是通用的。\n\n\nemptyDir卷是最简单的卷类型，其他类型的卷都是在它的基础上构建的，在创建空⽬录后，其他类型的卷会⽤数据填充它。\nhostPath ⽤于将⽬录从⼯作节点的⽂件系统挂载到Pod中。\n工作流程：\n就是将工作节点中的一个实际目录挂载进Pod中，以供容器使用，提供对主机上文件的访问。它可以保证数据的持久化，即使Pod被销毁了，数据也仍然在工作节点上。它和我们在docker run时的-v参数很类似。\n用途：\n\n如果Pod需要使用Node上的某些东西时，比如某容器需要访问Docker,可使用hostPath 挂载宿主机节点的/var/lib/docker(Docker的工作目录)\n某些系统级别的Pod（通常由DaemonSet管理，例如DDNS）确实需要读取节点的⽂件或使⽤节点⽂件系统来访问节点设备\n\n\nDaemonSet会在每个节点上都运行一个指定的的Pod实例，无论节点是什么时候加入或离开集群。\n\n缺点：\n\n仅提供了单节点的数据持久化，不提供跨节点的数据同步。一般不建议使用这种存储方式（除非是单节点），因为当Pod被调度到另一个Node上时，它会找不到数据。而且理论上⼤多数Pod应该忽略它们的主机节点，因此它们不应该访问节点⽂件系统上的任何⽂件。\n存在安全风险，可能会暴露特权系统凭据（例如 Kubelet）或特权 API（例如容器运行时套接字），可用于容器逃逸或攻击集群的其他部分。因此应该尽量避免使用hostPath，如确需使用，也应该限制它的范围只到所需的文件或目录，并且以只读的方式(readOnly: true)挂载。\n\nhostPath存储卷的配置示例：\nspec:  volumes:  - name: hostpath-volume    hostPath:      path: /path/to/node # 指定挂载目录      type: DirectoryOrCreate\n\n\n实例演示：使用hostPath实现Nginx的日志持久化\n指定两个容器：Nginx和BusyBox。\nNginx用于生成日志，并且将名为 logs-volume 的卷挂载到容器的 /var/log/nginx 目录。\nBusyBox用来证明容器之间的数据可以实现共享，它将名为 logs-volume 的卷挂载到容器的 /logs 目录。\n最后定义了名为 logs-volume 的卷，通过 hostPath 指定了它在宿主机上的路径为 /home/logs。\napiVersion: v1kind: Podmetadata:  name: volume-hostpath  namespace: lesson-demospec:  containers:  - name: nginx    image: reg.redrock.team/library/nginx:latest    ports:    - containerPort: 80    volumeMounts:    - name: logs-volume      mountPath: /var/log/nginx  - name: busybox    image: busybox:1.30    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;]    volumeMounts:    - name: logs-volume      mountPath: /logs  volumes:  - name: logs-volume    hostPath:      path: /home/logs      type: DirectoryOrCreate\n\n查看资源，Pod被调度在master3上。\n\n可以看到master3上已经自动生成了/home/logs目录，并且挂载了/var/log/nginx里的内容。\n\n连续访问三次Pod，访问被成功记录到了/home/logs/access.log中。\n\n进入busybox的容器中，查看busybox/logs/access.log中的内容，发现已经记录了三条访问记录。说明不同容器之间可以共享数据并进行相互交互。\n\n此时删除Pod后，发现数据仍然存在。\n\n\nNFS可以通过网络，让不同的机器、不同的操作系统可以共享彼此的文件。\n当运⾏在⼀个Pod中的应⽤程序需要将数据保存到磁盘上，并且即使该Pod重新调度到另⼀个节点时也要求具有相同的数据可⽤。这就不能使⽤到⽬前为⽌我们提到的任何一种卷类型，由于这些数据需要可以从任何集群节点访问， 因此必须将其存储在某种类型的⽹络存储NAS（Network Attached Storage）中。\n工作流程：\nNFS服务器可以让计算机将网络中的NFS服务器共享的目录挂载到本地端的文件系统中，而在本地端的系统中来看，那个远程主机的目录就好像是自己的一个磁盘分区一样，在使用上相当便利。\n用途：\n一般做数据的共享存储，保证多个节点提供一致性的程序。\n缺点：\n\n容易发生单点故障，Server机宕机了后所有客户端都不能访问\n在高并发下NFS性能有限\n客户端无用户认证机制，且数据是通过明文传送，安全性一般（一般建议在局域网内使用）\nNFS的数据是明文的，对数据完整性不做验证\n\n为了多节点数据同步及持久化，目前已有更优的cephfs方案，它也是一种网络文件系统。如果你在各大云平台购买Kubernetes容器服务，他们也会提供更底层的数据同步卷及插件，这些卷可挂载到容器中，也可卸载下来复用。但是由于NFS是最简单的多节点数据同步及持久化方案，我们使用它来演示。\nNFS服务端的安装与配置（Debian系）\n安装NFS服务端\napt updateapt install nfs-kernel-server\n\n创建一个NFS共享目录\nmkdir -p /root/nfs\n\n修改/etc/exports文件（这个文件包含了NFS 服务器共享的目录），编辑内容为：\n/root/nfs/ *(insecure,rw,sync,no_subtree_check,no_root_squash)# rw代表读写访问，sync代表所有数据在请求时写入共享，no_subtree_check代表不检查父目录权限，no_root_squash代表root用户具有根目录的完全管理访问权限\n\n使用以下命令启动NFS服务器\nsystemctl enable rpcbindsystemctl enable nfs-serversystemctl start rpcbindsystemctl start nfs-serverexportfs -r # 重新导入配置文件exportfs\n\n\n配置完NFS后我们就可以在Kubernetes中使用它了。\nNFS存储卷的配置示例：\nspec:  volumes:  - name: nfs-volume    nfs:      server: 172.20.14.180      path: /root/nfs\n\n持久卷和持久卷声明以上的存储方案看起来十分不错，但到⽬前为⽌，我们探索过的所有持久卷类型都要求Pod的开发⼈员了解集群中可⽤的真实⽹络存储的基础结构。例如，要创建⽀持NFS协议的卷，开发⼈员必须知道NFS节点所在的实际服务器。这对开发人员很不友好，因为他们应该专注于开发而不是部署。而且将这种涉及基础设施类型的信息塞到⼀个Pod设置中，意味着Pod设置与特定的Kubernetes集群有很⼤耦合度。这就不能在另⼀个Pod中使⽤相同的设置了。所以使⽤这样的卷并不是在Pod中附加持久化存储的最佳实践。\n在理想情况下，在Kubernetes上部署应用程序的开发人员是应该不需要知道集群提供了什么存储技术，与基础设施相关的交互才是集群管理员该做的事情。当开发⼈员需要⼀定数量的持久化存储来进⾏应⽤时，可以向Kubernetes请求，就像在创建Pod时可以请求CPU、内存和其他资源⼀样。集群管理员可以对集群进⾏配置让其可以为应⽤程序提供所需的服务。\n什么是持久卷和持久卷声明？为了解决上面出现的问题，K8s给出了一种解决方案：持久卷PersistentVolume （PV）和 持久卷声明PersistentVolumeClaim（PVC）\nPersistentVolume (PV) 是外部存储系统中的一块存储空间，由管理员创建和维护。与 Volume 一样，PV 具有持久性，生命周期独立于 Pod。\nPersistentVolumeClaim (PVC) 是对 PV 的申请 (Claim)。PVC 通常由普通用户创建和维护。需要为 Pod 分配存储资源时，用户可以创建一个 PVC，指明存储资源的容量大小和访问模式（比如只读）等信息，Kubernetes 会查找并提供满足条件的 PV。\n有了 PersistentVolumeClaim，用户只需要告诉 Kubernetes 需要什么样的存储资源，而不必关心真正的空间从哪里分配，如何访问等底层细节信息。这些需要提供存储的底层信息交给管理员来处理，只有管理员才应该关心创建 PersistentVolume 的细节信息。\n持久卷（PV）现在，为了使开发人员不用关心集群上的存储资源的具体细节，我们首先充当集群管理员来配置持久卷。之后，我们再充当开发人员，来使用持久卷完成开发。\n如何申请一个PV呢？\n在创建持久卷时，管理员需要告诉Kubernetes其对应的容量需求，以及它是否可以由单个节点或多个节点同时读取或写⼊。管理员还需要告诉Kubernetes当持久卷声明的绑定被删除时如何处理PersistentVolume。最后，管理员需要指定持久卷⽀持的实际存储类型、位置和其他属性。\n持久卷的配置示例：\napiVersion: v1kind: PersistentVolumemetadata:  name: nfs-pvspec:  capacity:    storage: 5Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain  nfs:    server: 172.20.14.180    path: /root/nfs\n\n配置项：\n\nCapacity：指定 PV 的容量\naccessModes：指定访问模式为 ReadWriteOnce，支持的访问模式有：\nReadWriteOnce：读写权限，并且只能被单个Node挂载。\nReadOnlyMany ：只读权限，允许被多个Node挂载。\nReadWriteMany：读写权限，允许被多个Node挂载。\n\n\npersistentVolumeReclaimPolicy：指定当 PV 的回收策略为 Recycle，支持的策略有：\nRetain：需要管理员手工回收\nRecycle：清除 PV 中的数据并使PV重新可用，效果相当于执行 rm -rf &#x2F;path&#x2F;*（将被弃用）\nDelete：删除存储资源并删除PV\n\n\n\n使用该命令查看PV。注意PV不属于任何一个命名空间，它跟节点⼀样是集群层⾯的资源。\nkubectl get pv\n\n\n此时的PV没有被任何PVC绑定，因此它的状态是Available可用状态。\n持久卷声明（PVC）现在我们扮演开发人员来使用之前创建的持久卷来开发应用。\n我们现在需要使用刚才创建的PV，这个PV是不能像存储卷那样直接在Pod使用的，我们得先创建一个持久卷声明，告诉Kubernetes我们现在需要一个PV，Kubernetes会根据这个清单（PVC）来寻找一个合适的PV。\n持久卷声明的配置实例：\napiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nfs-pvc  namespace：spec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 5Gi  storageClassName: &quot;&quot; # 下一部分将详述\n\n当创建好声明，Kubernetes就会自动找到适当的持久卷并将其绑定到声明，持久卷的容量必须⾜够⼤以满⾜声明的需求，并且卷的访问模式必须包含声明中指定的访问模式。在该⽰例中，声明请求5G的存储空间和ReadWriteOnce访问模式。之前创建的持久卷符合刚刚声明中的这两个条件，所以它被绑定到对应的声明中。我们可以通过检查声明来查看。\n创好持久卷声明之后，使用以下命令查看所有持久卷声明：\nkubectl get pvc\n\n访问模式的简写含义：\n\nRWO（ReadWriteOnce）：仅允许单个节点挂载读写。\nROX（ReadOnlyMany）：允许多个节点挂载只读。\nRWX（ReadWriteMany）：允许多个节点挂载读写这个卷。\n\n\nPV与PVC已经成功绑定（Bound）\nPV显⽰被绑定在default&#x2F;nfs-pvc的PVC上，这个default部分是PVC所在的命名空间（在默认命名空间中创建的声明），我们之前有提到过PV是集群范围的，因此不能在特定的命名空间中创建，但是PVC又只能在特定的命名空间创建，所以PV和PVC只能被同⼀命名空间内的Pod创建使⽤。\n在Pod中使用持久卷声明要在Pod中使⽤持久卷，需要在Pod的卷中引⽤持久卷声明名称。\napiVersion: v1kind: Podmetadata:  name: databasespec:  volumes:  - name: data    persistentVolumeClaim:      claimName: nfs-pvc  containers:  - image: busybox:latest    name: database    command: [&quot;sleep&quot;]    args: [&quot;999999&quot;]    volumeMounts:    - name: data      mountPath: /var/db\n\n我们进入容器，在/var/db下新增一个文件，然后退出容器，发现nfs共享目录/root/nfs也新增了同样的文件。\n\n说明Pod已经成功使用了这个持久卷。\n回收持久卷删除Pod和PVC后，PV的状态为Released，不像之前的Available\n\n此时我们再次apply刚才删除的PVC，此时的PVC的状态为Pending，PV并没有和PVC成功绑定。\n\n这是因为之前已经使⽤过这个卷，所以它可能包含前⼀个声明⼈的数据，如果集群管理员还没来得及清理，那么就不应该将这个卷绑定到全新的声明中。\n根据回收策略进行回收\n\n手动回收持久卷\n通过将persistentVolumeReclaimPolicy设置为Retain从⽽通知到Kubernetes，我们希望在创建持久卷后将其持久化，让Kubernetes可以在持久卷从持久卷声明中释放后仍然能保留它的卷和数据内容。⼿动回收持久卷并使其恢复可⽤的⽅法是删除和重新创建持久卷资源。当这样操作时，你将决定如何处理底层存储中的⽂件：可以删除这些⽂件，也可以妥善保存或者复⽤它们。\n\n自动回收持久卷\n存在两种其他可⾏的回收策略：Recycle和Delete。\n第⼀种Recycle删除卷的内容并使卷可⽤于再次声明，通过这种⽅式，持久卷可以被不同的持久卷声明和Pod反复使⽤。此回收策略即将被弃用，不建议使用。\n⽽另⼀种Delete策略删除底层存储及PV。\n\n\n并不是所有类型的持久卷都支持这三种回收策略，在创建持久卷之前，⼀定要检查卷中所⽤到的特定底层存储⽀持什么回收策略。\n持久卷的动态配置——存储类（StorageClass）到目前为止，集群管理员仍然需要按照开发人员的需求来手动配置持久卷。即每出现一个持久卷声明，就需要一个对应的持久卷。这样做还是有些麻烦，还好Kubernetes可以通过动态配置持久卷来自动执行任务。\n集群管理员可以创建一个持久卷配置，并且定义一个或多个存储类对象（StorageClass），这样开发人员就可以在其持久卷声明中引用存储类，K8s将根据持久卷声明动态分配一个符合要求的持久卷。\n与管理员预先提供⼀组持久卷不同的是，动态配置持久卷需要定义若干个StorageClass，并允许系统在每次通过持久卷声明请求时创建⼀个新的持久卷。\n存储类与持久卷类似，它也是一种集群资源。\n创建一个存储类如果你的集群部署在云服务提供商上，一般云服务提供商会提供好StorageClass资源，在使用时只需要指定SC的名称（storageClassName）即可。但我们的集群部署在本地，所以我们需要手动创建一个或多个StorageClass资源，然后才能创建新的持久卷。\nhelm创建storageClass资源\n我们在这里安装longhorn\nLonghorn | Documentation​Longhorn | Documentation\nlonghorn是一个轻量级且功能强大的云原生 Kubernetes 分布式存储平台，可以在任意基础设施上运行。\n它有以下的优点：\n\n可以创建跨集群灾难恢复卷，以便可以从第二个 Kubernetes 集群中的备份中快速恢复主 Kubernetes 集群中的数据\n跨多个节点和数据中心复制块存储以提高可用性\n将备份数据存储在 NFS 或 AWS S3 等外部存储中，然后从备份中还原卷等等\n\n首先安装open-iscsi。longhorn依赖此协议。iSCSI 是一种远程存储协议，允许在 IP 网络上通过标准以太网连接访问存储资源。和NFS很类似，都是一种远程访问存储数据的网络协议。\napt install open-iscsi\n\n然后使用helm安装\nhelm repo add longhorn https://charts.longhorn.iohelm repo updatekubectl create ns longhornhelm install longhorn longhorn/longhorn -n longhorn\n\n\n删除时参照如下方法，不要直接helm uninstall\n\nUninstall Longhorn - 《Longhorn v1.4.1 Documentation》 - 书栈网 · BookStack\n\n使用存储类创建好StorageClass资源后，我们在PVC中直接指定storageClass资源名称即可，K8s就会自动为我们分配一个符合要求的PV。\napiVersion: v1kind: PersistentVolumeClaimmetadata:  name: pvc-testspec:  storageClassName: longhorn # 指定storageClass的名称  accessModes:    - ReadWriteMany  resources:    requests:      storage: 1Gi\n\napply后storageClass为我们自动分配了一个PV\n\n动态配置的持久卷其容量和访问模式是在PVC中所要求的。它的回收策略是Delete，这意味着当PVC被删除时，PV也将被删除。\n集群管理员可以创建具有不同性能或其他特性的多个存储类，然后研发⼈员再决定对应每⼀个声明最适合的存储类。\n存储类的好处在于，声明是通过名称引⽤它们的。因此，只要StorageClass名称在所有这些名称中相同，PVC定义便可跨不同集群移植。\n不指定存储类的动态配置我们在用NFS创建storageClass资源时指定了它为默认的存储类，当我们创建PVC时，如果不指定storageClassName，它就会默认使用你设置为默认的（default）作为存储类。\n如果我们需要手动创建PV，不使用存储类，我们只需要把storageClassName: &quot;&quot;设置为空字符即可。\n配置向容器内传递命令行与参数每个应用程序都是可执行程序文件，例如Nginx、Tomcat、MySQL等，但我们使用中，通常不会通过默认的配置参数来运行应用，一般都需要自定义符合我们场景的配置，那么就需要定义配置文件来完成。那我们的应用运行在容器中，应该如何定义配置信息呢？\nDocker容器中的命令行与参数的传递向容器命令传递参数\n比如Dockerfile中的CMD与ENTRYPOINT指令。\nENTRYPOINT 指令用于设置容器启动时要执行的默认命令或程序，而 CMD 指令用于为 ENTRYPOINT 提供默认参数。当同时存在 ENTRYPOINT 和 CMD 指令时，CMD 中的内容会被解释为覆盖 ENTRYPOINT 指定程序的默认参数。\n比如这个Dockerfile。ENTRYPOINT 指定了在容器启动时要运行的命令，即 echo &quot;Hello,&quot;，而 CMD 指定了作为参数传递给 ENTRYPOINT 指定的程序的默认参数，即 &quot;world&quot;。因此，当你启动这个容器时，它会打印出 &quot;Hello, world&quot;。\nFROM ubuntuENTRYPOINT [&quot;echo&quot;, &quot;Hello,&quot;]CMD [&quot;world&quot;]\n\n但如果你在运行容器时覆盖了CMD时，那么CMD [&quot;world&quot;]中的world会被换位good morning，容器最后会打印出&quot;Hello, good morning&quot;\n# 添加参数docker run &lt;image&gt; &lt;arguments&gt;docker run my-image good morning\n\n这样可以方便用户灵活地更改参数，从而无需更改Dockerfile。\n将定义好的配置文件嵌入镜像文件中\n你可以提前把配置文件写好，然后COPY或ADD在容器的指定位置。或者你也可以通过RUN指令搭配echo、sed之类的命令同样向配置文件中写入指定参数以达到更改配置文件的效果。\n通过环境变量(Environment Variables)传递配置数据\n通过环境变量为容器提供配置信息是最常见的使用方式，例如，使用MySQL官方提供的镜像文件启动MySQL容器时使用的MYSQL_ROOT_PASSWORD环境变量，它用于为MYSQL服务器的root用户设置登陆密码。\n你可以在docker run时通过-e参数向环境变量传值即能实现应用配置。\n基于Docker卷传送配置文件\n你也可以事先将配置文件放置于宿主机之上的某个路径中，而后在启动容器时将其挂载进容器里。但这也依赖于用户需要事先将配置数据提供在宿主机上的特定路径下，而且在多主机模型中，若容器存在被调度至任一主机运行的可能性时，用户还需要将配置共享到任一宿主机来确保容器能够正常地获取到它们。\n在Kubernetes中覆写命令行与参数在Kubernetes中定义容器时，镜像的ENTRYPOINT和CMD均可以被覆盖，仅需在容器定义中分别设置属性command和args的值。\n示例：\napiVersion: v1kind: Podmetadata:  name: testspec:  containers:  - image: alpine:latest    name: test    command: [&quot;sleep&quot;]    args: [&quot;999999&quot;]\n\n绝⼤多数情况下，只需要设置⾃定义参数。命令⼀般很少被覆盖，除⾮针对⼀些未定义ENTRYPOINT的通⽤镜像，例如busybox。\n注意 command和args字段在pod创建后⽆法被修改。\n如果你想传递多个参数，args是一个列表，在这个列表中指定多个字符串即可。\nargs: [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;]\n\n\n可以看到其父进程（PID为1）为我们指定的命令与参数\n为容器设置环境变量在Kubernetes中使用镜像启动容器时，可以在Pod资源或Pod模版资源为容器配置使用env参数来定义所使用的环境变量列表。\napiVersion: v1kind: Podmetadata:  name: mysqlspec:  containers:  - name: mysql    image: mysql:5.7    env:      - name: MYSQL_ROOT_PASSWORD # 通过env指定了root用户的密码        value: &quot;redrock&quot; \n\nConfigMap\nKubernetes之ConfigMap详解及实践 - 知乎 (zhihu.com)\nConfigMaps | Kubernetes\n\n为什么要引入ConfigMap？\n如果一台服务器上部署多个服务：nginx、tomcat、apache等，那么这些配置都存在这个节点上。如果有一个服务出现问题，需要修改配置文件，每台物理节点上的配置都需要修改，这种方式肯定满足不了线上大批量的配置变更要求。所以，K8s 中引入了 ConfigMap资源对象，可以当成 Volume 挂载到 Pod 中，实现统一的配置管理。简单来说，一个ConfigMap对象就是一系列配置数据的集合，这些数据可“注入”到Pod对象中，并为容器应用所使用，注入方式有挂载为存储卷和传递为环境变量两种。\n优点：将配置存放在独⽴的资源对象中有助于在不同环境（开发、测试、质量保障和⽣产等）下拥有多份同名配置清单。Pod是通过名称引⽤ConfigMap的，因此可以在多环境下使⽤相同的Pod定义描述，同时保持不同的配置值以适应不同环境。\nConfigMap是名称空间级的资源，因此引用它的Pod必须处于同一名称空间中。\n\n创建ConfigMap命令\nkubectl create configmap &lt;map-name&gt; &lt;data-source&gt;\n\n&lt;map-name&gt;为ConfigMap对象的名称，&lt;data-source&gt;是数据源。数据源可以是键值对类型的数据，也可以指定文件或目录来获取。\n通过键值创建创建一个名为mysql-config的ConfigMap，键值对第一个key为mysql_ip值为1.2.3.4 键值对第二个key为mysql_port值为3306\nkubectl create configmap mysql-config \\--from-literal=mysql_ip=1.2.3.4 --from-literal=mysql_port=3306\n\n\n通过文件创建命令\nkubectl create configmap &lt;configmap_name&gt; --from-file=&lt;[key=]source&gt;\n\n配置文件以键值对形式写入\nmysql-config.yaml\nmysql_ip: 1.2.3.4mysql_port: 3306\n\nkubectl create configmap mysql-config --from-file=./mysql-config.yaml\n\n\n通过目录创建当配置文件数量较多时，我们可以在--from-file选项后所跟的路径指向一个目录路径就能把目录下的所有文件一同创建同一个 ConfigMap 资源中。\n命令\nkubectl create configmap &lt;configmap_name&gt; --from-file=&lt;path-to-directory&gt;\n\n例如\nls /data/configs/nginx/conf.d/myserver.conf  myserver-gzip.cfg  myserver-status.cfgkubectl create configmap nginx-config-files --from-file=/data/configs/nginx/conf.d/\n\n这种情况下，kubectl会为⽂件夹中的每个⽂件单独创建条⽬。\n当从文件创建ConfigMap时，所有条⽬第⼀⾏最后的管道符号表⽰后续的条⽬值是多⾏字⾯量。（字面量是一种在 ConfigMap 中直接指定键和值的方式）\n\n直接通过资源配置清单创建apiVersion: v1kind: ConfigMapmetadata:  name: mysql-configdata:  mysql_ip: 1.2.3.4  mysql_port: 3306\n\n向Pod中传递ConfigMap作为环境变量格式\nvalueFrom:  configMapKeyRef:     key: # 要引用ConfigMap对象中某键的键名    name: # 要引用的ConfigMap对象的名称    optional: # 用于为当前Pod资源指明此引用是否为可选\n\n创建一个ConfigMap\nkubectl create configmap mysql-config --from-literal=mysql_password=redrock\n\nvalueFrom单一映射apiVersion: v1kind: Podmetadata:  name: mysqlspec:  containers:  - name: mysql    image: mysql:5.7    env:      - name: MYSQL_ROOT_PASSWORD # 通过env指定了root用户的密码        valueFrom:  \t\t  configMapKeyRef:   \t\t  \tname: mysql-config\t\t# 要引用的ConfigMap对象的名称    \t\tkey: mysql_password\t\t# 要引用ConfigMap对象中某键的键名    \t   #optional: \t\t\t\t# 用于为当前Pod资源指明此引用是否为可选。当值为true时，即使ConfigMap不存在，容器也可以正常启动\n\nenvFrom全部映射如果ConfigMap包含不少条⽬，Kubernetes提供了暴露ConfigMap的所有条⽬作为环境变量的⼿段。\napiVersion: v1kind: Podmetadata:  name: mysqlspec:  containers:  - name: mysql    image: mysql:5.7    envFrom:    - prefix: HTCPG_ # 给键加前缀可以避免从多个ConfigMap引入键值数据时产生键key重名(名称冲突)的问题      configMapKeyRef:         name: mysql-config  # CM资源名称\n\n你也可以向command和args字段中引用环境变量，使用$(VAR_NAME)的格式\n......command: [&quot;mysql&quot;]args: [&quot;-u&quot;,&quot;root&quot;,&quot;-p&quot;,&quot;$(MYSQL_ROOT_PASSWORD)&quot;]\n\n以ConfigMap存储卷形式挂载Pod读取配置文件环境变量或者命令⾏参数值作为配置值通常适⽤于变量值较短的场景。由于ConfigMap中可以包含完整的配置⽂件内容，当你想要将其暴露给容器时，可以借助前⾯章节提到过的⼀种称为configMap卷的特殊卷格式。\nConfigMap卷会将ConfigMap中的每个条⽬均暴露成⼀个⽂件。运⾏在容器中的进程可通过读取⽂件内容获得对应的条⽬值。这种⽅法主要适⽤于传递较⼤的配置⽂件给容器。\n先创建一个cm\nkubectl create cm stu-config --from-file=/root/class3/cm-volume --from-literal=country=China\n\n\napiVersion: v1kind: Podmetadata:  name: stu-mountspec:  volumes:    - name: config-volume      configMap: # configMap形式存储卷        name: stu-config  containers:    - name: test-container      image: busybox      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 1000000&quot; ]      volumeMounts:        - name: config-volume          mountPath: /data/config\n\n\nConfigMap的所有条目都以文件的形式挂载进了Pod中\n如果只想要ConfigMap的部分内容，并自定义文件名，可通过items来配置，如下：\napiVersion: v1kind: Podmetadata:  name: stu-mountspec:  volumes:    - name: config-volume      configMap:        name: stu-config        items: # 引用部分内容          - key: Tom.yaml             path: tom.yaml # 自定义文件名          - key: Mary.yaml            path: mary.yaml  containers:    - name: test-container      image: busybox      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 1000000&quot; ]      volumeMounts:        - name: config-volume          mountPath: /data/config\n\n要注意挂载某⼀⽂件夹会隐藏该⽂件夹中已存在的⽂件。如果挂载⽂件夹是&#x2F;etc，该⽂件夹通常包含不少重要⽂件。由于&#x2F;etc下的所有⽂件不存在，容器极⼤可能会损坏。如果你希望添加⽂件⾄某个⽂件夹如&#x2F;etc，绝不能采⽤这种⽅法。\n如何能挂载ConfigMap对应⽂件⾄现有⽂件夹的同时不会隐藏现有⽂件？volumeMount额外的subPath字段可以被⽤作挂载卷中的某个独⽴⽂件或者是⽂件夹，⽆须挂载完整卷。挂载任意⼀种卷时均可以使⽤subPath属性。可以选择挂载部分卷⽽不是挂载完整的卷。\n在&#x2F;etc下增加一个Tom.yaml，而不是把全部覆盖掉&#x2F;etc\napiVersion: v1kind: Podmetadata:  name: stu-mount-1spec:  volumes:    - name: config-volume      configMap:        name: stu-config  containers:    - name: test-container      image: busybox      command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 1000000&quot; ]      volumeMounts:        - name: config-volume          mountPath: /etc/Tom.yaml          subPath: Tom.yaml\n\n\nConfigMap的更新更新ConfigMap之后ConfigMap卷中对应⽂件的更新可能耗费数分钟。如果使用环境变量的方法，这些值将不会更新；如果在ConfigMap卷中使用了subPath属性，则这些文件也不会更新。在使用时需要注意这一点，必要时手动重启Pod。\nSecret到⽬前为⽌传递给容器的所有信息都是⽐较常规的⾮敏感数据。然⽽配置通常会包含⼀些敏感数据，如证书和私钥，需要确保其安全性。为了存储与分发此类信息，Kubernetes提供了⼀种称为Secret的单独资源对象用来传递敏感配置。\nSecret结构与ConfigMap类似，均是键&#x2F;值对的映射，但Secret专门用于保存机密数据。Secret的使⽤⽅法也与ConfigMap相同，可以将Secret条⽬作为环境变量传递给容器，或将Secret条⽬暴露为卷中的⽂件。\nKubernetes通过仅仅将Secret分发到需要访问Secret的pod所在的机器节点来保障其安全性。另外，Secret只会存储在节点的内存中，永不写⼊物理存储，这样从节点上删除Secret时就不需要擦除磁盘了。另外Secret通过base64编码存储于其中的敏感数据，这样做有两点好处：\n\nbase64可以将一些非文本对象编码为文本值，如图片等。注意其大小不能超过1MB。\nbase64编码后的数据人类不可读，这避免了一些时候操作人员无意中看到并记住密码的情况。\n\n因此应采⽤ConfigMap存储⾮敏感的⽂本配置数据，采⽤Secret存储天⽣敏感的数据，通过键来引⽤。如果⼀个配置⽂件同时包含敏感与⾮敏感数据，该⽂件应该被存储在Secret中。\nSecret有三种类型：\n\nOpaque：base64编码格式的 Secret，用来存储密码、密钥等。\nService Account：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的 /run/secrets/kubernetes.io/serviceaccount目录中。\nDockerConfig：用来存储私有docker registry的认证信息。\n\n默认令牌（Service Account）默认被挂载⾄所有容器的Secret，用于容器访问Kubernetes API的身份认证。\n创建并使用Opaque类型Secret创建一个Secret用来存储mysql的root密码\nkubectl create secret generic mysql-passwd --from-literal=passwd=redrock\n\n\npasswd被自动base64编码\n在Pod中使用Secret\napiVersion: v1kind: Podmetadata:  name: mysql-secretspec:  containers:    - name: mysql      image: mysql:5.7      env:        - name: MYSQL_ROOT_PASSWORD           valueFrom:            secretKeyRef:              name: mysql-passwd              key: passwd\n\n通过Volume挂载到容器内部时，当该Secret的值发生变化时，容器内部具备自动更新的能力，但是通过环境变量设置到容器内部该值不具备自动更新的能力。所以一般推荐使用Volume挂载的方式使用Secret。\napiVersion: apps/v1 kind: Deploymentmetadata:  name: easybanner-deployment   namespace: bot   labels:      app: easybannerspec:   replicas: 1  selector:     matchLabels:       app: easybanner  template:     metadata:      labels:        app: easybanner    spec:      volumes:        - name: easybanner-secret-config          secret:            secretName: easybanner-secret      containers:       - name: easybanner        image: reg.redrock.team/library/easybanner:1.0        env:          - name: App_ID            valueFrom:              secretKeyRef:                name: easybanner-secret                key: App_ID          - name: App_Secret            valueFrom:              secretKeyRef:                name: easybanner-secret                key: App_Secret          - name: URL            valueFrom:              secretKeyRef:                name: easybanner-secret                key: URL          - name: GIN_MODE            valueFrom:              secretKeyRef:                name: easybanner-secret                key: GIN_MODE        volumeMounts:          - name: easybanner-secret-config            mountPath: /app/secrets            readOnly: true        ports:        - containerPort: 8080\n\n创建并使用DockerConfig Secret（镜像拉取Secret）⼤部分组织机构不希望它们的镜像开放给所有⼈，因此会使⽤私有镜像仓库。部署⼀个pod时，如果容器镜像位于私有仓库，Kubernetes需拥有拉取镜像所需的证书。\n我们可以直接create secret，只是参数有些差异。\nkubectl create secret docker-registry secret-dockercfg --docker-username=xxx --docker-password=xxx --docker-email=xxx\n\n而我们在宿主机docker login时，用户名和密码信息被保存在~/.docker/config.json，我们也可以对这个文件进行base64编码后填入.dockerconfigjson字段中。\nbase64 ~/.docker/config.jsonapiVersion: v1kind: Secretmetadata:  name: secret-dockercfgtype: kubernetes.io/dockerconfigjson #指定类型 使用新版本的 data:  .dockerconfigjson: |        &quot;&lt;base64 encoded ~/.docker/config.json file&gt;&quot;\n\n然后再Pod中使用Secret\napiVersion: v1kind: Podmetadata:  name: pull-imagesspec:  imagePullSecrets: # 在这里指定Secret名称  - name: secret-dockercfg  containers:  - image: zhangxinhui02/nginx:private    name: nginx-private\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"插件扩展指引","url":"/posts/31530.html","content":"\n项目的组件放在`internal/`​下\n\n\ncontroller（控制器）：负责核心业务逻辑的调度和协调，监听 Kubernetes 中 IPBlock 自定义资源的变化，维护和管理其生命周期。\nengine（封禁后端）：负责封禁命令的实际执行。\nnotify（通知机制）：负责将封禁、解封等事件以多种方式通知给运维人员或其他系统。\ntrigger（触发器）：事件触发中心，负责监听外部告警系统或业务事件（如 Grafana Alert等），并根据 Alert 策略触发相关封禁操作，自动创建 IPBlock CR，且支持自动解封。\npolicy（封禁策略）：定义 IP 封禁的判定规则和执行策略。\n\nengine介绍封禁后端均被抽象为API，IPBlock-Operator-Plus通过调用API来实现实际的封禁行为。\n目前通过engine/control.py​来实现API，接口列表如下：\n\n\n\n接口\n方法\n说明\n\n\n\n&#x2F;limit\nGET\niptables限流接口\n\n\n&#x2F;unlimit\nGET\niptables解限流接口\n\n\n&#x2F;limits\nGET\niptables查看当前限流情况\n\n\n&#x2F;update\n\nXDP封禁端口\n\n\n&#x2F;remove\n\nXDP解封禁端口\n\n\n&#x2F;ban\nGET\n（弃用）可从nginx日志查，返回超过一定次数的IP\n\n\n&#x2F;execute\nGET\n（弃用）接收IP，对其执行XDP封禁\n\n\nengine支持列表：\n\nXDP：依赖于evilsp&#x2F;xdp_banner: 一个简单的 XDP 小程序，用于 BAN IP\n\niptables：依赖于Linux工具iptables。\n规则为 IP 每分钟最多发起10个新连接（可突发20次），否则DROP\niptables -A INPUT -s &lt;IP&gt; -p tcp --dport &lt;TARGET_PORT&gt; \\  -m state --state NEW \\  -m hashlimit --hashlimit 10/min --hashlimit-burst 20 \\  --hashlimit-mode srcip --hashlimit-name limit_&lt;IP_REPLACED&gt; \\  -j ACCEPTiptables -A INPUT -s &lt;IP&gt; -p tcp --dport &lt;TARGET_PORT&gt; -j DROP\n\n扩展开发指南engine定义了接口，新adapter只需要实现这两个方法即可。\ntype Adapter interface &#123;\t// Ban 对某个 IP 发起封禁\t// ip: 要封禁的 IP 地址\t// isParmanent: 是否永久封禁（true 表示永久）\t// durationSeconds: 封禁时长（单位：秒，仅在临时封禁时生效）\tBan(ip string, isParmanent bool, durationSeconds int) (string, error)\t// UnBan 解封某个 IP\tUnBan(ip string) (string, error)&#125;\n\n然后在NewAdapter​注册对应的adapter\nfunc NewAdapter(name, gatewayHost string) Adapter &#123;\tswitch name &#123;\tcase &quot;xdp&quot;:\t\treturn &amp;XDPAdapter&#123;GatewayHost: gatewayHost&#125;\tcase &quot;iptables&quot;:\t\treturn &amp;IptablesAdapter&#123;GatewayHost: gatewayHost&#125;\tdefault:\t\treturn &amp;XDPAdapter&#123;GatewayHost: gatewayHost&#125;\t&#125;&#125;\n\n接着在controller.py​中要实现对应的API，最后在configmap​中engine​字段指定对应的adapter名即可。\nnotify介绍notify支持列表：\n\nlark：飞书，通过机器人来进行通知\n\n扩展开发指南notify定义了接口，新notify只需要实现这个接口即可。\n// Notifier 是通知发送接口的抽象定义，// 各种通知后端（如飞书、邮件、Webhook等）都应实现此接口。type Notifier interface &#123;\t// Notify 触发一个通知事件。\t//\t// ctx: 标准的 context 对象，用于控制取消、超时等上下文行为。\t// eventType: 事件类型，通常为 &quot;ban&quot;、&quot;resolve&quot;、&quot;common&quot; 等预定义事件名。\t// vars: 用于填充通知模板的变量键值对，例如 map[&quot;ip&quot;]=&quot;1.2.3.4&quot;，map[&quot;reason&quot;]=&quot;攻击频繁&quot;。\t//\t// 返回值:\t//   - 成功时返回 nil\t//   - 失败时返回 error，用于上层重试或告警记录\tNotify(ctx context.Context, eventType string, vars map[string]string) error&#125;\n\n以lark为例，在lark.go​中实现两个方法\n// NewLarkNotify 创建一个 LarkNotify（飞书通知器）实例。//// 参数://   - webhookURL: 飞书群机器人的 Webhook 地址。//   - templatePaths: 各类通知类型所用的 card 模板路径映射（如 map[&quot;ban&quot;]=&quot;templates/ban.json&quot;）。//// 返回值://   - 成功: 返回 LarkNotify 实例。//   - 失败: 返回 error，通常是模板解析失败或配置不合法。func NewLarkNotify(webhookURL string, templatePaths map[string]string) (*LarkNotify, error) &#123;&#125;// Notify 实现 Notifier 接口，// 根据事件类型和变量构造飞书卡片消息，并发送至 webhook。//// 参数://   - ctx: 请求上下文，用于控制超时和取消等。//   - eventType: 事件类型（如 &quot;ban&quot;、&quot;resolve&quot;、&quot;error&quot;）。//   - vars: 模板变量键值对，例如 &#123;&quot;ip&quot;: &quot;1.2.3.4&quot;, &quot;reason&quot;: &quot;恶意连接&quot;&#125;。//// 返回值://   - 成功: 返回 nil。//   - 失败: 返回 error，表示通知失败。func (l *LarkNotify) Notify(ctx context.Context, eventType string, vars map[string]string) error &#123;&#125;\n\n最后在main.go​中watchConfigMap​中完善loadNotify​。\n// 以飞书为例// 加载通知中心\t\tloadNotify := func(cm *corev1.ConfigMap) &#123;\t\t\tnotifyType := cm.Data[&quot;notifyType&quot;]\t\t\twebhookURL := cm.Data[&quot;notifyWebhookURL&quot;]\t\t\ttemplates := make(map[string]string)\t\t\tfor k, v := range cm.Data &#123;\t\t\t\tif strings.HasPrefix(k, &quot;notifyTemplate_&quot;) &#123;\t\t\t\t\teventType := strings.TrimPrefix(k, &quot;notifyTemplate_&quot;)\t\t\t\t\ttemplates[eventType] = v\t\t\t\t&#125;\t\t\t&#125;\t\t\tif notifyType == &quot;lark&quot; &amp;&amp; webhookURL != &quot;&quot; &amp;&amp; len(templates) &gt; 0 &#123;\t\t\t\tlarkNotify, err := lark.NewLarkNotify(webhookURL, templates)\t\t\t\tif err != nil &#123;\t\t\t\t\tlog.Log.Error(err, &quot;Failed to create LarkNotify instance&quot;)\t\t\t\t\treconciler.Notifier = nil\t\t\t\t\treturn\t\t\t\t&#125;\t\t\t\treconciler.Notifier = larkNotify\t\t\t\tlog.Log.Info(&quot;LarkNotify has been initialized&quot;)\t\t\t\treturn\t\t\t&#125;\t\t\t//TODO 其他通知方式...\n\ntrigger介绍trigger支持列表：\n\ngrafana：可以 Grafana Alert 联动，通过 Webhook 进行触发。\n\n扩展开发指南\n注意新 trigger 在开发时需要考虑并发问题。具体实现可查看 grafana.go 中的处理逻辑，也可参考核心功能模块开发文档中并发处理的逻辑介绍。\n\ntrigger同样定义了接口，新trigger只需要实现接口即可。\n// Trigger 是封禁事件的触发器接口，Start 启动监听任务，Stop 停止监听任务type Trigger interface &#123;\tName() string\tStart(ctx context.Context) error\tStop(ctx context.Context) error&#125;\n\ntrigger在manager.go​中实现了StartAll​和StopAll​，会启动在configmap​中指定的所有trigger。\ntriggers:                                          # 触发器，目前仅支持 Grafana  - name: grafana    addr: &quot;:8090&quot;    path: &quot;/trigger/grafana&quot;  - name: your_trigger    other: xxx\n\ntrigger自定义的配置字段，在main.go​中进行配置\n// Grafana Trigger 示例type TriggerConfig struct &#123;\tName string `yaml:&quot;name&quot;`\tAddr string `yaml:&quot;addr,omitempty&quot;`\tPath string `yaml:&quot;path,omitempty&quot;`&#125;// 解析 trigger 字符串为 YAML 列表func parseTriggers(yamlStr string) ([]TriggerConfig, error) &#123;\tvar triggers []TriggerConfig\terr := yaml.Unmarshal([]byte(yamlStr), &amp;triggers)\tif err != nil &#123;\t\treturn nil, err\t&#125;\treturn triggers, nil&#125;// 选择触发器func CreateTriggerByConfig(cfg TriggerConfig, mgr ctrl.Manager) trigger.Trigger &#123;\tswitch cfg.Name &#123;\tcase &quot;grafana&quot;:\t\treturn &amp;trigger.GrafanaTrigger&#123;\t\t\tClient: mgr.GetClient(),\t\t\tAddr:   cfg.Addr,\t\t\tPath:   cfg.Path,\t\t\t// 1000 个 IP， 60 秒防抖\t\t\t// LRU中最多保存1000个IP，达到1000个后会自动淘汰最近最少使用的IP\t\t\t// 对于同一个IP，如果最近60s内发生过一次封禁，那么这60s内再次收到该IP的相同请求时，会被防抖识别为重复\t\t\tDebouncer: utils.NewLRUDebouncer(1000, 60*time.Second),\t\t\tIPLocker:  utils.NewIPLock(),\t\t&#125;\t// TODO 其他触发器 ...\tdefault:\t\treturn nil\t&#125;&#125;\n\npolicypolicy支持功能列表：\n\nwhitelist：白名单机制\n\npolicy实现比较灵活，下面描述白名单机制的实现流程。\n\n在policy/watchlist.go​中实现三个函数\n // 白名单机制// 单IP白名单// CIDR白名单// 标签匹配type Whitelist struct &#123;\tipNets []*net.IPNet // IPNet的指针切片，保存CIDR网段\tips    []net.IP     // 精确IP白名单&#125;// 新建白名单func NewWhitelist(ipList []string) *Whitelist &#123;&#125;// 判断目标IP是否在白名单中func (w *Whitelist) IsWhitelisted(ip string) bool &#123;&#125;// 打印所有白名单内容func (w *Whitelist) StringSlice() []string &#123;&#125;\n\n在config/loader.go​中实现一个LoadWhitelistFromConfigMap​，用于加载定义在configmap​中的白名单IP。\n\n在main.go​的watchConfigMap​中来监听白名单的新建和更新情况。\n\n\n‍\n","categories":["Kubernetes"],"tags":["Achievements","Kubernetes","Operator"]},{"title":"NLP语义匹配学习赛记录","url":"/posts/37125.html","content":"\n# 赛题介绍\n\n背景本次项目来源于阿里云天池的日常学习赛：【NLP系列学习赛】语音助手：对话短文本语义匹配_学习赛_天池大赛-阿里云天池的排行榜\n背景是OPPO公司有一个语音助手，这个语音助手需要根据对话来识别意图。赛题要求是参赛队伍需要根据脱敏后的短文本query-pair，来预测他们是否属于同一语义，最终提交一个预测概率文件来进行评测。\n简单来说就是提供了两句话，需要判断这两句话是不是同一个意思（语义匹配）。\n比如用下面的几个训练样本来举例：\n肖战的粉丝叫什么名字 肖战的粉丝叫什么 1王者荣耀里面打野谁最厉害 王者荣耀什么英雄最好玩 0我想换个手机 我要换手机 1我是张睿 我想张睿 0不想 不想说 0\n\n可以看到意思完全不同的两句话，最终真值标签输出0，表示不匹配。而意思相同的两句话，最终真值标签输出1。\n提交最终需要提交一份预测结果文件，结果文件中每行为一个0-1的预测值，代表query-lair语义匹配的概率，与测试数据每行一一对应。\n0.0010.999\n\n评估\n赛题解析本赛题属于自然语言处理（NLP）领域的文本匹配&#x2F;语义相似度分类任务。给定两段脱敏后的短文本（query-pair），系统需要判断它们是否表达相同或相近的语义，并输出二分类结果。\n难点与挑战\n文本脱敏：原始词语被替换为数字ID，模型就无法直接使用词义信息，只能从数字序列中学习潜在的语义模式。\n如果直接提供了文本，那问题非常简单，直接使用bert-base-chinese这种预训练中文模型就能得到很好的结果，但是这种脱敏数据，由于不知道词表，就无法使用开源的预训练模型。\n\n句子成对输入：任务需要同时理解两段文本并进行比较，而不仅仅是单句分类。\n\n数字ID分布不均：高频词和低频词的数据差异很大，需要合理的enmedding初始化策略，避免低频数字影响训练效果\n\n句子长度和复杂关系：简单模型难以捕获远距离依赖，需要使用能够建模全局信息的网络结果（比如Transformer、BiLSTM等）\n\n\n数据集结构分析提供了4个.tsv数据集，学习赛采用gaiic_track3_round1_testB_20210317.tsv​来做为测试集。\ngaiic_track3_round1_testA_20210228.tsvgaiic_track3_round1_testB_20210317.tsvgaiic_track3_round1_train_20210228.tsv # 训练样本10万gaiic_track3_round2_train_20210407.tsv # 复赛训练样本30万\n\n对于训练集，每行为一个训练样本，由query-pair和真值组成，每行格式如下：\n\nquery-pair格式：query以中文为主，中间可能带有少量英文单词（如英文缩写、品牌词、设备型号等），采用UTF-8编码，未分词，两个query之间使用\\t分割。\n真值：真值可为0或1，其中1代表query-pair语义相匹配，0则代表不匹配，真值与query-pair之间也用\\t分割。\n\n72 29 68 69 70 533 1661 1877\t28 12 347 72 29 369 16\t112 453 317 43 1163 317 59 355 1024\t8 9 1847 6 1163 317 59\t012 19 1162 126 53 66\t12 19 79 389 126 53 66\t1275 552 553 433 881 338 1104 101 202 2343 14825\t995 551 550 1660 2830 1075 662 935\t0421 330 62 12 80 81 82 76\t202 62 12 80 838 76\t1177 455 456 3474 964 1364 55 1364\t133 134 2246\t129 168 12 19 1003 719 23 29 263 276\t29 23 12 115 115 263 16\t1\n\n方案介绍针对上述难点，本方案设计如下：\n\n双塔 Transformer 编码\n 分别编码两段文本，能够捕获全局语义特征，解决句子长度和复杂关系的问题。\n\n向量融合进行分类\n 将两段文本编码向量进行融合，再输入 MLP 分类器，实现句子成对匹配。\n\n结合频度表初始化 embedding\n 针对脱敏数字 ID，通过高频 ID 初始化 embedding，帮助模型快速学习有效特征，缓解数据稀疏问题。\n\n合理训练策略\n 合并训练集、划分验证集、使用早停机制和批量训练，充分利用数据和计算资源，提高模型稳定性和精度。\n\n\n利用这套方案，最终在比赛平台上获得了0.84的最终成绩\n\n代码结构\n# ================= 配置 =================config = &#123;    &quot;max_len&quot;: 128,    &quot;batch_size&quot;: 128,    &quot;epochs&quot;: 8,    &quot;lr&quot;: 2e-4,    &quot;device&quot;: &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;,    &quot;embed_dim&quot;: 512,    &quot;num_heads&quot;: 8,    &quot;num_layers&quot;: 4,&#125;\n\n张量转换：PairDataset功能：\n\n读取句子对数据（q1，q2）\n处理成固定长度序列（padding、截断）\n生成attention mask\n\ninit()构造q1、q2、label以及最大序列长度max_len（默认 64，每条句子只保留一半长度，因为双塔模型会拼接两条句子，保留一半长度虽然有丢失信息的风险，但是这么做可以确保两条句子信息对等，提高显存效率，因为序列越长显存占用越高）\nmax_len &#x3D; q1 + q2 + [CLS&#x2F;SEP&#x2F;padding]\n序列填充函数：pad_seq()功能：保证每条输入序列长度固定，返回half_len + 2的list\n\n[0]相当于[CLS]token，表示序列开头\n\n[1]相当于[SEP]token，表示序列结尾\n\n[2]相当于[PAD]，作为填充，保持固定长度\n\n\n获取样本函数：getitem()功能：返回对应数据集的训练格式。\n\n如果是训练集，返回（q1,a1,q2,a2）\n如果是测试集，返回（q1,a1,q2,a2,label）\n\n这里a1,a2表示attention_mask​，1表示有效token，0表示padding token​。Transformer 在编码时用 src_key_padding_mask=(attn_mask == 0)​ 屏蔽 padding。\n双塔Transformer：SiameseTransformer两条句子分别经过相同编码器 -&gt; 得到向量 -&gt; 融合 -&gt; 分类\ninit()self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n功能：为每一个数字ID创建一个随机向量（长度为 embed_dim​），也就是 vocab_size​ × embed_dim​ 的矩阵。之后通过 init_embedding_from_freq​ 用高频ID对应的更有意义的向量进行替换，以帮助模型更快学到有效语义。\n\nvocab_size：数字ID的总数量\nembed_dim：每个token被映射的向量维度\n\nencoder_layer = nn.TransformerEncoderLayer(    d_model=embed_dim,  # 输入向量维度    nhead=num_heads,    # 多头注意力的头数    dim_feedforward=embed_dim * 4, # 前馈网络隐藏层大小    batch_first=True, # 输入维度[batch, seq_len, embed_dim]    dropout=0.1 # 防止过拟合：随机丢弃部分神经元，也就是随机将10%的神经元输出置为0)self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n功能：定义Transformer编码器\nself.mlp = nn.Sequential(    nn.Linear(embed_dim * 2, embed_dim), # 两个句子的向量拼接【输入 1024 (512*2)，输出 512】    nn.ReLU(), # ReLU激活函数    nn.Linear(embed_dim, 2) # 分类：线性层输出2个logit（对应0/1）【输入 512，输出 2 (类别数)】)\n\n功能：融合两条句子的编码向量并做分类\nencode功能：把一句话从数字序列编码成固定长度向量\n\n将数字序列转换为向量序列emb\n把emb用Transformer编码，提示其在attm_mask&#x3D;&#x3D;0不参与计算，输出为out\n然后取out的第一个token的向量作为句子表示（从 Transformer 输出的一长串向量中，只取出一个代表整句话的向量）\n\nforward功能：模型前向传播\n\n对q1进行encode，得到句子向量v1\n对q2进行encode，得到句子向量v2\n使用torch.cat拼接两个向量\n使用mlp进行分类输出2个logit\n\n训练：train\n设置训练模式（model.train()）、优化器Adam与损失函数（交叉熵CrossEntropyLoss）\n\n设置早停，当连续patience轮验证准确率没有提升，就停止训练，降低过拟合风险。\n best_val_auc​：记录验证集的最好auc\n patience​：早停阈值，如果连续 patience​ 轮验证准确率没有提升，就停止训练\n early_stop_counter​：记录连续验证没有提升的轮数\n\n开始epoch循环\n 每次循环遍历训练集的每个batch，把数据转移到GPU上\n 接着前向传播：logits = model(...)​ → 模型输出每个类别的分数\n 计算损失Loss\n 最后反向传播与梯度更新\n\noptimizer.zero_grad()​ 清除上一步梯度\nloss.backward()​ 计算梯度\noptimizer.step()​ 更新模型参数\n\n 累计batch损失，打印输出\n\n验证阶段：model.eval()\n 遍历验证集，计算每一轮验证集的AUC。\n\n早停判断\n 如果当前验证准确率比历史最好值高，就更新best_val_acc，重置早停计数器并保存当前模型。否则早停计数器加1，若连续patience轮没提升就停止训练。\n\n\n预测：predict遍历测试集，输入两条句子及其attention_mask，得到模型输出logits。\n然后通过softmax将分数转换为概率，取标签1（匹配）的概率。\n频度表初始化：init_embedding_from_freq为了应对脱敏文本的问题，这里参考了论坛中的一位参赛选手的方案：AUC 0.9094 BERT经典方案_天池技术圈-阿里云天池\n即让这些脱敏数字ID变得相对来说有意义，让模型一开始就对高频数字有”合理的表示“，帮助模型更快学习语义。\n为此，我统计了训练集里所有数字ID的频率，然后从https://lingua.mtsu.edu/chinese-computing/statistics/char/list.php?Which=MO&amp;utm_source&#x3D;chatgpt.com中下载了一份汉字频度表文件，取前max_chars个最常用字符，保存在top_chars中。\n接着为top_chars生成随机向量（char_vectors），最后将最频繁出现的数字ID对应的embedding替换为char_vectors，这样高频ID就有了合理的表示，低频ID仍然保持随机初始化。\n\n数据集划分正式比赛分为了初赛和复赛，提供了两个训练集。对于学习赛，我将两个训练集合并为一个大的训练集，训练样本有10+40w条。\ndf_train = pd.concat([df_train1, df_train2], ignore_index=True)\n\n这里我划分了训练集和验证集。对于整个数据集，90%作为训练集，10%作为验证集来评估模型效果、调参和早停。\ndf_tr, df_val = train_test_split(    df_train,     test_size=0.1,     random_state=42,     stratify=df_train[&#x27;label&#x27;] # 保持正负样本比例一致，防止验证集偏斜，提高验证结果可靠性)\n\n统计vocab_size，因为Embedding层需要知道有多少词&#x2F;数字ID。刚开始没有统计这个，导致ID超过embedding行数就会报错：某个索引值超出了目标张量的大小。\nC:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:1290: block: [103,0,0], thread: [60,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cuda\\Indexing.cu:1290: block: [103,0,0], thread: [61,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.  File &quot;c:\\Users\\bearx\\Desktop\\project\\code\\train\\train.py&quot;, line 154, in &lt;module&gt;    predictions = predict(model, test_loader, device)  File &quot;c:\\Users\\bearx\\Desktop\\project\\code\\train\\train.py&quot;, line 111, in predict    probs = torch.softmax(out, dim=1)[:, 1]  # 取概率RuntimeError: CUDA error: device-side assert triggeredCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.For debugging consider passing CUDA_LAUNCH_BLOCKING=1.Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n提分尝试提分路径：0.78（BiLSTM） &#x3D;&gt; 0.73（BERT）&#x3D;&gt; 0.77（频度表映射+预训练base-bert-chinese） &#x3D;&gt; 0.84（双塔Transformer）\nBiLSTMBiLSTM（Bidirectional Long Short-Term Memory）：双向长短期记忆网络是一种RNN变体，能够处理序列文本数据（比如文本）并捕获前后依赖信息。\n它擅长捕捉长距离依赖，经常用于文本分类、句子相似度匹配等NLP任务。\nclass BiLSTM(nn.Module):    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128):        super().__init__()        # 1. Embedding 层，将数字ID映射为向量        self.embedding = nn.Embedding(vocab_size + 10, embed_dim, padding_idx=0)        # 2. BiLSTM 层        # batch_first=True 表示输入 shape = (batch, seq_len, embed_dim)        # bidirectional=True 表示双向LSTM        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)        # 3. 全连接层用于分类        self.fc = nn.Sequential(            nn.Linear(hidden_dim * 4, 256),  # hidden_dim*4：两条句子 * 双向输出            nn.ReLU(),            nn.Dropout(0.3),            nn.Linear(256, 2)  # 输出2类        )\t\t\t# 将每条句子变成一个固定长度向量    def encode(self, x):        x = self.embedding(x)      # 转为向量        out, _ = self.lstm(x)      # BiLSTM 编码        out, _ = torch.max(out, dim=1)  # 取序列维度的最大值（Max Pooling）        return out\t\t# 前向传播\tdef forward(self, q1, q2):        v1 = self.encode(q1)       # 编码第一条句子        v2 = self.encode(q2)       # 编码第二条句子        x = torch.cat([v1, v2], dim=1)  # 拼接两条句子向量        return self.fc(x)           # 分类输出\n\n利用BiLSTM获取了0.78的成绩\n\n放弃BiLSTM，选择Transformer的原因：\n\n对于很长的数字ID序列，BiLSTM捕获全局模式不如Transformer强。Transformer通过自注意力机制，可以让每个token直接与所有token建立联系\nBiLSTM层数少（&quot;embed_dim&quot;: 128​）、参数少，表达能力有限，难以学习到复杂的语义结构。Transformer可以堆叠多层encoder，embed_dim高（&quot;embed_dim&quot;: 512​）\n在相同的数据与训练流程下，BiLSTM 双塔的最佳得分约为 0.78，而 Transformer 双塔可以稳定达到 0.84。\n\nBERT在最初尝试中，我将脱敏后的数字 ID 序列直接转换为 tensor 输入 BERT。但这种做法与 BERT 的设计机制完全不匹配，导致模型性能显著下降，最终准确率只有 0.73。\n主要原因：\n\nBERT的词表是基于中文字符和词训练的\nBERT接收的不是数字ID，而是汉字，并且词表中包含万个token的语义分布。但是脱敏后的数字ID并没有出现在BERT词表里，所以模型无法区分句子之间的差异。\n\nBERT的enbedding是预训练好的中文语义空间\n比如很多中文都有固定语义的embedding，但数字ID完全破坏了这个模式，也就是说预训练只是全部失效\n\n\n在意识到这些问题后，参考了论坛里一位参赛选手的方案，选择了一份汉字频度表，更改了输入，也就是把数字ID频率和汉字频度结合，embedding后让数字ID拥有一定意义，并且使用bert-base-chinese预训练模型，最终的分数达到0.78。\n优化思路\n跑一轮epoch耗时过长，加上T4显卡是租的，按时间收费。所以目前仅跑了8个epoch，训练时验证集AUC一直在升高。其实可以再多跑几个epoch，结合早停机制，AUC应该还能继续提高。\n模型融合：将多个不同类型的模型进行融合，也就是说把各个模型的输出预测值做平均或者加权平均等。\n特征融合：目前只是concat(v1,v2)输入给MLP，可以加入差值（v1-v2）与乘积（v1 * v2）可能会提升判断力。\n\n心得体会以前虽然经常听到 Transformer、NLP 这些名词，但始终没有真正动手实践过。上学期的数据挖掘课程的O2O预测项目让我第一次接触到完整的机器学习流程，也感受到了它的魅力。而这学期通过这个 NLP 的小项目，我初步理解了处理文本类任务的基本思路，也认识了常用的模型和方法。\n这次的提分过程相比去年做O2O预测要轻松一些——一方面得益于 AI 的帮助，另一方面论坛里大佬们的“点拨”也非常关键。整体提分还算顺利，虽然中间也遇到一些曲折。尤其是从 0.78 提升到 0.84 这段，我第一次真正感受到 Transformer 的强大，也深刻意识到显卡对深度学习实验的重要性。在那个阶段，我常常觉得做机器学习像是在做“黑盒测试”，也难怪很多人说是在“炼丹”——训练一次结果出来后，如果想继续提升，你必须同时考虑模型结构、参数、数据、预处理等多个因素，而且一时很难判断问题究竟出在哪里。也可能是因为目前能力有限，提分的方向常常没有特别明确的思路。\n这次的 NLP 实践其实只是对 Transformer、BERT、BiLSTM 这些经典模型有了一个初步认识，它们内部的具体机制和细节我还没完全掌握。不过这次的体验激起了我继续深入学习的兴趣，未来如果有机会，我希望能够系统地理解这些模型背后的原理。\n‍\n","categories":["机器学习"],"tags":["机器学习"]},{"title":"K8s实践记录","url":"/posts/2351.html","content":"\n# K8s实践记录\n\nLevel 01.在节点上创建一个持久化目录mkdir -p /data/nginx/logs #创建多级目录chmod 777 /data/nginx/logs #为系统上的每个人提供读、写和执行权限\n\n\n2.创建一个nginx pod，并在其中配置一个存储卷来将持久化目录挂载到Pod的/var/log/nginx目录中。apiVersion: v1kind: Podmetadata:  name: nginxspec:  volumes:  - name: nginx-logs    hostPath:      path: /data/nginx/logs  containers:  - name: nginx    image: nginx    volumeMounts:    - name: nginx-logs      mountPath: /var/log/nginx\n\n\n\n3.配置nginx日志保存到指定的目录在nginx配置文件nginx.conf中添加以下配置,将nginx访问日志保存到/var/log/nginx/access.log文件中\n```nginxhttp &#123;    access_log /var/log/nginx/access.log main;&#125;```\n\n\n4.验证日志持久化外部访问nginx，在/data/nginx/logs/access.log中保存了访问记录\n\n\n进入容器内部，检查日志是否保存在/var/log/nginx/access.log中\nkubectl exec -it nginx bashtail /var/log/nginx/access.log\n\n\n成功记录\nLevel 1安装NFS服务端1.在主机上安装NFS服务器yum install -y nfs-utils rpcbind\n\n\n2.创建NFS共享目录并设置权限mkdir -p /opt/nfs_logschmod 777 /opt/nfs_logs\n\n\n3.编辑/etc/exports文件，添加NFS共享目录/opt/nfs_logs *(rw,sync,no_subtree_check,no_root_squash)#rw代表读写访问，sync代表所有数据在请求时写入共享，no_subtree_check代表不检查父目录权限，no_root_squash代表root 用户具有根目录的完全管理访问权限\n\n\n4.重启NFS服务器并检查配置结果systemctl enable rpcbindsystemctl enable nfs-serversystemctl start rpcbindsystemctl start nfs-serverexportfs -rexportfs\n\n\n\n配置基于NFS的持久卷1.创建持久卷编写pv.yaml\napiVersion: v1kind: PersistentVolumemetadata:  name: nfs-pv-logsspec:  capacity:    storage: 5Gi  accessModes:    - ReadWriteOnce  storageClassName: nfs  nfs:    server: 172.23.0.239    path: /opt/nfs_logs\n\n\n2.创建持久卷声明编写pvc.yaml\napiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nfs-pvc-logsspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 5Gi  storageClassName: nfs\n\n\n3.将PVC和PV绑定kubectl patch pvc nfs-pvc-logs -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;volumeName&quot;:&quot;nfs-pv-logs&quot;&#125;&#125;&#x27;\n\n\n\n将持久卷声明挂载到nginx容器编写nginx-pv.yaml\napiVersion: v1kind: Podmetadata:  name: nginx-pvspec:  containers:  - name: nginx    image: nginx    ports:      - containerPort: 80        name: nginx-server    volumeMounts:    - name: nfs-pvc-logs      mountPath: /var/log/nginx  volumes:  - name: nfs-pvc-logs    persistentVolumeClaim:      claimName: nfs-pvc-logs\n\n\n验证是否存储日志外部访问nginx，在/opt/nfs_logs/access.log查看日志\n\n成功记录\n遇到的问题（感谢杨鑫同学）\n#编写pv.yaml时候，要把nfs# server的ip地址写到NFS服务器的私网ip\n写成公网ip，最后创建pod的时候，pod状态一直是ContainerCreating，超时了\n用kubectl describe出现以下报错\n\n翻译：\n挂载命令：挂载\n装载参数：-t nfs 8.130.111.175:&#x2F;opt&#x2F;nfs_logs&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;pods&#x2F;d0abb5e10c87-490f-85dd-be3d00478462&#x2F;volumes&#x2F;kubernetes.io~nfs&#x2F;nfs pv-logs\n输出：mount.nfs:连接超时\n警告失败mount 13m（x3超过18m）kubelet无法连接或装载卷：未安装的卷&#x3D;[nfs pvc logs]，未连接的卷&#x3D;[nfs pvc logs kube-api-access-6wfbh]：等待条件超时\n#还有一个就是服务器好卡，重启了好多遍😭😭😭#\nLevel 2思路:\n(前提)在NFS服务器上创建好共享目录\n1.为mysql创建PV和PVC并绑定\n2.创建mysql的Deployment和Service\n3.创建wordpress的Deployment和Service\n参考: Kubernetes(k8s)1.6.0部署 WordPress以及hpa和滚动更新测试_k8s部署wordpress_程序猿（攻城狮）的博客-CSDN博客\n创建命名空间kubectl create namespace wordpress\n\n\n部署MySQL1.创建MySQL的PV和PVC并绑定编写mysql-pv.yaml\napiVersion: v1kind: PersistentVolumemetadata:  name: mysql-pv  namespace: wordpressspec:  capacity:    storage: 1Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  nfs:    path: /root/data/nfs/pv #共享目录    server: 124.221.233.12 #nfs服务器\n\n编写mysql-pvc.yaml\napiVersion: v1kind: PersistentVolumeClaimmetadata:  name: mysqlpvc  namespace: wordpressspec:  accessModes:    - ReadWriteOnce  volumeName: mysql-pv  resources:    requests:      storage: 1Gi\n\n\n2.创建MySQL的Deployment编写mysql-deploy.yaml\napiVersion: apps/v1kind: Deploymentmetadata:  name: mysql-deploy  namespace: wordpress  labels:    apps: mysqlspec:  selector:    matchLabels:      app: mysql  template:    metadata:      labels:        app: mysql    spec:      containers:      - name: mysql        image: mysql:5.6        imagePullPolicy: IfNotPresent        ports:        - containerPort: 3306          name: dbport        env:        - name: MYSQL_ROOT_PASSWORD          value: rootPassWord        - name: MYSQL_DATABASE          value: wordpress        - name: MYSQL_USER          value: wordpress        - name: MYSQL_PASSWORD          value: wordpress        volumeMounts:        - name: db-pv          mountPath: /var/lib/mysql      volumes:      - name: db-pv        persistentVolumeClaim:          claimName: mysqlpvc\n\n\n3.创建MySQL的Service编写mysql-service-yaml\napiVersion: v1kind: Servicemetadata:  name: mysql  namespace: wordpressspec:  selector:    app: mysql  ports:  - name: mysqlport    protocol: TCP    port: 3306    targetPort: 3306\n\n\n部署WordPress1.创建WordPress的Deployment编写wordpress-deploy.yaml\napiVersion: apps/v1kind: Deploymentmetadata:  name: wordpress-deploy  namespace: wordpress  labels:    app: wordpressspec:  selector:    matchLabels:      app: wordpress  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 50%      maxUnavailable: 0  template:    metadata:      labels:        app: wordpress    spec:      containers:      - name: wordpress        image: wordpress        imagePullPolicy: IfNotPresent        ports:        - containerPort: 80          name: wdport        env:        - name: WORDPRESS_DB_HOST          value: mysql:3306        - name: WORDPRESS_DB_USER          value: wordpress        - name: WORDPRESS_DB_PASSWORD\n\n\n2.创建WordPress的Service编写wordpress-service.yaml\napiVersion: v1kind: Servicemetadata:  name: wordpress  namespace: wordpressspec:  type: NodePort  selector:    app: wordpress  ports:  - name: wordpressport    protocol: TCP    port: 80    targetPort: wdport\n\n\n访问(失败)访问http://124.221.233.12:31585\n\n遇到的问题\n一切都是Running但是还是访问不了\nLevel 6参考:Helm:使用helm部署nginx_helm 部署nginx_zJayLiao的博客-CSDN博客\n​        Helm templates 中的语法 - klvchen - 博客园 (cnblogs.com)\n​        Helm | Docs\n安装helmwget https://download.osichina.net/tools/k8s/helm/helm-v3.3.1-linux-amd64.tar.gz cd /opt/helm/tar zxvf helm-v3.3.1-linux-amd64.tar.gzcp helm /usr/local/binchmod a+x /usr/local/bin/helm\n\n\n创建配置文件和目录\nChart.yamlChart 代表着 Helm 包。它包含在 Kubernetes 集群内部运行应用程序，工具或服务所需的所有资源定义。你可以把它看作是 Homebrew formula，apt dpkg，或 yum rpm 在Kubernetes 中的等价物。\n这里定义了chart的基本信息\napiVersion: v2name: nginxdescription: A Helm chart for deploying Nginxversion: 0.1.0\n\nvalues.yamlvalues.yaml用于定义Nginx服务的各种配置选项，如部署Pod的副本数、容器的镜像、端口号、卷的挂载等等。使用values.yaml文件，用户可以轻松灵活地配置并定制Nginx服务的各种参数，以满足他们在不同环境中的需求，在不同集群上部署Nginx服务，而无需重新编写Helm Chart的模板文件。\n相当于定义了一些变量，可以在后面的文件中进行引用。\n镜像拉取策略pullpolicy\n\nreplicaCount: 1 #Pod副本数image: #容器镜像  repository: nginx   tag: latest  pullPolicy: IfNotPresent #镜像拉取策略为IFNotPresentservice:  type: ClusterIP #service的类型  port: 80\n\ndeployment.yamldeployment.yaml是用来定义Nginx的部署资源的文件，它描述了Nginx的副本数，容器镜像，端口映射，环境变量，健康检查等信息.它还可以指定Nginx的服务类型，负载均衡器，安全组等。\n和平常普通的deployment基本相似。\napiVersion: apps/v1kind: Deploymentmetadata:  name: &#123;&#123; include &quot;nginx.fullname&quot; . &#125;&#125; #引用模板，格式：&#123;&#123; include &quot;模版名字&quot; 作用域&#125;&#125;  labels:    app: &#123;&#123; include &quot;nginx.name&quot; . &#125;&#125;spec:  replicas: &#123;&#123; .Values.replicaCount &#125;&#125; #Values代表的就是values.yaml定义的参数，通过.Values可以引用任意参数  selector:    matchLabels:      app: &#123;&#123; include &quot;nginx.name&quot; . &#125;&#125;  template:    metadata:      labels:        app: &#123;&#123; include &quot;nginx.name&quot; . &#125;&#125;    spec:      containers:        - name: nginx          image: &#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125;          imagePullPolicy: &#123;&#123; .Values.image.pullPolicy &#125;&#125;          ports:            - name: http              containerPort: 80\n\nservice.yamlapiVersion: v1kind: Servicemetadata:  name: &#123;&#123; include &quot;nginx.fullname&quot; . &#125;&#125;  labels:    app: &#123;&#123; include &quot;nginx.name&quot; . &#125;&#125;spec:  type: &#123;&#123; .Values.service.type &#125;&#125;  ports:    - name: http      port: &#123;&#123; .Values.service.port &#125;&#125;      targetPort: http  selector:    app: &#123;&#123; include &quot;nginx.name&quot; . &#125;&#125;\n\n_helpers.tpl扩展名是.tpl可用于生成非格式化内容的模板文件\n定义的模板(在&#123;&#123; define &#125;&#125;命令中定义的模板)是可全局访问的。这就意味着chart和所有的子chart都可以访问用&#123;&#123; define &#125;&#125;创建的所有模板。\nRelease 是运行在 Kubernetes 集群中的 chart 的实例。一个 chart 通常可以在同一个集群中安装多次。每一次安装都会创建一个新的 release。以 MySQL chart为例，如果你想在你的集群中运行两个数据库，你可以安装该chart两次。每一个数据库都会拥有它自己的 release 和 release name。\n&#123;&#123;/* 返回部署名称 */&#125;&#125;&#123;&#123;- define &quot;nginx.fullname&quot; -&#125;&#125;&#123;&#123;- printf &quot;%s-%s&quot; .Release.Name &quot;nginx&quot; -&#125;&#125;&#123;&#123;- end -&#125;&#125;&#123;&#123;/* 返回应用名称 */&#125;&#125;&#123;&#123;- define &quot;nginx.name&quot; -&#125;&#125;&#123;&#123;- &quot;nginx&quot; -&#125;&#125;&#123;&#123;- end -&#125;&#125;\n\n验证(失败)helm package nginx #打包helm install nginx ./nginx-0.1.0.tgz #安装\n\n遇到的问题当用helm打包时报错\nhelm package nginxError: validation: chart.metadata is required\n\nLevel 7参考:在 Kubernetes 安装 KubeSphere | KubeSphere Documents\n​         安装 Kuboard v3 - 内建用户库 | Kuboard\n​         Kuboard-Spray 图形化工具安装kubernetes集群_kubernetes图形化工具_一只懒惰的猿的博客-CSDN博客\n部署KubeSphere(失败)最小化安装\nkubectl apply -f https://raw.githubusercontent.com/kubesphere/ks-installer/v2.1.1/kubesphere-minimal.yaml\n\n查看Pod状态\nkubectl get pod --all-namespaces\n\n\n用Kuboard-Spray安装K8s并部署Kuboard1.安装docker-ce yum-config-manager \\&gt;     --add-repo \\&gt;     https://download.docker.com/linux/centos/docker-ce.repoyum updateyum install docker-ce\n\n2.安装Kuboard-Spraydocker run -d \\  --privileged \\  --restart=unless-stopped \\  --name=kuboard-spray \\  -p 80:80/tcp \\  -v /var/run/docker.sock:/var/run/docker.sock \\  -v ~/kuboard-spray-data:/data \\  eipwork/kuboard-spray:latest-amd64\n\n\n安装完成后访问https://8.130.175.111,用户名`admin`,默认密码 Kuboard123，即可登录 Kuboard-Spray 界面。\n\n3.下载资源包\n4.创建集群并选择各自的角色\n5.安装K8s集群\n安装成功\n6.部署Kuboarddocker run -d \\  --restart=unless-stopped \\  --name=kuboard \\  -p 80:80/tcp \\  -p 10081:10081/tcp \\  -e KUBOARD_ENDPOINT=&quot;http://10.0.4.12:80&quot; \\  -e KUBOARD_AGENT_SERVER_TCP_PORT=&quot;10081&quot; \\  -v /root/kuboard-data:/data \\  eipwork/kuboard:v3\n\n访问http://124.221.233.12/:80\n\n\n成功\n遇到的问题部署KubeSphere时报错\nkubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath=&#x27;&#123;.items[0].metadata.name&#125;&#x27;) -f\n\n\n提示版本不支持\nkubectl version\n\n\n版本是v1.25.0,按照官方文档提示应该是可以的\n\n不知道为啥\n","categories":["Kubernetes"],"tags":["Kubernetes"]},{"title":"IPBlock-Operator-Plus","url":"/posts/28178.html","content":"\n## 项目简介\n\nIPBlock-Operator-Plus 是一个基于 Kubernetes 的 Operator，构建了一个插件化、模块化的恶意 IP 封禁管理平台。该项目通过自定义资源（IPBlock CR）实现对恶意 IP 的声明式管理，支持手动和自动限流与封禁功能。其架构高度可扩展，便于集成多种触发器（Trigger）和通知机制（Notify），从而实现智能化的安全防护与运维管理。\n项目架构架构图\n\nIPBlock-Operator-Plus 项目由以下五个核心模块组成：\n\nController（控制器）\n\n​\t负责核心业务逻辑的调度和协调，监听 Kubernetes 中 IPBlock 自定义资源的变化，维护和管理其生命周期。它是整个项目的核心部分，协调各个模块协同工作。\n\nEngine（封禁后端）\n负责封禁命令的实际执行。目前支持接入多种封禁机制（如XDP，iptables等）。通过插件化设计，便于快速扩展和替换不同的封禁技术方案。\n\nNotify（通知机制）\n\n\n​\t负责将封禁、解封等事件以多种方式通知给运维人员或其他系统。现支持飞书通知渠道，开发人员可通过自定义插件（实现接口）灵活添加更多通知方式。\n\nTrigger（触发器）\n事件触发中心，负责监听外部告警系统或业务事件（如 Grafana Alert等），并根据 Alert 策略触发相关封禁操作，自动创建 IPBlock CR，且支持自动解封。模块设计也方便开发人员接入多种告警来源。\n\nPolicy（封禁策略）\n定义 IP 封禁的判定规则和执行策略。现支持白名单机制，保障了封禁行为的精准与合理。\n\n\n项目部署环境要求\ngo version v1.24.0+\nkubectl version v1.11.3+.\nAccess to a Kubernetes v1.11.3+ cluster.\nPython 3\nMake\n\n项目支持 Helm 和 Make两种部署方式\n封禁后端部署封禁后端需要在监测目标机器上手动执行python3 ../../ipblock-operator/internal/engine/control.py\n最好将其制作成Service，保证后台持久运行。\n这里提供get_ip.service文件供参考。\n[Unit] Description=Get IP Service After=network.target [Service] User=root WorkingDirectory=/root/yiiong/get_ip  # control.py所在目录Environment=&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;ExecStart=/bin/bash -c &#x27;source /root/yiiong/get_ip/venv/bin/activate &amp;&amp; exec python3 control.py&#x27; # 运行Python程序，注意文件路径Restart=always [Install] WantedBy=multi-user.target\n\nHelm项目的helm/下存放了IPBlock-Operator-Plus的helm chart，在安装前，请先填写values.yaml\n# values.yamlimage:  repo: beatrueman/ipblock-operator  tag: &quot;6.0&quot;  pullPolicy: IfNotPresentconfig:  gatewayHost: &quot;&quot;                                    # 封禁后端 URL  engine: &quot;&quot;                                         # 可选: xdp, iptables  whiteList: |\t\t\t\t\t\t\t\t\t\t                   # IP 白名单，支持在 ConfigMap中动态更新    1.2.3.4  notifyType: &quot;&quot;                                     # 可选: lark  notifyWebhookURL: &quot;&quot;                               # larkRobot Webhook  notifyTemplate:                                    # larkRobot发送的card消息模板    ban: &quot;/templates/lark/ban.json&quot;    resolve: &quot;templates/lark/resolve.json&quot;    common: &quot;/templates/lark/common.json&quot;  triggers:                                          # 触发器，目前仅支持 Grafana    - name: grafana      addr: &quot;:8090&quot;      path: &quot;/trigger/grafana&quot;\n\n填写完成后，安装helm chart即可\n注意需要将IPblock-Operator部署在default命名空间下。\nhelm install ipblock-operator .\n\nMake在集群上部署将 CRD 安装到集群中make install\n\n将 Manager 部署到集群中填写../../IPBlock-Operator-Plus/config/default下的configmap.yaml\n# configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: ipblock-operator-configdata:  gatewayHost: &quot;&quot;                                             # 封禁后端 URL  engine: &quot;&quot;                                                  # 可选: xdp, iptables  trigger: |                                                  # 触发器，目前仅支持 Grafana    - name: grafana      addr: &quot;:8090&quot;      path: &quot;/trigger/grafana&quot;  whitelist: |                                                # IP 白名单，支持在 ConfigMap中动态更新    1.2.3.4  notifyType: &quot;&quot;                                              # 可选: lark  notifyWebhookURL: &quot;&quot;                                        # larkRobot Webhook  notifyTemplate_ban: &quot;/templates/lark/ban.json&quot;              # larkRobot发送的card消息模板，请勿更改路径  notifyTemplate_resolve: &quot;/templates/lark/resolve.json&quot;  notifyTemplate_common: &quot;/templates/lark/common.json&quot;\n\n\n注意： 如果您遇到 RBAC 错误，您可能需要授予自己 cluster-admin 权限或以 admin 身份登录。\n\nmake deploy\n\n创建样例kubectl apply -k config/samples/\n\n卸载从集群中删除实例（CRs）kubectl delete ipblock --all\n\n从集群中删除API（CRDs）make uninstall\n\n从集群中删除控制器make undeploy\n\n配置项说明ConfigMap# configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: ipblock-operator-configdata:  gatewayHost: &quot;&quot;                                             # 封禁后端 URL  engine: &quot;&quot;                                                  # 可选: xdp, iptables  trigger: |                                                  # 触发器，目前仅支持 Grafana    - name: grafana      addr: &quot;:8090&quot;      path: &quot;/trigger/grafana&quot;  whitelist: |                                                # IP 白名单，支持在 ConfigMap中动态更新    1.2.3.4  notifyType: &quot;&quot;                                              # 可选: lark  notifyWebhookURL: &quot;&quot;                                        # larkRobot Webhook  notifyTemplate_ban: &quot;/templates/lark/ban.json&quot;              # larkRobot发送的card消息模板，请勿更改路径  notifyTemplate_resolve: &quot;/templates/lark/resolve.json&quot;  notifyTemplate_common: &quot;/templates/lark/common.json&quot;\n\nvalues.yaml# values.yamlimage:  repo: beatrueman/ipblock-operator  tag: &quot;6.0&quot;  pullPolicy: Alwaysconfig:  gatewayHost: &quot;&quot;                                    # 封禁后端 URL  engine: &quot;&quot;                                         # 可选: xdp, iptables  whiteList: |\t\t\t\t\t\t\t\t\t\t                   # IP 白名单，支持在 ConfigMap中动态更新    1.2.3.4  notifyType: &quot;&quot;                                     # 可选: lark  notifyWebhookURL: &quot;&quot;                               # larkRobot Webhook  notifyTemplate:                                    # larkRobot发送的card消息模板    ban: &quot;/templates/lark/ban.json&quot;    resolve: &quot;templates/lark/resolve.json&quot;    common: &quot;/templates/lark/common.json&quot;  ServiceType: NodePort  triggers:                                          # 触发器，目前仅支持 Grafana    - name: grafana      addr: &quot;:8090&quot;      path: &quot;/trigger/grafana&quot;\n\nTrigger配置Grafana字段介绍\n\n\n字段\n说明\n必需\n\n\n\nname\n触发器名称，当前支持 grafana\n是\n\n\naddr\n监听地址和端口，例如 &quot;:8090&quot;\n是\n\n\npath\nWebhook请求路径，例如 /trigger/grafana\n是\n\n\nGrafana Alert配置配置联络点\n将配置好的URL填入联络点的URL中。\n举例：http://&lt;your-ip&gt;:&lt;NodePort&gt;/trigger/grafana​\n\n配置警报规则\n自定义警报规则，并选择联络点为刚才配置好的联络点。\n本例为1min内若访问次数超过80，则触发警报。\n\nNotigy配置目前仅支持飞书Lark，后续将添加更多，如邮件、钉钉、企业微信等。\nLark添加自定义机器人，并将获取到的Webhook地址填入配置的notifyWebhookURL即可。\n\n使用示例创建一个 IPBlock 资源apiVersion: ops.yiiong.top/v1kind: IPBlockmetadata:  name: test-ipblockspec:  ip: &quot;1.2.3.4&quot;                       # 支持单IP / CIDR形式  reason: &quot;模拟异常请求&quot;               # 封禁原因  source: &quot;manual&quot;                    # 封禁源  by: &quot;admin&quot;                         # 封禁者  duration: &quot;10m&quot;                     # 封禁时长，当字段为空时，永久封禁  \n\nCR创建成功后，如接入飞书，会向用户发起通知。\n\n对IP解封apiVersion: ops.yiiong.top/v1kind: IPBlockmetadata:  name: test-ipblockspec:  ip: &quot;1.2.3.4&quot;                        reason: &quot;模拟异常请求&quot;                  source: &quot;manual&quot;                      by: &quot;admin&quot;                           unblock: true                      # 解封字段\n\n成功后，飞书会通知用户\n\n手动强制封禁apiVersion: ops.yiiong.top/v1kind: IPBlockmetadata:  name: test-ipblockspec:  ip: &quot;1.2.3.4&quot;                        reason: &quot;模拟异常请求&quot;                  source: &quot;manual&quot;                      by: &quot;admin&quot;                           trigger: true                      # 重复封禁\n\n白名单跳过当在配置文件中指定了WhiteList（支持单IP &#x2F; CIDR），CR会检测封禁IP是否在白名单中，如在则跳过。\n","categories":["Kubernetes"],"tags":["Achievements","Kubernetes","Operator"]},{"title":"iManager-您的私人镜像管家","url":"/posts/27345.html","content":"\n# iManager-您的私人镜像管家\n\n尽管Docker官方提供了公共的镜像仓库DockerHub，但从安全性和稳定性等方面考虑，部署私有镜像仓库是非常有必要的。Harbor是一个由VMware公司开源的企业级的Docker Registry管理项目，是我们搭建私有镜像仓库的不二之选。\n而自动化一直是运维工作中的重中之重。\nYiiong希望你能够将Harbor与飞书机器人联系起来，方便管理员统一管理镜像，使管理员随时都可以知晓仓库镜像的变动信息，管理员动动手指就可以对仓库里的镜像进行操作等。\n前置准备\n建议使用服务器搭建Harbor仓库，并且配置好https\n申请一个飞书企业账号\n可能需要一个域名？\n\n任务友善的用户交互用户肯定需要知道你的机器人都可以干些什么事情吧\n用户向机器人发送帮助，机器人返回可以实现的功能列表\n当用户发送了一些乱七八糟的信息时，机器人需要做出一定的回复，比如我听不懂捏\nLevel 0 通知当镜像状态发生改变时，机器人向管理员发起通知\n通知内容包括：\n\n镜像状态（Push or Pull or Delete）\n镜像名称以及标签\n镜像状态改变的日期\n镜像状态改变者\n\nLevel 1 简单查询管理员向机器人发送查询项目，机器人返回所有项目的名称、所属者以及访问级别\n管理员向机器人发送查询仓库，机器人返回所有仓库的名称、创建时间以及更新时间\nLevel 2 镜像管理在我们的团队开发过程中，难免会出现多次推送镜像但镜像测试不通过的情况，\nYiiong希望当镜像被以Push状态推送时，通知内容中新增加一个超链接按钮，点击这个按钮即可删除这个镜像\n另一种方式是，向机器人发送删除镜像：镜像名，即可删除某个镜像\nLevel 3 用户管理我们的整个仓库不可能只有一个管理员用户，管理员经常需要为不同的成员创建不同的用户\nYiiong希望管理员向机器人发送创建用户：用户名-密码-邮箱即可创建一个新用户\n并且可以将特定用户以开发者身份加入到特定项目的成员中\nLevel 4 打包编写Dockerfile，将你的所有文件用Docker打包制作成镜像，使其他用户填入一些必要的信息就可以愉快地使用你的作品辣！\n加分项\n发挥你的奇思妙想，实现一些意想不到的功能\n使用Kubernetes完成必要的内网穿透\n使用UptimeKuma监控你的仓库，当出现问题时通过机器人返回一些必要信息\n飞书开放平台提供了一种美观的”卡片式“消息，使用它的话答辩的时候一定很潇洒啊\n\n","categories":["服务器运维"],"tags":["服务器运维"]},{"title":"Gitlab CICD学习记录","url":"/posts/63036.html","content":"\n# Gitlab CICD学习记录\n\n用Flask在K8s集群编写后端项目编写app.py编写一个最简单的后端项目，命名为app.py，返回hello world!\nfrom flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello():    return &quot;hello world!&quot;if __name__ == &#x27;__main__&#x27;:    app.run(host=&#x27;0.0.0.0&#x27;, port=8080)\n\n用pipreqs导出依赖文件pip3 install pipreqs\n\npipreqs . --encoding=utf8 --force\n\n\n构建Docker镜像编写dockerfile\nFROM python:3.9WORKDIR /appCOPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txtCOPY . .CMD [&quot;python&quot;,&quot;app.py&quot;]\n\ndocker build -t beatrueman/ci:2.0 .\n\n上传镜像\ndocker push beatrueman/ci:2.0\n\nK8s集群部署后端服务使用命令创建Pod。创建一个名为flask-hello-world的Pod，并将容器的端口映射到主机的端口8080。\nkubectl run flask-hello-world --image=beatrueman/ci:2.0 --port=8080 \n\n\n使用命令暴露服务。以LoadBalancer类型创建服务，映射到Pod的8080端口\nkubectl expose pod flask-hello-world --type=LoadBalancer --port=80 --target-port=8080\n\n查看服务\nkubectl get service flask-hello-world\n\n\n开放32091端口，访问外部ip\n\n\n成功\n安装runner获取registration-token\n使用docker安装runner安装docker run -d --name gitlab-runner --restart always \\  -v /srv/gitlab-runner/config:/etc/gitlab-runner \\  -v /var/run/docker.sock:/var/run/docker.sock \\  gitlab/gitlab-runner:latest\n\n注册docker exec -it gitlab-runner bash\n\ngitlab-runner register \\--non-interactive \\--url &quot;https://gitlab.com/&quot; \\--registration-token &quot;GR1348941F_CsNBNspPP2DccgUVcx&quot; \\ --executor &quot;docker&quot; \\--docker-image ubuntu:22.04 \\--description &quot;docker-runner&quot; \\--maintenance-note &quot;Free-form maintainer notes about this runner&quot; \\--tag-list &quot;docker&quot; \\--run-untagged=&quot;true&quot; \\--locked=&quot;false&quot; \\--access-level=&quot;not_protected&quot;\n\n成功\n\n#一直把registration-token当成runner里的token了，导致后面不停报错#\n使用helm安装runner（失败）下载package并解压helm repo add gitlab https://charts.gitlab.iohelm pull gitlab/gitlab-runnertar -zxvf gitlab-runner-0.53.2.tgz\n\n文件目录如下\n\n配置token填入和docker鉴权apiVersion: v1kind: Secretmetadata:  name: gitlab-runner-secrettype: Opaquedata:# 配置runner的token  runner-registration-token: &quot;R1IxMzQ4OTQxRl9Dc05CTnNwUFAyRGNjZ1VWY3gK&quot; # 将token转为base64后写这里  runner-token: &quot;&quot;---apiVersion: v1data:# 配置docker的config.json  .dockerconfigjson: &quot;ewogICAgICAgICJhdXRocyI6IHsKICAgICAgICAgICAgICAgICJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOiB7CiAgICAgICAgICAgICAgICAgICAgICAgICJhdXRoIjogIlltVmhkSEoxWlcxaGJqcHNlWGd3TXpBNU1UWT0iCiAgICAgICAgICAgICAgICB9CiAgICAgICAgfQp9Cgo=&quot;kind: Secretmetadata:  name: regsecrettype: kubernetes.io/dockerconfigjson\n\n在values.yaml中应用配置文件gitlabUrl: https://gitlab.com/Beatrueman/test.gitrunners:# 在这里指定我们刚刚创建的secret  secret: gitlab-runner-secret  imagePullSecrets: regsecret  tags: &quot;k8s&quot;  config: |    [[runners]]      [runners.kubernetes]        namespace = &quot;&#123;&#123;.Release.Namespace&#125;&#125;&quot;        image = &quot;ubuntu:20.04&quot;\n\n镜像上传自定义kaniko镜像提前在CI&#x2F;CD &gt; Variables下设置好变量\n\n在/root/kaniko下编写dockerfile和build-upload\n\ndockerfile\nFROM lonewalkerchr/kaniko:debugCOPY ./build-upload /kaniko/build-uploadSHELL [&quot;/busybox/sh&quot;]RUN [&quot;/busybox/chmod&quot;, &quot;a+x&quot;, &quot;/kaniko/build-upload&quot;]\n\nbuild-upload\nprintf &quot;%s&quot; &quot;$&#123;DOCKER_AUTH&#125;&quot; &gt; /kaniko/.docker/config.json/kaniko/executor \\     --force \\     --cache \\     --context &quot;$&#123;CI_PROJECT_DIR&#125;&quot; \\     --dockerfile dockerfile \\     --destination $&#123;destination&#125;\n\n构建镜像并上传至dockerhub\ndocker build -t beatrueman/kaniko:1.0 .docker push beatrueman/kaniko:1.0\n\n上传代码创建并配置.gitlab-ci.yaml添加KUBE_CONFIG变量，内容为~/.kube/config的内容\n\n创建并配置.gitlab-ci.yaml\nstages:  - build  - deployimagebuilder:  image: beatrueman/ci:kaniko  stage: build  variables:    destination: beatrueman/ci:2.0  script:    - /kaniko/build-uploaddeploy:  image: bitnami/kubectl  stage: deploy  script:    - echo $&#123;KUBE_CONFIG&#125; &gt; kubeconfig    - kubectl run flask-hello-world --image=beatrueman/ci:2.0 --port=8080     - kubectl expose pod flask-hello-world --type=LoadBalancer --port=80 --target-port=8080\n\n上传到仓库git initgit add .git commit -m &quot;first&quot;git remote add origin https://gitlab.com/Beatrueman/test-ci.gitgit push -u origin master\n\n\n问题与解决1.构建Docker镜像时CMD的时候少了一个后引号，折腾了半天。#感谢一下杨鑫同学了，多亏了他的慧眼。\n2.安装runner时用的是registration-token，不是创建runner时候的token。\n3.${CI_PROJECT_DIR}变量是自带的，是runner里面的一个不固定的位置，可以通过它来找到绝对路径，不能自行指定，不然会找不到文件导致报错。\n\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple\n\npip安装的时候使用-i指定镜像源，下载速度会提升很多很多。\n5..gitlab-ci.yaml里deploy加entrypoint覆盖。\n\n6..gitlab-ci.yaml里deploy阶段执行kubectl命令建议每条命令都加–kuboeconfig&#x3D;.&#x2F;kubeconfig，保证每条命令都有权限。\n","categories":["服务器运维"],"tags":["服务器运维","CICD"]},{"title":"EasyCreater——一站式简历生成器","url":"/posts/23431.html","content":"\n## 简介\n\nEasyCreater——让简历制作更高效、更智能！\n产品简介\nEasyCreater，让简历制作更轻松，让求职之路更顺畅！ 🚀✨\n\nEasyCreater 是一款在线简历制作工具，旨在帮助用户专注于内容创作，而无需操心排版和格式，让简历制作更加高效便捷。\n🚀 高度自定义\n\n提供极高的自由度，用户可以自由调整简历侧栏颜色、宽度、头像形状等，打造个性化简历。\n\n🤖 AI 智能辅助\n\n内置 AI 大模型，无论是优化表达、润色内容，还是解决写作难题，都可以随时向 AI 寻求帮助，让简历更专业、更出色。\n\n📝 自动保存草稿\n\n系统会自动保存用户的编辑内容，防止数据丢失，确保写作过程更安心顺畅。\n\n💡 灵感词汇推荐\n\n编辑界面侧边栏提供丰富的灵感词汇和参考语句，助力用户精准表达，提升简历亮点。\n\n🌍 简历广场分享\n\n用户可将简历分享至简历广场，不同用户可对简历广场上的简历点赞。用户可与他人交流、学习，实现互帮互助，共同提升求职竞争力。\n\n📌 求职导航\n\n提供就业经验荟萃专区，整合知名面试刷题网站的快捷入口，并精选简历撰写必备词句，助力高效求职。\n\n🖨 一键导出 PDF\n\n支持简历导出为 PDF，方便打印或在线投递，确保格式美观、排版整齐，让求职更高效。\n\n📥 多格式简历导入\n\n支持导入 PDF、Word 及图片格式的简历，并提供云端存储，便捷管理已有简历。无论何时何地，用户都能随时访问、编辑和优化自己的简历，确保求职更高效、更从容。\n\n\n首页\n\n简历制作编辑\n\n我的简历\n\n简历广场\n\n通过EasyCreater所生成的简历\n\nAI 简历内容生成\n\nAI 简历润色\n\n灵感词汇\n\n简历点赞\n\n技术架构\nEasyCreater，助力高效求职，提供强大技术支撑！ 🚀\n\nEasyCreater 采用前后端分离架构开发，支持多种灵活的部署方式，包括 Linux 服务器、Docker Compose 及 Kubernetes，确保高效稳定的运行。\n架构图\n技术栈💻 前端技术栈\n\nVue 3 + Element Plus —— 提供流畅的用户交互体验，界面简洁美观，操作便捷高效。\n\n⚙️ 后端技术栈\n\nGo（Gin 框架 + Gorm）  —— 高性能 Web 框架 Gin，结合 Gorm ORM，高效处理业务逻辑，提供稳定可靠的 API 服务。\n\n🗄 数据库\n\nMySQL —— 采用 MySQL 作为核心数据库，支持高并发数据存储，保障简历数据的稳定存储与管理。\nRedis — 用于缓存简历数据，构建高效稳定的 AI 流式输出基础设施。\n\n☁ 对象存储\n\n阿里云 OSS —— 负责存储简历缩略图和用户上传的已有简历，确保数据安全、访问高效。\n\n🧠 AI 智能支持\n\n阿里云通义千问 API — 提供 AI 内容优化、智能润色及写作建议，支持流式输出，模拟真实 AI 问答体验，助力简历更专业、更出彩。\n\n🚀 多种部署方式\n\nLinux 单机部署 —— 适用于轻量级服务器，简单易维护。\nDocker Compose —— 通过容器化管理，快速部署，降低环境依赖。\nKubernetes（K8s）  —— 适用于大规模分布式部署，实现弹性扩展与高可用性。\n\n部署建议使用docker-compose部署，方便快捷\nLinux单机部署本项目提供了一键启动脚本start.sh\n环境准备\n\nLinux\nGolang1.23\nNode.js 20\nNginx\nMySQL\n通义大模型_企业拥抱 AI 时代首选-阿里云\n阿里云 对象存储OSS_\n\n如何开始？\nchmod +x start.sh./start.sh\n\nDocker compose一键部署本项目支持 docker-compose 一键部署\n注意\n\n运行前需要填写 be/config/config.yaml，其中MySQL.host请填写 mysql，如果要使用外部MySQL，请修改相关配置。\n按照如下配置，项目启动后，前端运行在8080端口，后端运行在8888端口。如果启动后提示端口占用，请自行修改端口ports\n\nversion: &#x27;3&#x27;services:  redis:    image: redis:latest    networks:      - EasyCreater-network      mysql:    image: mysql:latest    # 确保与config.yaml中填写的信息一致    environment:      MYSQL_ROOT_PASSWORD: 123456      MYSQL_DATABASE: demo      MYSQL_PASSWORD: 123456    networks:      - EasyCreater-network  frontend:    build:      context: ./fe    ports:      - &quot;8080:80&quot;    depends_on:      - backend    networks:      - EasyCreater-network  backend:    build:      context: ./be    ports:      - &quot;8888:8888&quot;    volumes:      - ./be/config/config.yaml:/EasyCreater/config/config.yaml    depends_on:      - mysql      - redis    networks:      - EasyCreater-networknetworks:  EasyCreater-network:    driver: bridge\n\n如何开始？\ndocker-compose up -d\n\nKubernetes部署请切换至cloud分支查看部署方法。\nCI 自动化流水线部署支持添加了用于打包和推送镜像的Github Action\n使用时在Settings &gt;&gt; Secrets and varibles &gt;&gt; Actions中添加secrets\nREGISTRY_USERNAME和REGISTRY_PASSWORD\n\n如果要推送到类似Harbor的自建仓库，请添加varibles\nIMAGE_REGISTRY_SERVICE：默认为docker.io\nIMAGE_FE_REPOSITORY：默认为beatrueman&#x2F;easycreater-fe\nIMAGE_BE_REPOSITORY：默认为beatrueman&#x2F;easycreater-be\n推送时请指定tag，格式为v1.0.0，用于指定镜像版本\ngit tag v1.0.0git push origin v1.0.0\n\n或者手动指定tag\n","categories":["Achievements"],"tags":["Achievements"]},{"title":"EasyBanner","url":"/posts/2684.html","content":"\n# EasyBanner\n\n功能介绍通过​ ​@机器人​​来触发飞书机器人对特定机器远程封禁恶意IP，依赖于​**GitHub - evilsp&#x2F;xdp_banner: 一个简单的 XDP 小程序，用于 BAN IP**\n1.当机器人未连接服务器时，提示如下消息。\n\n2.当未发现访问次数超过250次的IP，提示如下消息。\n\n3.当发现访问次数超过250次的恶意IP，罗列出恶意IP以及对应的访问次数，并显示封禁BAN按钮。\n\n点击按钮，可以对远程机器进行封禁恶意IP，然后卡片更新，提示封禁完成。\n\n逻辑介绍接口目标机器开放两个接口\n\nGET &#x2F;execute：用于查询日志里当前小时内访问量排名前十的IP以及对应的次数。\nPOST &#x2F;ban：用于接收需要封禁的IP，然后执行xdp封禁命令。\n\n机器人设置两个接口\n\nPOST &#x2F;webhook：用来处理接收消息事件。\nPOST &#x2F;event：用来处理卡片回调\n\n运行逻辑用户给机器人发消息，触发接收消息v2.0事件，飞书服务器返回消息体，通过消息体里messageBody.Event.Message.Mentions的mention.ID.UserID是否为空来判断用户发的消息是否为@机器人，不是则忽略。\n是则发送卡片消息。这时调用目标机器/execute接口来获得IP以及对应的次数，通过判断 ip 次数，发送不同的模板。\n没有返回数据，则发送未连接服务器模板。有返回数据但没有大于250次的IP，返回未检测到恶意IP模板。\n有返回数据且有大于250次的IP时，获取所有次数大于250次的IP以及对应的次数动态填充至 JSON 模板，然后发送。\n此时用户点击红色按钮BAN，会触发卡片回传交互事件，此时会回传数据。接下来对回传消息体进行一些判断：\n\n判断event_type是否为card.action.trigger\n判断action.Tag是否为button\n判断action.Value中键action对应的值是否为ban_ip\n\n如果全部满足，则将需要封禁的IP制作成请求体，对目标机器POST /ban进行调用。\nAPI成功调用后，调用飞书更新卡片API，对卡片内容进行更新。\n部署申请机器人app_id与app_secret获取方法1.用企业账户，在开发者后台中，创建企业自建应用\n\n2.找到app_id与qpp_secret\n\n3.添加应用能力，选择机器人\n\n4.添加以下权限\nim:message,im:message.group_at_msg,im:message.group_at_msg:readonly,im:message.group_msg,im:message.p2p_msg,im:message.p2p_msg:readonly,im:message:readonly,im:chat:readonly,im:chat,im:message:send_as_bot\n\n\n5.订阅接收消息和卡片回传交互\n若要使机器人有互动对话功能，需要填写请求配置地址，并添加接收消息v2.0和消息已读v2.0事件\n卡片交互需要订阅卡片回传交互\n\n\n裸机部署首先需要在目标机器上执行EasyBanner/pkgs/data/app_current.py，保持其稳定运行。\n最好将其制作成Service，保证后台持久运行。\n这里提供get_ip.service文件供参考。\n[Unit] Description=Get IP Service After=network.target [Service] User=root WorkingDirectory=/root/yiiong/get_ip  # app_current.py所在目录Environment=&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;ExecStart=/bin/bash -c &#x27;source /root/yiiong/get_ip/venv/bin/activate &amp;&amp; exec python3 app_current.py&#x27;  # 运行Python程序，注意文件路径Restart=always [Install] WantedBy=multi-user.target\n\n或者手动运行\npython3 ../EasyBanner/pkgs/data/app_current.py\n\n然后下载依赖后，运行main.go\ngo mod tidygo run main.go\n\nDocker部署目标机器的API只能裸机部署\n容器启动\ndocker run -d -p 8080:8080 \\-e APP_ID=cli_a42011585561100d \\ # 填写飞书应用 AppID-e APP_SECRET=AYRFbDlUH8OKxRweuXM47cLLFwRpO12X \\ # 填写飞书应用 AppSecret-e URL=http://172.20.14.2:9521 \\  # 填写检测主机地址-e GIN_MODE=release beatrueman/easybanner:stable\n\nKubernetes部署先创建Secret设置环境变量\nkubectl create secret generic easybanner-secret \\--from-literal=App_ID=your_AddID \\--from-literal=App_Secret=your_AppSecret \\--from-literal=URL=your_url \\--from-literal=GIN_MODE=release \\ # 设置gin为生产模式--namespace=your_namespace\n\n然后apply..&#x2F;deply目录下的deployment.yaml，service.yaml。\n所有注意修改命名空间。\n还有要注意部署的机器人与目标机器接口的通信问题\nkubectl apply deployment.yamlkuectl apply service.yaml# 如果有域名需求，还可以添加ingressroute或者ingress，注意修改host# 这里使用ingressroutekubectl apply -f ingressroute.yaml\n\n缺陷\n机器人可能会重复发送消息。\n因为会对大日志文件利用bash进行查询，所以执行GET /execute速度会比较慢，测试大概会消耗20s左右。\nBAN操作比较慢，需要等待一会才能更新卡片。\n\n","categories":["Achievements"],"tags":["Achievements"]},{"title":"BIND9 DNS智能解析实验","url":"/posts/53842.html","content":"\n**BIND（Berkeley Internet Name Domain）**  是全球使用最广泛的 **DNS 服务器软件**，最初由加州大学伯克利分校开发，目前由 **ISC（Internet Systems Consortium）**  维护。\n\nBIND9 让一台服务器可以充当 DNS 解析器（递归）或权威服务器（发布域名解析记录）\n目标基于bind9的view功能，实现DNS的智能解析。\n\nmaster和两个slave之间实现主从同步\nslave-1和slave-2均提供对www.a.com的解析，其中slave-1解析www至1.1.1.1；slave-2解析www至2.2.2.2\n\n环境容器\n\nbind-master：172.28.0.2\nbind-slave-1：172.28.0.3\nbind-slave-2：172.28.0.4\n\nzone：a.com\n域名：www.a.com\n\nbind-slave-1视图：1.1.1.1\nbind-slave-2视图：2.2.2.2\n\n权威NS：ns1.a.com（172.28.0.2）\nNS：ns2.a.com（172.28.0.3、172.28.0.4）\n目录结构root@RainYun-8iZCjURn:/home/bind9-2# tree ..├── docker-compose.yml├── master│   ├── config│   │   ├── named.conf│   │   └── rndc.key│   └── zones│       ├── a.com.zone.1│       ├── a.com.zone.2│       └── named.log├── slave-1│   ├── config│   │   └── named.conf│   └── zones└── slave-2    ├── config    │   └── named.conf    └── zones\n\ndocker-compose.ymlversion: &quot;3.9&quot;services:  bind-master:    image: internetsystemsconsortium/bind9:9.18    container_name: bind-master    restart: always    entrypoint: /bin/sh    tty: true    stdin_open: true    ports:      - &quot;253:53/tcp&quot;      - &quot;253:53/udp&quot;    volumes:      - ./master/config:/etc/bind      - ./master/zones:/var/lib/bind    environment:      - TZ=Asia/Shanghai    networks:      bindnet:        ipv4_address: 172.28.0.2  bind-slave-1:    image: internetsystemsconsortium/bind9:9.18    container_name: bind-slave-1    restart: always    ports:      - &quot;153:53/tcp&quot;      - &quot;153:53/udp&quot;    volumes:      - ./slave-1/config:/etc/bind    environment:      - TZ=Asia/Shanghai    depends_on:      - bind-master    networks:      bindnet:        ipv4_address: 172.28.0.3  bind-slave-2:    image: internetsystemsconsortium/bind9:9.18    container_name: bind-slave-2    restart: always    ports:      - &quot;353:53/tcp&quot;      - &quot;353:53/udp&quot;    volumes:      - ./slave-2/config:/etc/bind    environment:      - TZ=Asia/Shanghai    depends_on:      - bind-master    networks:      bindnet:        ipv4_address: 172.28.0.4networks:  bindnet:    driver: bridge    ipam:      config:        - subnet: 172.28.0.0/16\n\n配置master添加zone，配置主从options &#123;\t// bind工作目录，所有zone文件、日志文件都会在这个目录下找    directory &quot;/var/lib/bind&quot;;\t\t// bind监听的 ipv4 地址    listen-on &#123; any; &#125;;    listen-on-v6 &#123; any; &#125;;\t// 指定允许哪些客户端向该服务器发起查询（Query），对于递归DNS不要使用any    allow-query &#123; any; &#125;;\t\t// 是否允许递归查询    recursion no;\t\t// 控制是否启用DNSSEC签名验证，递归服务器时适合开启，权威一般不用    dnssec-validation no;    \t// 隐藏bind版本号\tversion &quot;hidden&quot;;&#125;;// 定义 IP 源acl &quot;slave1&quot; &#123; 172.28.0.3; &#125;;acl &quot;slave2&quot; &#123; 172.28.0.4; &#125;;// 定义视图view &quot;view1&quot; &#123;\t// 允许访问zone的IP来源    match-clients &#123; slave1; &#125;;\t    zone &quot;a.com&quot; &#123;        type master;\t\t\t\t// zone file存储路径        file &quot;/var/lib/bind/a.com.zone.1&quot;;        \t\t// 允许区域文件传输的IP地址\t\tallow-transfer &#123; 172.28.0.3; &#125;;\t\t\t\t// zone file变更后主动通知 slave        also-notify &#123; 172.28.0.3; &#125;;    &#125;;&#125;;view &quot;view2&quot; &#123;    match-clients &#123; slave2; &#125;;    zone &quot;a.com&quot; &#123;        type master;        file &quot;/var/lib/bind/a.com.zone.2&quot;;        allow-transfer &#123; 172.28.0.4; &#125;;        also-notify &#123; 172.28.0.4; &#125;;    &#125;;&#125;;\n\n添加bind-slave-1视图的zone file# 添加记录;; BIND Zone File;; $TTL 5 表示该DNS表项在client的缓存时间为5s;; Refresh 表示slave与master同步的时间; Retry    表示slave与master断连后，多久进行重连检查; Expire    表示slave与master断连多久后，该区域失效; Negative Cache TTL 表示查询的数据不存在时，这个否定的回答在其他服务器的缓存时间; @代表当前$ORIGIN（zone的根，通常是zone文件声明的域名，例如a.com）; IN 表示 Internet class; SOA（Start Of Authority）记录zone的元数据; Serial很关键，用来判定zone是否更新（slave只在master的serial增大时拉取更新），通常为日期风格YYYYMMDDNN; NS声明负责该zone的权威name server; 末尾的.表示完整域名（绝对名称），如果没有末尾点会被当作相对名称，追加当前 $ORIGIN：ns1 → ns1.a.com.; 常见的其他记录：CNAME、MX（邮箱），TXT（文本记录）、SRV（服务定位记录）、PTR（反向解析）$TTL 1H@   IN  SOA ns1.a.com. admin.a.com. (        2025102304 ; Serial        1H        15M        1W        1H )    IN  NS ns1.a.com.    IN  NS ns2.a.com.ns1 IN  A 172.28.0.2ns2 IN  A 172.28.0.3; slave-1 视图www对应 1.1.1.1www IN  A 1.1.1.1\n\n添加bind-slave-2视图的zone file$TTL 1H@   IN  SOA ns1.a.com. admin.a.com. (        2025102305 ; Serial        1H        15M        1W        1H )    IN  NS ns1.a.com.    IN  NS ns2.a.com.ns1 IN  A 172.28.0.2ns2 IN  A 172.28.0.4www IN  A 2.2.2.2\n\nslaveoptions &#123;    directory &quot;/var/cache/bind&quot;;    listen-on &#123; any; &#125;;    listen-on-v6 &#123; any; &#125;;    allow-query &#123; any; &#125;;    recursion no;    dnssec-validation no;    version &quot;hidden&quot;;&#125;;zone &quot;a.com&quot; &#123;    type slave;    masters &#123; 172.28.0.2; &#125;;    file &quot;slaves/a.com.zone&quot;;&#125;;logging &#123;    channel default_log &#123;        file &quot;/var/cache/bind/named.log&quot; versions 3 size 5m;        severity info;        print-time yes;    &#125;;    category default &#123; default_log; &#125;;&#125;;\n\n验证slave-1解析dig @172.28.0.3 www.a.com\n\n\nslave-2解析dig @172.28.0.4 www.a.com\n\n可以看到解析至2.2.2.2\n\n注意点容器重启实验采用internetsystemsconsortium/bind9:9.18​镜像，该镜像的entrypoint​为\n/usr/sbin/named -u bind -f -c /etc/bind/named.conf -L /var/log/bind/default.log\n\n当配置文件named.conf​语法出现问题或文件权限不正确时，容器会重启，且日志不会输出到标准输出上，也就是说通过docker log是看不到日志的。\n‍\n这时我们可以修改entrypoint​为/bin/sh​，手动进入容器进行调试，通过named命令来启动服务。\n配置文件权限不正确22-Oct-2025 12:37:11.678 ---------------------------------------------------- 22-Oct-2025 12:37:11.678 adjusted limit on open files from 1073741816 to 104857622-Oct-2025 12:37:11.678 found 2 CPUs, using 2 worker threads22-Oct-2025 12:37:11.678 using 2 UDP listeners per interface22-Oct-2025 12:37:11.682 DNSSEC algorithms: RSASHA1 NSEC3RSASHA1 RSASHA256 RSASHA512 ECDSAP256SHA256 ECDSAP384SHA384 ED25519 ED44822-Oct-2025 12:37:11.682 DS algorithms: SHA-1 SHA-256 SHA-38422-Oct-2025 12:37:11.682 HMAC algorithms: HMAC-MD5 HMAC-SHA1 HMAC-SHA224 HMAC-SHA256 HMAC-SHA384 HMAC-SHA51222-Oct-2025 12:37:11.682 TKEY mode 2 support (Diffie-Hellman): yes22-Oct-2025 12:37:11.682 TKEY mode 3 support (GSS-API): yes22-Oct-2025 12:37:11.682 the initial working directory is &#x27;/&#x27;22-Oct-2025 12:37:11.682 loading configuration from &#x27;/etc/bind/named.conf&#x27; 22-Oct-2025 12:37:11.686 directory &#x27;/var/lib/bind&#x27; is not writable22-Oct-2025 12:37:11.686 /etc/bind/named.conf:2: parsing failed: permission denied22-Oct-2025 12:37:11.686 loading configuration: permission denied22-Oct-2025 12:37:11.686 exiting (due to fatal error)\n\n调试用，最直接的方式是给777\nps：给过755，而且文件权限属于bind:bind，我在测试时还是没有写权限。\nrndc reload密钥问题rndc​ 是 BIND 的远程控制工具（Remote Name Daemon Control）\n可以用来让正在运行的 named​：\n\n重新加载配置文件或 zone (rndc reload​)\n重新签名 DNSSEC\n清理缓存\n停止&#x2F;启动服务等\n\n‍\n当修改zone file（增加或删除解析记录）、SOA序列号（Serial）需要自增1\n然后使用rndc reload刷新配置\n如果遇到\n/ # rndc reload a.com rndc: neither /etc/bind/rndc.conf nor /etc/bind/rndc.key was found\n\n需要添加rndc.key​\ncd /etc/bindrndc-confgen -a -b 512\n\n然后对生成的rndc.key​赋予权限\nchown bind:bind /etc/bind/rndc.keychmod 600 /etc/bind/rndc.key\n\n理论赋上面的权限，但是测试时经常提示无写入权限，所以就直接777了\n‍\n","categories":["服务器运维"],"tags":["服务器运维","DNS"]},{"title":"2022红岩杯WriteUp","url":"/posts/49152.html","content":"\n# PWN\n\n一口一个flag题目：nc一下下\n\nidea：根据提示，nc是linux的命令，所以本题需要在linux环境求解\n\nstep：打开Linux的终端，nc所给的端口输入：nc 172.20.14.18 31751得到答案：redrock&#123;Ech0eCHo-IWaNtF10g_&#125;\n\n\n猫猫flag题目：nc二下下\n\nidea:本题与上题类似\n\nstep:nc端口后无回显，继续输入指令ls后出现flag然后cat flag得到答案:redrock&#123;SEhLlCAT_IWaNtf1a9_&#125;\n\n\nWeb你是哪里的题目：https://96a84eec-4f7b-45a6-a5bb-224f1064604e.ctf.redrock.team/\n\nidea:进入第一层网页，显示（you must be from https://redrock.team）没什么破绽，按下F12,F5刷新一下，点进网页显示（A word in the http message header is wrong,do you know?），按下F12并刷新依然没有什么破绽，于是想到用Burpsuite进行抓包\n\nstep：打开burpsuite，进行抓包，根据题目的暗示（你来自哪里），联想到来自https://redrock.team于是添加Referer：https://redrock.team后send找到答案：redrock&#123;we1c0me t0 redr0ck ctf&#125;\n\n\nRemindYourHead题目：https://19ab54b4-79ea-469a-b457-f79ca700a343.ctf.redrock.team/\n\nidea：点击网页发现是被黑了的网站，按下F12并刷新，没有什么破绽于是想到抓包\n\nstep：用Burpsuite抓包，直接找到答案得到答案：redrock&#123;d95c187c-df9d-463f-80e2-8eea94e73595&#125;\n\n\nMisc签到题目：微信公众号关注 &quot;重邮小帮手&quot;, 发送 &quot;红岩杯CTF&quot;, 获得 flag\n\n得到答案：redrock&#123;020408&#125;\n\n\n啵啵的魔法药水题目：魔法世界里的小魔女啵啵找不到她的魔法药水了，快来帮帮她ヾ(′▽‘)ﾉ 进入魔法世界需要一点点魔力呢° (๑˘ ˘๑) ♥啵啵给你的小tips：用docker run &#x27;bobo&#x27; 可以短暂的打开进入魔法世界的大门【拉取镜像的咒语：docker pull wgyao/redrock-ctf:bobo】\n\nidea:根据提示，本题需要用到docker。\n\nstep：载后在cmd命令行里输入docker pull wgyao/redrock-ctf:bobo然后在dockers的Image里，找到wgyao/redrock-ctf后点进，发现第21行出现了答案然后点击右边Command得到答案：redrock&#123;Wit-Sharpening_Potion.&#125;\n\n\n\n流量审计题目：点击就送flag\n\nidea：下载附件，发现.pcapng类型文件需要用Wireshark打开\n\nstep：用Wireshark打开，发现有线索下面有text/plain，然后点进在最后找到答案：redrock&#123;yyz_is_god&#125;\n\n\n\nCrypto来自红岩的密文1题目：听说这是一段一直流传在红岩的密文，每一个解密出这段密文的他/她都会开启属于自己的旅程，你能否成功的解密这段密文？开启属于自己的旅程\n\nidea：下载附件，发现是数字，想到ASCII\n\nstep：对应ASCII转码即可求解redrock&#123;Welc0me-T0-The-CTF-0f-redrock!!!&#125;\n\n来自红岩的密文2题目：邪恶的力量妄想夺取密文，危难关头清水拿出了一段晦涩的咒语，只要成功的解密这段咒语就能成功的阻止邪恶的Dog&amp;Cat，并且听说这段咒语是开启新冒险的钥匙之一\n\nidea：下载附件，发现是一串emoji表情，在网上搜索密码类型，查到这是base100类型，进行转码，没有得到答案，继续寻找转码，多次尝试找到是base64类型转码，依然没有得到答案，继续寻找，多次尝试找到这是base32转码，最终得到答案。\n\nstep：多次转码，得到答案redrock&#123;BaSe100_ba5E64_Base32_u_find_me!&#125;\n\n\n你干嘛题目：听，各种动物的叫声\n\nidea：下载附件，打开是一堆有规律的中文。在网上查找密码类型，发现“Ook”刚好可以和“你干嘛”对应\n\nstep：用word文档将“你干嘛”换成“Ook”“。”换成“.”“？”（中文）换成“?&quot;（英文）“！”（中文）换成“!&quot;（英文）在在线Ook转换工具转换出一堆“奇怪的叫声”查找密码类型，发现是“兽音译者”密码类型进行转换，得到答案：redrock&#123;4re y0u Ok?&#125;\n\n\n\n可惜我年轻无知题目：但是我是如此的年轻无知，不曾听到她的心声。 救赎之道，就在其中！ yveypbl&#123;kfu_h_kvhsq_mpfsq_dse_appjhgx_rhux_xvy_rpfje_spu_dqyvv&#125;\n\nidea:根据格式可知yveypbl=redrock在网络上寻找解码，找到一个网站https://quipqiup.com/可以进行解码，刚好与题目对应得到答案：redrock&#123;but i being young and foolish with her would not agree&#125;\n\n\n\nA和B题目：最终flag字母均为小写\n\nidea:下载附件，发现是一堆A和B，并且是最终答案的格式想到培根密码对照转换表进行手动转换，得到r&#123;zeyydyyrzzoyzczykz&#125;隐约看到有些规律，这串字符里包含了redrock这样排列r e d r o c k&#123; y y z y z zz y y z z y &#125;得到答案：redrock&#123;yyzyzzzyyzzy&#125;\n\n\nReversejust_re_it题目：Try to submit everything like redrock&#123;...&#125;\n\nidea：先下载附件，百度搜索，reverse类型题目需要用到IDA\n\nstep:直接把附件拖入IDA64中，查找字符串，得到答案redrock&#123;This_is_fake_flag&#125;\n\n\n水水爱听歌题目：你能猜中我的心思吗~ Python Message.pyc 就可以运行！\n\nidea：下载附件，发现后缀是.pyc，百度搜索，发现这种文件需要反编译，在在线工具中反编译得到后缀为.py的文件，用VS打开。这是python代码，需要修改些东西。先把  清水大帅比 修改，改成A，保证可以运行细看代码，发现if语句与flag的输出有关修改A=check（flag）为A=True\n\n观察此句，知道这行代码，如果flag的base64加密和zzz相等，则返回true所以把zzz解密就可以了在网上搜索，zzz后的解码类型为URL进行解码得到She_bid_me_to_take_love_easy_as_the_leaves_grow_on_the_tree\n\n\n然后运行代码，出现下面的情况\n\n\n还可以输入东西，于是输入She_bid_me_to_take_love_easy_as_the_leaves_grow_on_the_tree得到答案：redrock&#123;044d7a01a972dc5882831e89676220c2dc3a3c142e16379a76a45680137a6b55&#125;\n\n\n赛博丁真题目：喜欢我 HeLang 吗？\n\nidea：平时网上冲浪，知道本题目和何同学有关下载附件，发现是补充代码在网上搜索这段代码是何同学制作键盘时被网友发现的一段错误代码我尝试把错误代码补全，发现成功得到答案！redrock&#123;ggg_ding_zhen&#125;\n\n\njmp_or_nop题目：下载附件\n\nidea：下载附件后发现和反汇编有关，拖入dbg（32）中\n\nstep1:搜索字符串，找到相关信息，可知最终答案格式为RedRock&#123;&#125;\n\n\nstep2：观察和答案相关部分的汇编语言，可知有直接输出flag的操作，但打开程序并不显示。所以应该修改汇编指令。\n\n\nstep3：可以看到这里有cmp进行了判断，再用jae指令判断是否跳跃，将其修改。以下同理。\n\n\nstep4：接下来就可以运行了最终得到答案：RedRock&#123;jmp_and_nop_is_really_useful&#125;\n\n\n","categories":["Achievements"],"tags":["安全"]}]